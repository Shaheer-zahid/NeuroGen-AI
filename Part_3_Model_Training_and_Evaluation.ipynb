{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ray\n",
        "!pip install stable-baselines3\n",
        "!pip install shimmy\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SPp51vVw1TR",
        "outputId": "04f03578-02a5-4535-82ef-45e39a2275a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.11/dist-packages (2.46.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray) (5.29.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.25.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2025.4.26)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.13.2)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Requirement already satisfied: shimmy in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0: Setup and Initialization\n",
        "# Purpose: Set up environment, initialize Ray, load preprocessed data.\n",
        "\n",
        "import torch\n",
        "import ray\n",
        "import psutil\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "drive_path = '/content/drive/MyDrive/Sentiment_Project'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "print(f\"Drive mounted at {drive_path}\")\n",
        "\n",
        "# Initialize Ray\n",
        "if not ray.is_initialized():\n",
        "    ray.init(address='local', ignore_reinit_error=True, logging_level=logging.INFO)\n",
        "    print(\"Ray cluster initialized in local mode.\")\n",
        "else:\n",
        "    print(\"Ray cluster already initialized, skipping reinitialization.\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define Coordinator Agent\n",
        "@ray.remote(num_cpus=1, num_gpus=0 if not torch.cuda.is_available() else 0.1)\n",
        "class CoordinatorAgent:\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "        self.start_time = datetime.now()\n",
        "        self.log_file = os.path.join(drive_path, 'coordinator_logs.txt')\n",
        "        logger.info(\"Coordinator Agent initialized.\")\n",
        "        self._check_log_file()\n",
        "\n",
        "    def _check_log_file(self):\n",
        "        if os.path.exists(self.log_file) and os.path.getsize(self.log_file) > 10 * 1024 * 1024:\n",
        "            with open(self.log_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "            with open(self.log_file, 'w') as f:\n",
        "                f.writelines(lines[-1000:])\n",
        "            logger.info(\"Log file rotated due to size limit.\")\n",
        "\n",
        "    def log_message(self, message):\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_entry = f\"[{timestamp}] {message}\"\n",
        "        self.logs.append(log_entry)\n",
        "        logger.info(log_entry)\n",
        "        try:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                f.write(log_entry + '\\n')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to write to log file: {e}\")\n",
        "        return log_entry\n",
        "\n",
        "    def get_resource_usage(self):\n",
        "        try:\n",
        "            cpu_usage = psutil.cpu_percent(interval=1)\n",
        "            ram = psutil.virtual_memory()\n",
        "            ram_usage = ram.percent\n",
        "            gpu_usage = 0.0\n",
        "            memory_usage = 0.0\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_query = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,noheader,nounits'],\n",
        "                                          stdout=subprocess.PIPE, text=True, timeout=5)\n",
        "                gpu_data = [float(x.strip()) for x in gpu_query.stdout.strip().split(',') if x.strip()]\n",
        "                gpu_usage = gpu_data[0] if gpu_data else 0.0\n",
        "                memory_used = gpu_data[1] if len(gpu_data) > 1 else 0.0\n",
        "                memory_total = gpu_data[2] if len(gpu_data) > 2 else 1.0\n",
        "                memory_usage = (memory_used / memory_total * 100) if memory_total > 0 else 0.0\n",
        "            resource_dict = {\n",
        "                \"cpu_usage (%)\": cpu_usage,\n",
        "                \"ram_usage (%)\": ram_usage,\n",
        "                \"gpu_usage (%)\": gpu_usage,\n",
        "                \"gpu_memory_usage (%)\": memory_usage,\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "            logger.info(f\"Resource usage: {json.dumps(resource_dict)}\")\n",
        "            return resource_dict\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Resource monitoring error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def get_logs(self):\n",
        "        return self.logs\n",
        "\n",
        "# Instantiate Coordinator Agent\n",
        "coordinator = CoordinatorAgent.remote()\n",
        "\n",
        "# Load preprocessed data\n",
        "preprocessed_path = os.path.join(drive_path, 'preprocessed_data.pt')\n",
        "try:\n",
        "    data = torch.load(preprocessed_path, weights_only=False)\n",
        "    input_ids = data['inputs']['input_ids']\n",
        "    attention_mask = data['inputs']['attention_mask']\n",
        "    embeddings = data['embeddings']  # Optional, not used currently\n",
        "    labels = data['labels']\n",
        "    print(f\"Loaded preprocessed data: {len(labels)} samples\")\n",
        "    print(f\"Input tensor shape: {input_ids.shape}\")\n",
        "    print(f\"Embedding shape: {embeddings.shape}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Failed to load preprocessed data: {e}. Using dummy data.\")\n",
        "    input_ids = np.zeros((100, 128), dtype=np.int32)\n",
        "    attention_mask = np.ones((100, 128), dtype=np.int32)\n",
        "    labels = np.random.randint(0, 2, 100)\n",
        "    embeddings = np.zeros((100, 768))  # Dummy embeddings\n",
        "\n",
        "# Load original dataset for metadata\n",
        "dataset_path = os.path.join(drive_path, 'processed_dataset.csv')\n",
        "try:\n",
        "    df = pd.read_csv(dataset_path, encoding='utf-8')\n",
        "    print(f\"Loaded original dataset: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Failed to load dataset: {e}. Using empty DataFrame.\")\n",
        "    df = pd.DataFrame(columns=['review', 'sentiment'])\n",
        "\n",
        "# Load target accuracy from data_analysis_report.json\n",
        "report_path = os.path.join(drive_path, 'data_analysis_report.json')\n",
        "try:\n",
        "    with open(report_path, 'r') as f:\n",
        "        analysis_report = json.load(f)\n",
        "    target_accuracy = analysis_report['target_accuracy']\n",
        "    print(f\"Target accuracy: {target_accuracy}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Failed to load target accuracy: {e}. Using default 0.95.\")\n",
        "    target_accuracy = 0.95\n",
        "\n",
        "# Set intent and GPU availability\n",
        "intent = 'performance'  # Default, can be adjusted\n",
        "gpu_available = torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofitDy9SwtnL",
        "outputId": "8807753e-9e02-4da4-bb6a-6b9df0f1efbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted at /content/drive/MyDrive/Sentiment_Project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-28 23:41:00,212\tINFO worker.py:1888 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray cluster initialized in local mode.\n",
            "Loaded preprocessed data: 50000 samples\n",
            "Input tensor shape: (50000, 128)\n",
            "Embedding shape: (50000, 768)\n",
            "Loaded original dataset: 50000 rows\n",
            "Target accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhUcb0oABWYi"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "# Purpose: Import additional libraries and configure the environment.\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertForSequenceClassification, \\\n",
        "    RobertaForSequenceClassification\n",
        "from torch.optim import AdamW # Import AdamW from torch.optim\n",
        "import joblib\n",
        "from tqdm import tqdm # Import tqdm for progress bars\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Preprocessing (Fallback)\n",
        "# Purpose: Preprocess dataset if preprocessed data is unavailable, with dynamic column detection.\n",
        "\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Ensure logger is defined\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define SentimentDataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        try:\n",
        "            self.input_ids = torch.tensor(input_ids, dtype=torch.long) if input_ids is not None else torch.empty(0)\n",
        "            self.attention_mask = torch.tensor(attention_mask, dtype=torch.long) if attention_mask is not None else torch.ones(0, 128)\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else torch.empty(0)\n",
        "            if len(self.input_ids) != len(self.attention_mask) or len(self.input_ids) != len(self.labels):\n",
        "                raise ValueError(f\"Length mismatch: input_ids ({len(self.input_ids)}), attention_mask ({len(self.attention_mask)}), labels ({len(self.labels)})\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize SentimentDataset: {e}. Using empty dataset.\")\n",
        "            self.input_ids = torch.empty(0)\n",
        "            self.attention_mask = torch.ones(0, 128)\n",
        "            self.labels = torch.empty(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "\n",
        "# Check if preprocessing is needed\n",
        "try:\n",
        "    if len(input_ids) == 0 or len(attention_mask) == 0 or len(labels) == 0:\n",
        "        # Validate df\n",
        "        if 'df' not in globals() or df.empty:\n",
        "            raise ValueError(\"DataFrame 'df' is empty or not loaded.\")\n",
        "\n",
        "        # Load column names from user_dataset_prompt.json or infer dynamically\n",
        "        project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "        prompt_file = os.path.join(project_dir, 'user_dataset_prompt.json')\n",
        "        if os.path.exists(prompt_file):\n",
        "            with open(prompt_file, 'r') as f:\n",
        "                prompt_data = json.load(f)\n",
        "            text_column = prompt_data.get('text_column', 'review')\n",
        "            label_column = prompt_data.get('label_column', 'sentiment')\n",
        "            logger.info(\"Loaded columns from prompt - Text: %s, Label: %s\", text_column, label_column)\n",
        "        else:\n",
        "            # Infer columns dynamically\n",
        "            text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]\n",
        "            label_cols = [col for col in df.columns if df[col].dtype in ['object', 'int', 'float'] and df[col].nunique() < len(df) / 10]\n",
        "            if not text_cols or not label_cols:\n",
        "                raise ValueError(\"Could not infer text or label columns. Ensure dataset contains text and categorical label columns.\")\n",
        "            text_column, label_column = text_cols[0], label_cols[0]\n",
        "            logger.info(\"Inferred columns - Text: %s, Label: %s\", text_column, label_column)\n",
        "\n",
        "        # Validate columns exist\n",
        "        if text_column not in df.columns or label_column not in df.columns:\n",
        "            raise ValueError(f\"Missing inferred columns: {text_column} or {label_column}.\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        try:\n",
        "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load tokenizer: {e}. Using dummy data.\")\n",
        "            raise\n",
        "\n",
        "        # Tokenize reviews\n",
        "        reviews = list(df[text_column].fillna(''))\n",
        "        if not reviews:\n",
        "            raise ValueError(\"No reviews to tokenize after filling NaN values.\")\n",
        "        encodings = tokenizer(reviews, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "        # Convert to NumPy\n",
        "        input_ids = encodings['input_ids'].numpy()\n",
        "        attention_mask = encodings['attention_mask'].numpy()\n",
        "\n",
        "        # Dynamically map labels to integers\n",
        "        unique_labels = df[label_column].unique()\n",
        "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        labels = df[label_column].map(label_mapping).fillna(0).astype(int).values\n",
        "        logger.info(\"Dynamic label mapping: %s\", label_mapping)\n",
        "\n",
        "        # Validate shapes\n",
        "        if len(input_ids) != len(labels):\n",
        "            raise ValueError(f\"Shape mismatch: input_ids ({len(input_ids)}), labels ({len(labels)})\")\n",
        "    else:\n",
        "        logger.info(\"Preprocessed data already loaded; skipping fallback preprocessing.\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Preprocessing failed: {e}. Using dummy data.\")\n",
        "    input_ids = np.zeros((100, 128), dtype=np.int32)\n",
        "    attention_mask = np.ones((100, 128), dtype=np.int32)\n",
        "    labels = np.random.randint(0, 2, 100)\n",
        "\n",
        "# Create dataset and loader\n",
        "try:\n",
        "    dataset = SentimentDataset(input_ids, attention_mask, labels)\n",
        "    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "    logger.info(f\"Dataset created with {len(dataset)} samples.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to create DataLoader: {e}. Using empty dataset.\")\n",
        "    dataset = SentimentDataset(None, None, None)\n",
        "    loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "Rv1GOyBDBwj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Model Selector Agent with PPO\n",
        "# Purpose: Dynamically select the best sentiment analysis approach using reinforcement learning.\n",
        "\n",
        "class ModelSelectionEnv(gym.Env):\n",
        "    def __init__(self, dataset_size, intent, gpu_available, target_accuracy, available_memory, text_complexity):\n",
        "        super(ModelSelectionEnv, self).__init__()\n",
        "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
        "        self.dataset_size = max(1, dataset_size)\n",
        "        self.intent = intent if intent in ['performance', 'efficiency'] else 'performance'\n",
        "        self.gpu_available = bool(gpu_available)\n",
        "        self.target_accuracy = min(1.0, max(0.0, target_accuracy))\n",
        "        self.available_memory = min(12.0, max(0.1, available_memory)) / 12.0\n",
        "        self.text_complexity = min(1.0, max(0.0, text_complexity))\n",
        "        self.state = self._get_state()\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 50\n",
        "\n",
        "    def _get_state(self):\n",
        "        memory = psutil.virtual_memory()\n",
        "        return np.array([\n",
        "            self.dataset_size / 100000,\n",
        "            1.0 if self.intent == 'performance' else 0.0,\n",
        "            1.0 if self.gpu_available else 0.0,\n",
        "            self.target_accuracy,\n",
        "            self.available_memory,\n",
        "            self.text_complexity,\n",
        "            memory.used / (1024 ** 3) / 12.0,\n",
        "            psutil.cpu_percent() / 100.0\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, **kwargs):\n",
        "        self.step_count = 0\n",
        "        self.state = self._get_state()\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        model_config = self._action_to_model_config(action)\n",
        "        reward = self._compute_reward(action, model_config)\n",
        "        done = self.step_count >= self.max_steps\n",
        "        info = {'model_config': model_config}\n",
        "        return self.state, reward, done, done, info\n",
        "\n",
        "    def _action_to_model_config(self, action):\n",
        "        architecture_type, computational_cost, learning_capacity, data_fit, pretraining_level, \\\n",
        "        ensemble_weight, latency_sensitivity, regularization_strength = action\n",
        "\n",
        "        if architecture_type < 0.1:  # Rule-Based\n",
        "            approach = \"Rule-Based (Pattern/Syntactic)\"\n",
        "            memory_usage = 0.2\n",
        "            inference_speed = 0.1\n",
        "            training_time = 0.0\n",
        "            accuracy_potential = 0.70 + data_fit * 0.1\n",
        "            robustness = 0.6\n",
        "            scalability = 0.8\n",
        "            latency = 0.05\n",
        "        elif architecture_type < 0.2:  # Traditional ML (Naive Bayes)\n",
        "            approach = \"Traditional ML (Naive Bayes-like)\"\n",
        "            memory_usage = 0.3\n",
        "            inference_speed = 0.2\n",
        "            training_time = 0.5\n",
        "            accuracy_potential = 0.78 + data_fit * 0.15\n",
        "            robustness = 0.7\n",
        "            scalability = 0.6\n",
        "            latency = 0.1\n",
        "        elif architecture_type < 0.3:  # Traditional ML (Logistic Regression)\n",
        "            approach = \"Traditional ML (Logistic Regression-like)\"\n",
        "            memory_usage = 0.5\n",
        "            inference_speed = 0.3\n",
        "            training_time = 0.6\n",
        "            accuracy_potential = 0.80 + data_fit * 0.15\n",
        "            robustness = 0.7\n",
        "            scalability = 0.6\n",
        "            latency = 0.15\n",
        "        elif architecture_type < 0.4:  # Traditional ML (SVM)\n",
        "            approach = \"Traditional ML (SVM-like)\"\n",
        "            memory_usage = 0.7\n",
        "            inference_speed = 0.4\n",
        "            training_time = 1.0\n",
        "            accuracy_potential = 0.82 + data_fit * 0.15\n",
        "            robustness = 0.7\n",
        "            scalability = 0.6\n",
        "            latency = 0.2\n",
        "        elif architecture_type < 0.5:  # Traditional ML (Gradient Boosting)\n",
        "            approach = \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\"\n",
        "            memory_usage = 0.8\n",
        "            inference_speed = 0.5\n",
        "            training_time = 1.2\n",
        "            accuracy_potential = 0.83 + data_fit * 0.15\n",
        "            robustness = 0.7\n",
        "            scalability = 0.6\n",
        "            latency = 0.25\n",
        "        elif architecture_type < 0.6:  # Traditional ML (Random Forest)\n",
        "            approach = \"Traditional ML (Random Forest-like)\"\n",
        "            memory_usage = 0.6\n",
        "            inference_speed = 0.4\n",
        "            training_time = 0.9\n",
        "            accuracy_potential = 0.81 + data_fit * 0.15\n",
        "            robustness = 0.7\n",
        "            scalability = 0.6\n",
        "            latency = 0.2\n",
        "        elif architecture_type < 0.65:  # Traditional ML (Decision Tree)\n",
        "            approach = \"Traditional ML (Decision Tree-like)\"\n",
        "            memory_usage = 0.4\n",
        "            inference_speed = 0.3\n",
        "            training_time = 0.7\n",
        "            accuracy_potential = 0.75 + data_fit * 0.15\n",
        "            robustness = 0.6\n",
        "            scalability = 0.7\n",
        "            latency = 0.15\n",
        "        elif architecture_type < 0.7:  # Traditional ML (K-Nearest Neighbors)\n",
        "            approach = \"Traditional ML (K-Nearest Neighbors-like)\"\n",
        "            memory_usage = 0.5\n",
        "            inference_speed = 0.35\n",
        "            training_time = 0.8\n",
        "            accuracy_potential = 0.77 + data_fit * 0.15\n",
        "            robustness = 0.65\n",
        "            scalability = 0.65\n",
        "            latency = 0.18\n",
        "        elif architecture_type < 0.75:  # Traditional ML (AdaBoost)\n",
        "            approach = \"Traditional ML (AdaBoost-like)\"\n",
        "            memory_usage = 0.6\n",
        "            inference_speed = 0.45\n",
        "            training_time = 1.0\n",
        "            accuracy_potential = 0.80 + data_fit * 0.15\n",
        "            robustness = 0.7\n",
        "            scalability = 0.6\n",
        "            latency = 0.22\n",
        "        elif architecture_type < 0.8:  # Traditional ML (LightGBM)\n",
        "            approach = \"Traditional ML (LightGBM-like)\"\n",
        "            memory_usage = 0.9\n",
        "            inference_speed = 0.55\n",
        "            training_time = 1.3\n",
        "            accuracy_potential = 0.84 + data_fit * 0.15\n",
        "            robustness = 0.75\n",
        "            scalability = 0.55\n",
        "            latency = 0.25\n",
        "        elif architecture_type < 0.85:  # Traditional ML (CatBoost)\n",
        "            approach = \"Traditional ML (CatBoost-like)\"\n",
        "            memory_usage = 1.0\n",
        "            inference_speed = 0.6\n",
        "            training_time = 1.4\n",
        "            accuracy_potential = 0.85 + data_fit * 0.15\n",
        "            robustness = 0.75\n",
        "            scalability = 0.55\n",
        "            latency = 0.28\n",
        "        elif architecture_type < 0.875:  # Shallow Neural Network\n",
        "            approach = \"Shallow Neural Network (MLP-like)\"\n",
        "            memory_usage = 1.0\n",
        "            inference_speed = 0.5\n",
        "            training_time = 1.5\n",
        "            accuracy_potential = 0.85 + data_fit * 0.1 + pretraining_level * 0.05\n",
        "            robustness = 0.8\n",
        "            scalability = 0.5\n",
        "            latency = 0.3\n",
        "        elif architecture_type < 0.9:  # Recurrent Neural Network\n",
        "            approach = \"Recurrent Neural Network (LSTM/GRU-like)\"\n",
        "            memory_usage = 2.0\n",
        "            inference_speed = 1.0\n",
        "            training_time = 2.0\n",
        "            accuracy_potential = 0.87 + data_fit * 0.1 + pretraining_level * 0.05\n",
        "            robustness = 0.8\n",
        "            scalability = 0.5\n",
        "            latency = 0.5\n",
        "        elif architecture_type < 0.925:  # Convolutional Neural Network\n",
        "            approach = \"Convolutional Neural Network (CNN-like)\"\n",
        "            memory_usage = 1.5\n",
        "            inference_speed = 0.8\n",
        "            training_time = 1.8\n",
        "            accuracy_potential = 0.86 + data_fit * 0.1 + pretraining_level * 0.05\n",
        "            robustness = 0.8\n",
        "            scalability = 0.5\n",
        "            latency = 0.4\n",
        "        elif architecture_type < 0.95:  # Bidirectional LSTM\n",
        "            approach = \"Bidirectional LSTM (BiLSTM-like)\"\n",
        "            memory_usage = 2.5\n",
        "            inference_speed = 1.2\n",
        "            training_time = 2.2\n",
        "            accuracy_potential = 0.88 + data_fit * 0.1 + pretraining_level * 0.05\n",
        "            robustness = 0.85\n",
        "            scalability = 0.45\n",
        "            latency = 0.6\n",
        "        elif architecture_type < 0.9625:  # Gated Recurrent Unit\n",
        "            approach = \"Gated Recurrent Unit (GRU-like)\"\n",
        "            memory_usage = 2.0\n",
        "            inference_speed = 1.0\n",
        "            training_time = 2.0\n",
        "            accuracy_potential = 0.87 + data_fit * 0.1 + pretraining_level * 0.05\n",
        "            robustness = 0.8\n",
        "            scalability = 0.5\n",
        "            latency = 0.5\n",
        "        elif architecture_type < 0.975:  # Feedforward Neural Network\n",
        "            approach = \"Feedforward Neural Network (FNN-like)\"\n",
        "            memory_usage = 1.2\n",
        "            inference_speed = 0.6\n",
        "            training_time = 1.6\n",
        "            accuracy_potential = 0.84 + data_fit * 0.1 + pretraining_level * 0.05\n",
        "            robustness = 0.75\n",
        "            scalability = 0.55\n",
        "            latency = 0.35\n",
        "        elif architecture_type < 0.9875:  # Hybrid (CNN-RNN)\n",
        "            approach = \"Hybrid (CNN-RNN)\"\n",
        "            memory_usage = 4.0\n",
        "            inference_speed = 2.0\n",
        "            training_time = 3.0\n",
        "            accuracy_potential = 0.90 + data_fit * 0.08\n",
        "            robustness = 0.85\n",
        "            scalability = 0.4\n",
        "            latency = 0.8\n",
        "        else:  # Transformer-Based Models\n",
        "            if pretraining_level < 0.3:\n",
        "                approach = \"Deep Learning (Custom Transformer)\"\n",
        "                memory_usage = 5.0\n",
        "                inference_speed = 2.5\n",
        "                training_time = 5.0\n",
        "                accuracy_potential = 0.92 + data_fit * 0.06\n",
        "                robustness = 0.9\n",
        "                scalability = 0.3\n",
        "                latency = 1.0\n",
        "            elif pretraining_level < 0.5:\n",
        "                approach = \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\"\n",
        "                memory_usage = 2.0\n",
        "                inference_speed = 1.5\n",
        "                training_time = 2.0\n",
        "                accuracy_potential = 0.90 + data_fit * 0.08 + pretraining_level * 0.05\n",
        "                robustness = 0.85\n",
        "                scalability = 0.4\n",
        "                latency = 0.6\n",
        "            elif pretraining_level < 0.7:\n",
        "                approach = \"BERT (Bidirectional Encoder Representations from Transformers)\"\n",
        "                memory_usage = 3.0\n",
        "                inference_speed = 1.8\n",
        "                training_time = 2.5\n",
        "                accuracy_potential = 0.93 + data_fit * 0.06 + pretraining_level * 0.04\n",
        "                robustness = 0.9\n",
        "                scalability = 0.35\n",
        "                latency = 0.7\n",
        "            elif pretraining_level < 0.8:\n",
        "                approach = \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\"\n",
        "                memory_usage = 3.5\n",
        "                inference_speed = 2.0\n",
        "                training_time = 2.8\n",
        "                accuracy_potential = 0.94 + data_fit * 0.06 + pretraining_level * 0.04\n",
        "                robustness = 0.9\n",
        "                scalability = 0.35\n",
        "                latency = 0.75\n",
        "            elif pretraining_level < 0.85:\n",
        "                approach = \"ALBERT (A Lite BERT)\"\n",
        "                memory_usage = 2.5\n",
        "                inference_speed = 1.6\n",
        "                training_time = 2.2\n",
        "                accuracy_potential = 0.91 + data_fit * 0.07 + pretraining_level * 0.04\n",
        "                robustness = 0.85\n",
        "                scalability = 0.4\n",
        "                latency = 0.65\n",
        "            elif pretraining_level < 0.9:\n",
        "                approach = \"XLNet (Generalized Autoregressive Pretraining)\"\n",
        "                memory_usage = 4.0\n",
        "                inference_speed = 2.2\n",
        "                training_time = 3.0\n",
        "                accuracy_potential = 0.95 + data_fit * 0.05 + pretraining_level * 0.04\n",
        "                robustness = 0.9\n",
        "                scalability = 0.3\n",
        "                latency = 0.9\n",
        "            elif pretraining_level < 0.925:\n",
        "                approach = \"T5 (Text-To-Text Transfer Transformer)\"\n",
        "                memory_usage = 5.5\n",
        "                inference_speed = 2.5\n",
        "                training_time = 3.5\n",
        "                accuracy_potential = 0.94 + data_fit * 0.06 + pretraining_level * 0.04\n",
        "                robustness = 0.9\n",
        "                scalability = 0.3\n",
        "                latency = 1.0\n",
        "            elif pretraining_level < 0.95:\n",
        "                approach = \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\"\n",
        "                memory_usage = 3.5\n",
        "                inference_speed = 2.0\n",
        "                training_time = 2.8\n",
        "                accuracy_potential = 0.95 + data_fit * 0.05 + pretraining_level * 0.04\n",
        "                robustness = 0.9\n",
        "                scalability = 0.35\n",
        "                latency = 0.75\n",
        "            elif pretraining_level < 0.975:\n",
        "                approach = \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\"\n",
        "                memory_usage = 3.0\n",
        "                inference_speed = 1.9\n",
        "                training_time = 2.6\n",
        "                accuracy_potential = 0.93 + data_fit * 0.06 + pretraining_level * 0.04\n",
        "                robustness = 0.9\n",
        "                scalability = 0.35\n",
        "                latency = 0.7\n",
        "            elif pretraining_level < 0.9875:\n",
        "                approach = \"Longformer (for long documents)\"\n",
        "                memory_usage = 6.0\n",
        "                inference_speed = 2.8\n",
        "                training_time = 4.0\n",
        "                accuracy_potential = 0.96 + data_fit * 0.05 + pretraining_level * 0.03\n",
        "                robustness = 0.9\n",
        "                scalability = 0.25\n",
        "                latency = 1.2\n",
        "            else:\n",
        "                approach = \"BigBird (sparse attention for long sequences)\"\n",
        "                memory_usage = 6.5\n",
        "                inference_speed = 3.0\n",
        "                training_time = 4.5\n",
        "                accuracy_potential = 0.97 + data_fit * 0.04 + pretraining_level * 0.03\n",
        "                robustness = 0.9\n",
        "                scalability = 0.25\n",
        "                latency = 1.3\n",
        "\n",
        "        # Lexicon fallback (no training, fixed reward)\n",
        "        if approach.startswith(\"Lexicon\"):\n",
        "            approach = \"Lexicon-Based (e.g., Pattern-like)\"\n",
        "            memory_usage = 0.1\n",
        "            inference_speed = 0.05\n",
        "            training_time = 0.0\n",
        "            accuracy_potential = 0.65\n",
        "            robustness = 0.5\n",
        "            scalability = 0.9\n",
        "            latency = 0.03\n",
        "            return {\n",
        "                \"approach\": approach, \"computational_cost\": 0.1, \"learning_capacity\": 0.0,\n",
        "                \"data_fit\": 0.0, \"pretraining_level\": 0.0, \"ensemble_weight\": 0.0,\n",
        "                \"latency_sensitivity\": 0.0, \"regularization_strength\": 0.0,\n",
        "                \"memory_usage\": memory_usage, \"inference_speed\": inference_speed,\n",
        "                \"training_time\": training_time, \"accuracy_potential\": accuracy_potential,\n",
        "                \"robustness\": robustness, \"scalability\": scalability, \"latency\": latency\n",
        "            }\n",
        "\n",
        "        # Resource and performance adjustments\n",
        "        if memory_usage > self.available_memory * 12.0 and not self.gpu_available:\n",
        "            memory_usage = self.available_memory * 12.0 * 0.9\n",
        "            inference_speed *= 1.5\n",
        "            training_time *= 1.2\n",
        "            accuracy_potential *= 0.95\n",
        "\n",
        "        return {\n",
        "            \"approach\": approach, \"computational_cost\": computational_cost,\n",
        "            \"learning_capacity\": learning_capacity, \"data_fit\": data_fit,\n",
        "            \"pretraining_level\": pretraining_level, \"ensemble_weight\": ensemble_weight,\n",
        "            \"latency_sensitivity\": latency_sensitivity, \"regularization_strength\": regularization_strength,\n",
        "            \"memory_usage\": memory_usage, \"inference_speed\": inference_speed,\n",
        "            \"training_time\": training_time, \"accuracy_potential\": accuracy_potential,\n",
        "            \"robustness\": robustness, \"scalability\": scalability, \"latency\": latency\n",
        "        }\n",
        "\n",
        "    def _compute_reward(self, action, model_config):\n",
        "        architecture_type, computational_cost, learning_capacity, data_fit, pretraining_level, \\\n",
        "        ensemble_weight, latency_sensitivity, regularization_strength = action\n",
        "        memory_usage = model_config[\"memory_usage\"]\n",
        "        inference_speed = model_config[\"inference_speed\"]\n",
        "        training_time = model_config[\"training_time\"]\n",
        "        accuracy_potential = model_config[\"accuracy_potential\"]\n",
        "        robustness = model_config[\"robustness\"]\n",
        "        scalability = model_config[\"scalability\"]\n",
        "        latency = model_config[\"latency\"]\n",
        "\n",
        "        # Multi-objective reward function with adaptive weights\n",
        "        weight_accuracy = 0.5 if self.intent == 'performance' else 0.3\n",
        "        weight_efficiency = 0.3 if self.intent == 'performance' else 0.5\n",
        "        weight_robustness = 0.1\n",
        "        weight_scalability = 0.05\n",
        "        weight_latency = 0.05\n",
        "\n",
        "        # Base accuracy reward, adjusted for dataset size and complexity\n",
        "        base_accuracy_reward = accuracy_potential * weight_accuracy\n",
        "        if self.dataset_size < 100:\n",
        "            base_accuracy_reward *= (1.0 + self.text_complexity * 0.2)\n",
        "        elif self.dataset_size < 1000:\n",
        "            base_accuracy_reward *= (1.0 + learning_capacity * 0.3 - computational_cost * 0.2)\n",
        "        elif self.dataset_size < 10000:\n",
        "            base_accuracy_reward *= (1.0 + pretraining_level * 0.2 + data_fit * 0.2)\n",
        "        else:\n",
        "            base_accuracy_reward *= (1.0 + pretraining_level * 0.3 + ensemble_weight * 0.1)\n",
        "\n",
        "        # Efficiency reward (computational cost, memory, training time)\n",
        "        efficiency_reward = (1.0 - computational_cost) * 0.4 / (memory_usage + 1e-6) * 0.3 / (training_time + 1e-6) * weight_efficiency\n",
        "\n",
        "        # Robustness and scalability reward\n",
        "        robustness_reward = robustness * weight_robustness\n",
        "        scalability_reward = scalability * weight_scalability\n",
        "\n",
        "        # Latency reward\n",
        "        latency_reward = (1.0 - latency_sensitivity * latency) * weight_latency\n",
        "\n",
        "        # Regularization penalty to prevent overfitting\n",
        "        regularization_penalty = -regularization_strength * 0.1 if accuracy_potential > 0.95 else 0.0\n",
        "\n",
        "        # Penalties\n",
        "        memory_penalty = -5.0 if memory_usage > self.available_memory * 12.0 else 0.0\n",
        "        runtime_penalty = -2.0 if inference_speed > 2.0 and not self.gpu_available else 0.0\n",
        "        training_penalty = -2.0 if training_time > 3.0 and self.intent == 'efficiency' else 0.0\n",
        "        complexity_mismatch = -3.0 if (self.text_complexity < 0.3 and computational_cost > 0.7) else 0.0\n",
        "        simplicity_mismatch = -3.0 if (self.text_complexity > 0.7 and computational_cost < 0.3) else 0.0\n",
        "\n",
        "        # Generalization potential bonus\n",
        "        generalization_bonus = 0.1 * (1.0 - abs(self.target_accuracy - accuracy_potential)) if robustness > 0.7 else 0.0\n",
        "\n",
        "        # Total reward\n",
        "        reward = (base_accuracy_reward + efficiency_reward + robustness_reward +\n",
        "                  scalability_reward + latency_reward + generalization_bonus +\n",
        "                  regularization_penalty + memory_penalty + runtime_penalty +\n",
        "                  training_penalty + complexity_mismatch + simplicity_mismatch)\n",
        "\n",
        "        return np.clip(reward, -10.0, 10.0)\n",
        "\n",
        "# Initialize environment and PPO agent\n",
        "env = DummyVecEnv([lambda: ModelSelectionEnv(len(df) if 'df' in globals() else 1000,\n",
        "                                            intent, gpu_available, target_accuracy,\n",
        "                                            psutil.virtual_memory().available / (1024 ** 3),\n",
        "                                            (sum(len(r.split()) for r in df['review']) / len(df) / 50 +\n",
        "                                             len(set(\" \".join(df['review']).split())) / 10000) / 2 if 'df' in globals() else 0.5)])\n",
        "ppo_model = PPO('MlpPolicy', env, device='cpu', verbose=0, learning_rate=0.0001, batch_size=128, clip_range=0.2, ent_coef=0.01)\n",
        "ppo_model.learn(total_timesteps=10000, progress_bar=False)\n",
        "\n",
        "# Select initial model configuration\n",
        "obs = env.reset()\n",
        "best_reward = float('-inf')\n",
        "best_config = None\n",
        "for i in tqdm(range(20), desc=\"Initial Model Selection Trials\"):\n",
        "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if reward > best_reward:\n",
        "        best_reward = reward\n",
        "        best_config = info[0]['model_config']\n",
        "selected_config = best_config\n",
        "ray.get(coordinator.log_message.remote(f\"Initial Selected Model: {selected_config['approach']}\"))\n",
        "print(f\"Initial Selected Model: {selected_config['approach']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMCUSWZ4hq1I",
        "outputId": "05e58f6e-87b6-4348-a247-fa3ec99f5652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initial Model Selection Trials: 100%|| 20/20 [00:00<00:00, 357.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Selected Model: Rule-Based (Pattern/Syntactic)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: MAML Pre-Training\n",
        "# Purpose: Pre-train MAML on benchmark datasets for deep learning models.\n",
        "\n",
        "class BenchmarkDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, num_labels=len(set(labels)) if 'labels' in globals() else 2):\n",
        "        self.input_ids = torch.randint(0, 30000, (num_samples, 128))\n",
        "        self.attention_mask = torch.ones(num_samples, 128, dtype=torch.long)\n",
        "        self.labels = torch.randint(0, num_labels, (num_samples,))\n",
        "\n",
        "    def __len__(self): return len(self.labels)\n",
        "    def __getitem__(self, idx): return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "\n",
        "class MAML:\n",
        "    def __init__(self, model, lr_inner=0.01, lr_outer=0.001):\n",
        "        self.model = model\n",
        "        self.lr_inner = lr_inner\n",
        "        self.lr_outer = lr_outer\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
        "\n",
        "    def inner_update(self, inputs, labels, params=None):\n",
        "        model = self.model\n",
        "        if params: model.load_state_dict(params)\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "        return {name: param - self.lr_inner * grad for (name, param), grad in zip(model.named_parameters(), grads)}\n",
        "\n",
        "    def meta_train(self, tasks, num_iterations=100):\n",
        "        self.model.to(device)\n",
        "        for iteration in tqdm(range(num_iterations), desc=\"MAML Pre-Training\"):\n",
        "            meta_loss = 0\n",
        "            for task in tasks:\n",
        "                loader = DataLoader(task, batch_size=16, shuffle=True)\n",
        "                support_batch = next(iter(loader))\n",
        "                query_batch = next(iter(loader))\n",
        "                support_inputs = {k: v.to(device) for k, v in support_batch.items() if k != 'labels'}\n",
        "                support_labels = support_batch['labels'].to(device)\n",
        "                query_inputs = {k: v.to(device) for k, v in query_batch.items() if k != 'labels'}\n",
        "                query_labels = query_batch['labels'].to(device)\n",
        "                updated_params = self.inner_update(support_inputs, support_labels)\n",
        "                self.model.load_state_dict(updated_params)\n",
        "                query_outputs = self.model(**query_inputs, labels=query_labels)\n",
        "                meta_loss += query_outputs.loss\n",
        "            self.optimizer.zero_grad()\n",
        "            meta_loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return self.model.state_dict()\n",
        "\n",
        "maml_model = None\n",
        "if any(s in (selected_config.get('approach', 'performance')) for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                \"Hybrid\", \"Deep Learning\"]):\n",
        "    from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        "    model_map = {\n",
        "        \"Shallow Neural Network (MLP-like)\": DistilBertForSequenceClassification,\n",
        "        \"Recurrent Neural Network (LSTM/GRU-like)\": DistilBertForSequenceClassification,\n",
        "        \"Convolutional Neural Network (CNN-like)\": DistilBertForSequenceClassification,\n",
        "        \"Bidirectional LSTM (BiLSTM-like)\": BertForSequenceClassification,\n",
        "        \"Gated Recurrent Unit (GRU-like)\": BertForSequenceClassification,\n",
        "        \"Feedforward Neural Network (FNN-like)\": DistilBertForSequenceClassification,\n",
        "        \"Hybrid (CNN-RNN)\": BertForSequenceClassification,\n",
        "        \"Deep Learning (Custom Transformer)\": BertForSequenceClassification,\n",
        "        \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": DistilBertForSequenceClassification,\n",
        "        \"BERT (Bidirectional Encoder Representations from Transformers)\": BertForSequenceClassification,\n",
        "        \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": RobertaForSequenceClassification,\n",
        "        \"ALBERT (A Lite BERT)\": DistilBertForSequenceClassification,\n",
        "        \"XLNet (Generalized Autoregressive Pretraining)\": BertForSequenceClassification,\n",
        "        \"T5 (Text-To-Text Transfer Transformer)\": BertForSequenceClassification,\n",
        "        \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": BertForSequenceClassification,\n",
        "        \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": BertForSequenceClassification,\n",
        "        \"Longformer (for long documents)\": BertForSequenceClassification,\n",
        "        \"BigBird (sparse attention for long sequences)\": BertForSequenceClassification,\n",
        "        \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": RobertaForSequenceClassification\n",
        "    }\n",
        "    model_class = model_map.get(selected_config.get('model', 'Shallow Neural Network (MLP-like)'), DistilBertForSequenceClassification)\n",
        "    # Dynamically set num_labels based on unique labels in the dataset\n",
        "    num_labels = len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2\n",
        "    maml_model = model_class.from_pretrained(model_class.pretrained_model_name_or_path if hasattr(model_class, 'pretrained_model_name_or_path') else 'distilbert-base-uncased', num_labels=num_labels)\n",
        "    if torch.cuda.is_available(): maml_model.cuda()\n",
        "    tasks = [BenchmarkDataset() for _ in range(3)]\n",
        "    maml = MAML(maml_model)\n",
        "    meta_params = maml.meta_train(tasks)\n",
        "    drive_path = '/content/drive/MyDrive'\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    meta_params_path = os.path.join(drive_path, 'meta_learned_params.pt')\n",
        "    torch.save(meta_params, meta_params_path)\n",
        "    ray.get(coordinator.log_message.remote(f\"Meta-learned parameters saved to {meta_params_path}\"))\n",
        "    logger.info(f\"Meta-learned parameters saved to {meta_params_path}\")\n",
        "else:\n",
        "    meta_params_path = None\n",
        "    # Use a fallback model name if selected_config['model'] is missing\n",
        "    default_model = selected_config.get('model', 'Unknown Model')\n",
        "    ray.get(coordinator.log_message.remote(f\"Skipping MAML pre-training for {default_model} (non-deep learning model).\"))\n",
        "    logger.info(f\"Skipping MAML pre-training for {default_model} (non-deep learning model).\")"
      ],
      "metadata": {
        "id": "3_PaNN_Yhq9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Fine-Tune with MAML\n",
        "# Purpose: Fine-tune the selected model on the target dataset.\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = torch.tensor(input_ids, dtype=torch.long) if input_ids is not None else torch.empty(0)\n",
        "        self.attention_mask = torch.tensor(attention_mask, dtype=torch.long) if attention_mask is not None else torch.ones(0, 128)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else torch.empty(0)\n",
        "    def __len__(self): return len(self.labels)\n",
        "    def __getitem__(self, idx): return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "\n",
        "try:\n",
        "    dataset = SentimentDataset(input_ids, attention_mask, labels)\n",
        "except Exception:\n",
        "    logger.warning(\"Using empty dataset due to missing inputs.\")\n",
        "    dataset = SentimentDataset(None, None, None)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "maml_model = None\n",
        "# Ensure selected_config has a default value if not defined\n",
        "if 'selected_config' not in globals():\n",
        "    selected_config = {\n",
        "        'approach': 'Deep Learning',\n",
        "        'model': 'Shallow Neural Network (MLP-like)',\n",
        "        'hyperparams': {'learning_rate': 2e-5, 'batch_size': 16}\n",
        "    }\n",
        "    logger.warning(\"selected_config not found. Initialized with default values: %s\", selected_config)\n",
        "\n",
        "if any(s in selected_config.get('approach', 'Deep Learning') for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                \"Hybrid\", \"Deep Learning\"]):\n",
        "    from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        "    model_map = {\n",
        "        \"Shallow Neural Network (MLP-like)\": DistilBertForSequenceClassification,\n",
        "        \"Recurrent Neural Network (LSTM/GRU-like)\": DistilBertForSequenceClassification,\n",
        "        \"Convolutional Neural Network (CNN-like)\": DistilBertForSequenceClassification,\n",
        "        \"Bidirectional LSTM (BiLSTM-like)\": BertForSequenceClassification,\n",
        "        \"Gated Recurrent Unit (GRU-like)\": BertForSequenceClassification,\n",
        "        \"Feedforward Neural Network (FNN-like)\": DistilBertForSequenceClassification,\n",
        "        \"Hybrid (CNN-RNN)\": BertForSequenceClassification,\n",
        "        \"Deep Learning (Custom Transformer)\": BertForSequenceClassification,\n",
        "        \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": DistilBertForSequenceClassification,\n",
        "        \"BERT (Bidirectional Encoder Representations from Transformers)\": BertForSequenceClassification,\n",
        "        \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": RobertaForSequenceClassification,\n",
        "        \"ALBERT (A Lite BERT)\": DistilBertForSequenceClassification,\n",
        "        \"XLNet (Generalized Autoregressive Pretraining)\": BertForSequenceClassification,\n",
        "        \"T5 (Text-To-Text Transfer Transformer)\": BertForSequenceClassification,\n",
        "        \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": BertForSequenceClassification,\n",
        "        \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": BertForSequenceClassification,\n",
        "        \"Longformer (for long documents)\": BertForSequenceClassification,\n",
        "        \"BigBird (sparse attention for long sequences)\": BertForSequenceClassification,\n",
        "        \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": RobertaForSequenceClassification\n",
        "    }\n",
        "    model_class = model_map.get(selected_config.get('model', 'Shallow Neural Network (MLP-like)'), DistilBertForSequenceClassification)\n",
        "    # Dynamically set num_labels based on unique labels in the dataset\n",
        "    num_labels = len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2\n",
        "    maml_model = model_class.from_pretrained(model_class.pretrained_model_name_or_path if hasattr(model_class, 'pretrained_model_name_or_path') else 'distilbert-base-uncased', num_labels=num_labels)\n",
        "    maml_model.to(device)\n",
        "    meta_params_path = '/content/drive/MyDrive/meta_learned_params.pt'\n",
        "    if os.path.exists(meta_params_path):\n",
        "        try: maml_model.load_state_dict(torch.load(meta_params_path))\n",
        "        except: logger.warning(f\"Failed to load meta parameters. Using pre-trained weights.\")\n",
        "    if len(dataset) > 0:\n",
        "        maml = MAML(maml_model)\n",
        "        ray.get(coordinator.log_message.remote(\"Starting MAML fine-tuning...\"))\n",
        "        logger.info(\"Starting MAML fine-tuning...\")\n",
        "        optimizer = AdamW(maml_model.parameters(), lr=2e-5)\n",
        "        maml_model.train()\n",
        "        for epoch in tqdm(range(2), desc=\"Fine-Tuning Epochs\"):\n",
        "            for batch in loader:\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                labels_batch = batch['labels'].to(device)\n",
        "                updated_params = maml.inner_update(inputs, labels_batch)\n",
        "                maml_model.load_state_dict(updated_params)\n",
        "                outputs = maml_model(**inputs, labels=labels_batch)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "        drive_path = '/content/drive/MyDrive'\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "        fine_tuned_path = os.path.join(drive_path, f'fine_tuned_{selected_config.get(\"model\", \"default_model\").replace(\" \", \"_\").lower()}.pt')\n",
        "        torch.save(maml_model.state_dict(), fine_tuned_path)\n",
        "        ray.get(coordinator.log_message.remote(f\"Fine-tuned model saved to {fine_tuned_path}\"))\n",
        "        logger.info(f\"Fine-tuned model saved to {fine_tuned_path}\")\n",
        "    else:\n",
        "        ray.get(coordinator.log_message.remote(\"No fine-tuning performed (empty dataset).\"))\n",
        "        logger.info(\"No fine-tuning performed (empty dataset).\")\n",
        "elif \"Traditional ML\" in selected_config.get('approach', ''):\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "    ml_model_map = {\n",
        "        \"Traditional ML (Naive Bayes-like)\": None,  # Placeholder for MultinomialNB\n",
        "        \"Traditional ML (Logistic Regression-like)\": LogisticRegression(max_iter=1000),\n",
        "        \"Traditional ML (SVM-like)\": SVC(probability=True),\n",
        "        \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\": GradientBoostingClassifier(),\n",
        "        \"Traditional ML (Random Forest-like)\": RandomForestClassifier(),\n",
        "        \"Traditional ML (Decision Tree-like)\": None,  # Placeholder for DecisionTreeClassifier\n",
        "        \"Traditional ML (K-Nearest Neighbors-like)\": None,  # Placeholder for KNeighborsClassifier\n",
        "        \"Traditional ML (AdaBoost-like)\": None,  # Placeholder for AdaBoostClassifier\n",
        "        \"Traditional ML (LightGBM-like)\": None,  # Placeholder for LGBMClassifier\n",
        "        \"Traditional ML (CatBoost-like)\": None  # Placeholder for CatBoostClassifier\n",
        "    }\n",
        "    ml_model = ml_model_map.get(selected_config.get('model', 'Traditional ML (Logistic Regression-like)'))\n",
        "    if ml_model is not None:\n",
        "        # Load column names dynamically\n",
        "        project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "        prompt_file = os.path.join(project_dir, 'user_dataset_prompt.json')\n",
        "        if os.path.exists(prompt_file):\n",
        "            with open(prompt_file, 'r') as f:\n",
        "                prompt_data = json.load(f)\n",
        "            text_column = prompt_data.get('text_column', 'review')\n",
        "            label_column = prompt_data.get('label_column', 'sentiment')\n",
        "        else:\n",
        "            text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]\n",
        "            label_cols = [col for col in df.columns if df[col].dtype in ['object', 'int', 'float'] and df[col].nunique() < len(df) / 10]\n",
        "            text_column, label_column = text_cols[0], label_cols[0]\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X = vectorizer.fit_transform(df[text_column].fillna(''))\n",
        "        # Dynamically map labels\n",
        "        unique_labels = df[label_column].unique()\n",
        "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        y = df[label_column].map(label_mapping).fillna(0).astype(int)\n",
        "        ml_model.fit(X, y)\n",
        "        fine_tuned_path = os.path.join('/content/drive/MyDrive', f'fine_tuned_{selected_config.get(\"model\", \"logistic_regression\").replace(\" \", \"_\").lower()}.joblib')\n",
        "        joblib.dump(ml_model, fine_tuned_path)\n",
        "        ray.get(coordinator.log_message.remote(f\"Fine-tuned ML model saved to {fine_tuned_path}\"))\n",
        "        logger.info(f\"Fine-tuned ML model saved to {fine_tuned_path}\")\n",
        "    else:\n",
        "        model_name = selected_config.get('model', 'Unknown Model')\n",
        "        ray.get(coordinator.log_message.remote(f\"Skipping fine-tuning for {model_name} (model not implemented).\"))\n",
        "        logger.info(f\"Skipping fine-tuning for {model_name} (model not implemented).\")\n",
        "else:\n",
        "    model_name = selected_config.get('model', 'Unknown Model')\n",
        "    approach_name = selected_config.get('approach', 'Unknown Approach')\n",
        "    ray.get(coordinator.log_message.remote(f\"Skipping fine-tuning for unknown approach {approach_name} with model {model_name}\"))\n",
        "    logger.info(f\"Skipping fine-tuning for unknown approach {approach_name} with model {model_name}\")"
      ],
      "metadata": {
        "id": "f3ZUQ2ArmO99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUlY6-zJP9PG",
        "outputId": "11a951f7-fd4d-4fec-d91f-1f1996af9962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Model Switching and Iterative Training\n",
        "# Purpose: Switch to alternative models if target accuracy is not met and retrain, with memory optimization.\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from collections import deque\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import psutil\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Function to monitor and log memory usage\n",
        "def log_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    mem_info = process.memory_info()\n",
        "    ram_usage_mb = mem_info.rss / 1024 ** 2\n",
        "    logger.info(f\"Current RAM usage: {ram_usage_mb:.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.memory_allocated() / 1024 ** 2\n",
        "        logger.info(f\"Current GPU memory usage: {gpu_mem:.2f} MB\")\n",
        "    return ram_usage_mb\n",
        "\n",
        "# SentimentDataset definition\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        try:\n",
        "            if input_ids is None or attention_mask is None or labels is None:\n",
        "                raise ValueError(\"Input data (input_ids, attention_mask, labels) cannot be None\")\n",
        "            self.input_ids = torch.tensor(input_ids, dtype=torch.long) if input_ids is not None else torch.empty(0)\n",
        "            self.attention_mask = torch.tensor(attention_mask, dtype=torch.long) if attention_mask is not None else torch.ones(0, 128)\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else torch.empty(0)\n",
        "            if len(self.input_ids) != len(self.attention_mask) or len(self.input_ids) != len(self.labels):\n",
        "                raise ValueError(f\"Length mismatch: input_ids ({len(self.input_ids)}), attention_mask ({len(self.attention_mask)}), labels ({len(self.labels)})\")\n",
        "            logger.info(f\"SentimentDataset initialized with {len(self.labels)} samples\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize SentimentDataset: {e}. Using empty dataset.\")\n",
        "            self.input_ids = torch.empty(0)\n",
        "            self.attention_mask = torch.ones(0, 128)\n",
        "            self.labels = torch.empty(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in __getitem__ at index {idx}: {e}\")\n",
        "            raise\n",
        "\n",
        "# Recreate loader if not defined\n",
        "if 'loader' not in globals() or loader is None:\n",
        "    try:\n",
        "        logger.info(\"Loader not found. Recreating from input_ids, attention_mask, and labels.\")\n",
        "        if 'input_ids' not in globals() or 'attention_mask' not in globals() or 'labels' not in globals():\n",
        "            raise NameError(\"Required data (input_ids, attention_mask, labels) not found in globals. Please run earlier cells (e.g., Cell 6).\")\n",
        "        dataset = SentimentDataset(input_ids, attention_mask, labels)\n",
        "        if len(dataset) == 0:\n",
        "            raise ValueError(\"Dataset is empty after initialization.\")\n",
        "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "        logger.info(f\"Recreated loader with {len(dataset)} samples.\")\n",
        "        first_batch = next(iter(loader), None)\n",
        "        if first_batch is None:\n",
        "            raise ValueError(\"Loader is empty; no batches available.\")\n",
        "        logger.info(f\"First batch shapes: input_ids={first_batch['input_ids'].shape}, attention_mask={first_batch['attention_mask'].shape}, labels={first_batch['labels'].shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to recreate loader: {e}. Using empty loader.\")\n",
        "        dataset = SentimentDataset(None, None, None)\n",
        "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Limit dataset size for memory efficiency\n",
        "if len(loader.dataset) > 1000:\n",
        "    subset_indices = torch.randperm(len(loader.dataset))[:1000]\n",
        "    subset_dataset = Subset(loader.dataset, subset_indices)\n",
        "    loader = DataLoader(subset_dataset, batch_size=8, shuffle=True)\n",
        "    logger.info(f\"Dataset size capped at 1000 samples to reduce memory usage.\")\n",
        "else:\n",
        "    loader = DataLoader(loader.dataset, batch_size=8, shuffle=True)\n",
        "    logger.info(f\"Using full dataset with {len(loader.dataset)} samples.\")\n",
        "\n",
        "# Define missing variables with fallbacks\n",
        "if 'accuracy' not in globals():\n",
        "    accuracy = 0.0\n",
        "    logger.warning(\"accuracy not found. Defaulting to 0.0.\")\n",
        "if 'selected_config' not in globals():\n",
        "    selected_config = {\n",
        "        'approach': 'performance',\n",
        "        'model': 'Shallow Neural Network (MLP-like)',\n",
        "        'hyperparams': {'learning_rate': (1e-5, 5e-5), 'batch_size': (8, 32)}\n",
        "    }\n",
        "    logger.warning(\"selected_config not found. Defaulting to Shallow Neural Network (MLP-like) with default hyperparameters.\")\n",
        "else:\n",
        "    if 'model' not in selected_config and 'approach' in selected_config:\n",
        "        current_approach = selected_config['approach']\n",
        "        if 'Traditional ML' in current_approach:\n",
        "            selected_config['model'] = 'Traditional ML (Logistic Regression-like)'\n",
        "            selected_config['approach'] = 'Traditional ML'\n",
        "        elif 'Deep Learning' in current_approach:\n",
        "            selected_config['model'] = 'Shallow Neural Network (MLP-like)'\n",
        "            selected_config['approach'] = 'Deep Learning'\n",
        "        else:\n",
        "            selected_config['model'] = 'Hybrid (CNN-RNN)'\n",
        "            selected_config['approach'] = 'Hybrid'\n",
        "        logger.info(f\"Inferred 'model' from existing 'approach'. Updated selected_config: {selected_config}\")\n",
        "if 'target_accuracy' not in globals():\n",
        "    target_accuracy = 0.85\n",
        "    logger.warning(\"target_accuracy not found. Defaulting to 0.85.\")\n",
        "if 'model_priority' not in globals():\n",
        "    model_priority = deque([\n",
        "        \"Traditional ML (Logistic Regression-like)\",\n",
        "        \"Traditional ML (Random Forest-like)\",\n",
        "        \"Shallow Neural Network (MLP-like)\",\n",
        "        \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\",\n",
        "        \"BERT (Bidirectional Encoder Representations from Transformers)\",\n",
        "        \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\",\n",
        "        \"ALBERT (A Lite BERT)\",\n",
        "        \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\",\n",
        "        \"Traditional ML (SVM-like)\",\n",
        "        \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\",\n",
        "        \"Recurrent Neural Network (LSTM/GRU-like)\",\n",
        "        \"Convolutional Neural Network (CNN-like)\",\n",
        "        \"Bidirectional LSTM (BiLSTM-like)\",\n",
        "        \"Gated Recurrent Unit (GRU-like)\",\n",
        "        \"Feedforward Neural Network (FNN-like)\",\n",
        "        \"Hybrid (CNN-RNN)\",\n",
        "        \"Deep Learning (Custom Transformer)\",\n",
        "        \"XLNet (Generalized Autoregressive Pretraining)\",\n",
        "        \"T5 (Text-To-Text Transfer Transformer)\",\n",
        "        \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\",\n",
        "        \"Longformer (for long documents)\",\n",
        "        \"BigBird (sparse attention for long sequences)\",\n",
        "        \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\",\n",
        "        \"Traditional ML (Naive Bayes-like)\",\n",
        "        \"Traditional ML (Decision Tree-like)\",\n",
        "        \"Traditional ML (K-Nearest Neighbors-like)\",\n",
        "        \"Traditional ML (AdaBoost-like)\",\n",
        "        \"Traditional ML (LightGBM-like)\",\n",
        "        \"Traditional ML (CatBoost-like)\"\n",
        "    ])\n",
        "    logger.info(\"model_priority not found. Initialized with default model list.\")\n",
        "\n",
        "# Validate selected_config\n",
        "logger.info(f\"Initial selected_config: {selected_config}\")\n",
        "if not isinstance(selected_config, dict):\n",
        "    logger.error(f\"selected_config is not a dictionary: {selected_config}\")\n",
        "    raise ValueError(\"selected_config must be a dictionary with 'approach' and 'model' keys.\")\n",
        "required_keys = ['approach', 'model']\n",
        "for key in required_keys:\n",
        "    if key not in selected_config:\n",
        "        logger.error(f\"selected_config missing required key: '{key}'. Current selected_config: {selected_config}\")\n",
        "        raise ValueError(f\"selected_config must contain '{key}' key. Please set it in an earlier cell.\")\n",
        "\n",
        "# Validate 'approach' and 'model' values\n",
        "valid_approaches = ['Traditional ML', 'Deep Learning', 'Hybrid']\n",
        "valid_models = [\n",
        "    \"Shallow Neural Network (MLP-like)\", \"Recurrent Neural Network (LSTM/GRU-like)\", \"Convolutional Neural Network (CNN-like)\",\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\", \"Gated Recurrent Unit (GRU-like)\", \"Feedforward Neural Network (FNN-like)\",\n",
        "    \"Hybrid (CNN-RNN)\", \"Deep Learning (Custom Transformer)\",\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\",\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\",\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\", \"ALBERT (A Lite BERT)\",\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\", \"T5 (Text-To-Text Transfer Transformer)\",\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\",\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\",\n",
        "    \"Longformer (for long documents)\", \"BigBird (sparse attention for long sequences)\",\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\",\n",
        "    \"Traditional ML (Naive Bayes-like)\", \"Traditional ML (Logistic Regression-like)\", \"Traditional ML (SVM-like)\",\n",
        "    \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\", \"Traditional ML (Random Forest-like)\",\n",
        "    \"Traditional ML (Decision Tree-like)\", \"Traditional ML (K-Nearest Neighbors-like)\",\n",
        "    \"Traditional ML (AdaBoost-like)\", \"Traditional ML (LightGBM-like)\", \"Traditional ML (CatBoost-like)\"\n",
        "]\n",
        "if selected_config['model'] not in valid_models:\n",
        "    logger.warning(f\"Invalid 'model' in selected_config: {selected_config['model']}. Must be one of {valid_models}.\")\n",
        "    if 'Traditional ML' in selected_config['approach']:\n",
        "        selected_config['model'] = 'Traditional ML (Logistic Regression-like)'\n",
        "    elif 'Deep Learning' in selected_config['approach']:\n",
        "        selected_config['model'] = 'Shallow Neural Network (MLP-like)'\n",
        "    else:\n",
        "        selected_config['model'] = 'Hybrid (CNN-RNN)'\n",
        "    logger.info(f\"Set 'model' to fallback: {selected_config['model']}\")\n",
        "if selected_config['approach'] not in valid_approaches:\n",
        "    logger.error(f\"Invalid 'approach' in selected_config: {selected_config['approach']}. Must be one of {valid_approaches}.\")\n",
        "    raise ValueError(f\"selected_config['approach'] must be one of {valid_approaches}.\")\n",
        "\n",
        "# Add default hyperparameters if missing\n",
        "if 'hyperparams' not in selected_config:\n",
        "    if 'Traditional ML' in selected_config['approach']:\n",
        "        selected_config['hyperparams'] = {\n",
        "            'n_estimators': (50, 200),\n",
        "            'max_depth': (10, 30, None),\n",
        "            'min_samples_split': (2, 10)\n",
        "        }\n",
        "    else:  # Deep Learning or Hybrid\n",
        "        selected_config['hyperparams'] = {\n",
        "            'learning_rate': (1e-5, 5e-5),\n",
        "            'batch_size': (8, 32)\n",
        "        }\n",
        "    logger.info(f\"Added default hyperparameters: {selected_config['hyperparams']}\")\n",
        "\n",
        "# Save selected_config to a checkpoint file\n",
        "checkpoint_path = '/content/drive/MyDrive/selected_config_checkpoint.pkl'\n",
        "try:\n",
        "    import pickle\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        pickle.dump(selected_config, f)\n",
        "    logger.info(f\"Saved selected_config to {checkpoint_path}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Failed to save selected_config checkpoint: {str(e)}\")\n",
        "\n",
        "best_accuracy = accuracy\n",
        "best_config = selected_config.copy()\n",
        "max_trials = 3\n",
        "trials = 0\n",
        "ram_threshold_mb = 10240\n",
        "\n",
        "def log_message(msg):\n",
        "    logger.info(msg)\n",
        "\n",
        "while accuracy < target_accuracy and trials < max_trials and model_priority:\n",
        "    current_ram = log_memory_usage()\n",
        "    if current_ram > ram_threshold_mb:\n",
        "        logger.warning(f\"RAM usage ({current_ram:.2f} MB) exceeds threshold ({ram_threshold_mb} MB). Stopping trials.\")\n",
        "        break\n",
        "\n",
        "    next_approach = model_priority.popleft()\n",
        "    log_message(f\"Switching to {next_approach} for trial {trials + 1}\")\n",
        "    selected_config['approach'] = 'Traditional ML' if 'Traditional ML' in next_approach else 'Deep Learning' if 'Deep Learning' in next_approach else 'Hybrid'\n",
        "    selected_config['model'] = next_approach\n",
        "\n",
        "    maml_model = None\n",
        "    ml_model = None\n",
        "    vectorizer = None\n",
        "\n",
        "    if any(s in next_approach for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                       \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                       \"Hybrid\", \"Deep Learning\"]):\n",
        "        from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        "        model_map = {\n",
        "            \"Shallow Neural Network (MLP-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "            \"Recurrent Neural Network (LSTM/GRU-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "            \"Convolutional Neural Network (CNN-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "            \"Bidirectional LSTM (BiLSTM-like)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "            \"Gated Recurrent Unit (GRU-like)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "            \"Feedforward Neural Network (FNN-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "            \"Hybrid (CNN-RNN)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "            \"Deep Learning (Custom Transformer)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "            \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "            \"BERT (Bidirectional Encoder Representations from Transformers)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "            \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": (RobertaForSequenceClassification, 'roberta-base'),\n",
        "            \"ALBERT (A Lite BERT)\": (DistilBertForSequenceClassification, 'albert-base-v2'),\n",
        "            \"XLNet (Generalized Autoregressive Pretraining)\": (BertForSequenceClassification, 'xlnet-base-cased'),\n",
        "            \"T5 (Text-To-Text Transfer Transformer)\": (BertForSequenceClassification, 't5-small'),\n",
        "            \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": (BertForSequenceClassification, 'deberta-base'),\n",
        "            \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": (BertForSequenceClassification, 'electra-base-discriminator'),\n",
        "            \"Longformer (for long documents)\": (BertForSequenceClassification, 'longformer-base-4096'),\n",
        "            \"BigBird (sparse attention for long sequences)\": (BertForSequenceClassification, 'google/bigbird-roberta-base'),\n",
        "            \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": (RobertaForSequenceClassification, 'roberta-base')\n",
        "        }\n",
        "        model_class, pretrained_model = model_map.get(next_approach, (DistilBertForSequenceClassification, 'distilbert-base-uncased'))\n",
        "        try:\n",
        "            maml_model = model_class.from_pretrained(pretrained_model, num_labels=len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2)\n",
        "            maml_model.to(device)\n",
        "            meta_params_path = '/content/drive/MyDrive/meta_learned_params.pt'\n",
        "            if os.path.exists(meta_params_path):\n",
        "                try:\n",
        "                    maml_model.load_state_dict(torch.load(meta_params_path))\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Failed to load meta parameters: {e}. Using pre-trained weights.\")\n",
        "            maml = MAML(maml_model)\n",
        "            maml_model.train()\n",
        "            for epoch in tqdm(range(1), desc=f\"Fine-Tuning {next_approach}\"):\n",
        "                for batch in loader:\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels_batch = batch['labels'].to(device)\n",
        "                    updated_params = maml.inner_update(inputs, labels_batch)\n",
        "                    maml_model.load_state_dict(updated_params)\n",
        "                    torch.cuda.empty_cache()\n",
        "            fine_tuned_path = os.path.join('/content/drive/MyDrive', f'fine_tuned_{next_approach.replace(\" \", \"_\").lower()}.pt')\n",
        "            torch.save(maml_model.state_dict(), fine_tuned_path)\n",
        "            log_message(f\"Fine-tuned model saved to {fine_tuned_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to train {next_approach}: {e}. Skipping to next model.\")\n",
        "            continue\n",
        "        finally:\n",
        "            del maml_model\n",
        "            del maml\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    elif \"Traditional ML\" in next_approach:\n",
        "        ml_model_map = {\n",
        "            \"Traditional ML (Naive Bayes-like)\": None,\n",
        "            \"Traditional ML (Logistic Regression-like)\": LogisticRegression(max_iter=1000),\n",
        "            \"Traditional ML (SVM-like)\": SVC(probability=True),\n",
        "            \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\": GradientBoostingClassifier(),\n",
        "            \"Traditional ML (Random Forest-like)\": RandomForestClassifier(),\n",
        "            \"Traditional ML (Decision Tree-like)\": None,\n",
        "            \"Traditional ML (K-Nearest Neighbors-like)\": None,\n",
        "            \"Traditional ML (AdaBoost-like)\": None,\n",
        "            \"Traditional ML (LightGBM-like)\": None,\n",
        "            \"Traditional ML (CatBoost-like)\": None\n",
        "        }\n",
        "        ml_model = ml_model_map.get(next_approach)\n",
        "        if ml_model is not None:\n",
        "            # Load column names dynamically\n",
        "            project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "            prompt_file = os.path.join(project_dir, 'user_dataset_prompt.json')\n",
        "            if os.path.exists(prompt_file):\n",
        "                with open(prompt_file, 'r') as f:\n",
        "                    prompt_data = json.load(f)\n",
        "                text_column = prompt_data.get('text_column', 'review')\n",
        "                label_column = prompt_data.get('label_column', 'sentiment')\n",
        "            else:\n",
        "                text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]\n",
        "                label_cols = [col for col in df.columns if df[col].dtype in ['object', 'int', 'float'] and df[col].nunique() < len(df) / 10]\n",
        "                text_column, label_column = text_cols[0], label_cols[0]\n",
        "\n",
        "            vectorizer = TfidfVectorizer(max_features=5000)\n",
        "            X = vectorizer.fit_transform(df[text_column].fillna(''))\n",
        "            # Dynamically map labels\n",
        "            unique_labels = df[label_column].unique()\n",
        "            label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "            y = df[label_column].map(label_mapping).fillna(0).astype(int)\n",
        "            ml_model.fit(X, y)\n",
        "            fine_tuned_path = os.path.join('/content/drive/MyDrive', f'fine_tuned_{next_approach.replace(\" \", \"_\").lower()}.joblib')\n",
        "            joblib.dump(ml_model, fine_tuned_path)\n",
        "            log_message(f\"Fine-tuned ML model saved to {fine_tuned_path}\")\n",
        "        else:\n",
        "            log_message(f\"Skipping fine-tuning for {next_approach} (model not implemented).\")\n",
        "            ml_model = None\n",
        "\n",
        "    # Re-evaluate\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    if any(s in next_approach for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                       \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                       \"Hybrid\", \"Deep Learning\"]):\n",
        "        try:\n",
        "            maml_model = model_class.from_pretrained(pretrained_model, num_labels=len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2)\n",
        "            maml_model.to(device)\n",
        "            if os.path.exists(fine_tuned_path):\n",
        "                maml_model.load_state_dict(torch.load(fine_tuned_path))\n",
        "            maml_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(loader, desc=f\"Evaluating {next_approach}\"):\n",
        "                    if len(batch['input_ids']) == 0:\n",
        "                        log_message(\"Empty batch in loader. Skipping evaluation.\")\n",
        "                        break\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels_batch = batch['labels'].to(device)\n",
        "                    outputs = maml_model(**inputs)\n",
        "                    logits = outputs.logits\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    predictions.extend(preds.cpu().numpy())\n",
        "                    true_labels.extend(labels_batch.cpu().numpy())\n",
        "                    torch.cuda.empty_cache()\n",
        "            accuracy = accuracy_score(true_labels, predictions) if predictions else 0.0\n",
        "            f1 = f1_score(true_labels, predictions, average='weighted') if predictions else 0.0\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed for {next_approach}: {e}. Setting accuracy to 0.\")\n",
        "            accuracy = 0.0\n",
        "            f1 = 0.0\n",
        "        finally:\n",
        "            del maml_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    elif \"Traditional ML\" in next_approach and ml_model is not None:\n",
        "        try:\n",
        "            X_test = vectorizer.transform(df[text_column].fillna(''))\n",
        "            predictions = ml_model.predict(X_test)\n",
        "            true_labels = y\n",
        "            accuracy = accuracy_score(true_labels, predictions)\n",
        "            f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed for {next_approach}: {e}. Setting accuracy to 0.\")\n",
        "            accuracy = 0.0\n",
        "            f1 = 0.0\n",
        "        finally:\n",
        "            del X_test\n",
        "            gc.collect()\n",
        "    else:\n",
        "        accuracy = 0.0\n",
        "        f1 = 0.0\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_config = selected_config.copy()\n",
        "        log_message(f\"New best model: {next_approach} with accuracy {best_accuracy:.3f}\")\n",
        "    trials += 1\n",
        "\n",
        "# Finalize selected_config\n",
        "if best_accuracy > accuracy:\n",
        "    selected_config = best_config\n",
        "    accuracy = best_accuracy\n",
        "    log_message(f\"Final selected model switched to {selected_config['model']} with best accuracy {best_accuracy:.3f}\")\n",
        "else:\n",
        "    log_message(f\"No better model found. Sticking with initial {selected_config['model']} (accuracy: {accuracy:.3f})\")\n",
        "\n",
        "# Save updated selected_config to checkpoint\n",
        "try:\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        pickle.dump(selected_config, f)\n",
        "    logger.info(f\"Updated selected_config saved to {checkpoint_path}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Failed to update selected_config checkpoint: {str(e)}\")\n",
        "\n",
        "# Final memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "log_memory_usage()\n",
        "\n",
        "# Optional: Print final chosen model configuration\n",
        "print(\"\\nFinal Model Configuration:\")\n",
        "print(f\"Approach: {selected_config['approach']}\")\n",
        "print(f\"Model: {selected_config['model']}\")\n",
        "print(f\"Best Accuracy Achieved: {best_accuracy:.3f}\")\n",
        "print(f\"Target Accuracy: {target_accuracy:.3f}\")\n",
        "print(f\"Target Achieved: {best_accuracy >= target_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsUVdbL5hrH8",
        "outputId": "81a20928-ec1c-4f1b-faaa-85be628bf94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:accuracy not found. Defaulting to 0.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Configuration:\n",
            "Approach: Traditional ML\n",
            "Model: Traditional ML (Random Forest-like)\n",
            "Best Accuracy Achieved: 1.000\n",
            "Target Accuracy: 0.920\n",
            "Target Achieved: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Model Switching and Iterative Training with Hyperparameter Optimization\n",
        "# Purpose: Switch to alternative models if target accuracy is not met, optimize hyperparameters using Bayesian Optimization, and retrain with memory optimization.\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from collections import deque\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import psutil\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define project directory\n",
        "project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# Function to monitor and log memory usage\n",
        "def log_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    mem_info = process.memory_info()\n",
        "    ram_usage_mb = mem_info.rss / 1024 ** 2\n",
        "    logger.info(f\"Current RAM usage: {ram_usage_mb:.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.memory_allocated() / 1024 ** 2\n",
        "        logger.info(f\"Current GPU memory usage: {gpu_mem:.2f} MB\")\n",
        "    return ram_usage_mb\n",
        "\n",
        "# SentimentDataset definition\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        try:\n",
        "            if input_ids is None or attention_mask is None or labels is None:\n",
        "                raise ValueError(\"Input data (input_ids, attention_mask, labels) cannot be None\")\n",
        "            self.input_ids = torch.tensor(input_ids, dtype=torch.long) if input_ids is not None else torch.empty(0)\n",
        "            self.attention_mask = torch.tensor(attention_mask, dtype=torch.long) if attention_mask is not None else torch.ones(0, 128)\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else torch.empty(0)\n",
        "            if len(self.input_ids) != len(self.attention_mask) or len(self.input_ids) != len(self.labels):\n",
        "                raise ValueError(f\"Length mismatch: input_ids ({len(self.input_ids)}), attention_mask ({len(self.attention_mask)}), labels ({len(self.labels)})\")\n",
        "            logger.info(f\"SentimentDataset initialized with {len(self.labels)} samples\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize SentimentDataset: {e}. Using empty dataset.\")\n",
        "            self.input_ids = torch.empty(0)\n",
        "            self.attention_mask = torch.ones(0, 128)\n",
        "            self.labels = torch.empty(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in __getitem__ at index {idx}: {e}\")\n",
        "            raise\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    dataset_path = os.path.join(project_dir, 'processed_dataset.csv')\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    logger.info(f\"Loaded dataset from {dataset_path} with {len(df)} samples.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load dataset from {dataset_path}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Recreate loader if not defined\n",
        "if 'loader' not in globals() or loader is None:\n",
        "    try:\n",
        "        logger.info(\"Loader not found. Recreating from input_ids, attention_mask, and labels.\")\n",
        "        if 'input_ids' not in globals() or 'attention_mask' not in globals() or 'labels' in globals():\n",
        "            raise NameError(\"Required data (input_ids, attention_mask, labels) not found in globals. Please run earlier cells (e.g., Cell 2).\")\n",
        "        dataset = SentimentDataset(input_ids, attention_mask, labels)\n",
        "        if len(dataset) == 0:\n",
        "            raise ValueError(\"Dataset is empty after initialization.\")\n",
        "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "        logger.info(f\"Recreated loader with {len(dataset)} samples.\")\n",
        "        first_batch = next(iter(loader), None)\n",
        "        if first_batch is None:\n",
        "            raise ValueError(\"Loader is empty; no batches available.\")\n",
        "        logger.info(f\"First batch shapes: input_ids={first_batch['input_ids'].shape}, attention_mask={first_batch['attention_mask'].shape}, labels={first_batch['labels'].shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to recreate loader: {e}. Using empty loader.\")\n",
        "        dataset = SentimentDataset(None, None, None)\n",
        "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Limit dataset size for memory efficiency\n",
        "if len(loader.dataset) > 1000:\n",
        "    subset_indices = torch.randperm(len(loader.dataset))[:1000]\n",
        "    subset_dataset = Subset(loader.dataset, subset_indices)\n",
        "    loader = DataLoader(subset_dataset, batch_size=8, shuffle=True)\n",
        "    logger.info(f\"Dataset size capped at 1000 samples to reduce memory usage.\")\n",
        "else:\n",
        "    loader = DataLoader(loader.dataset, batch_size=8, shuffle=True)\n",
        "    logger.info(f\"Using full dataset with {len(loader.dataset)} samples.\")\n",
        "\n",
        "# Define all possible models\n",
        "ALL_MODELS = [\n",
        "    \"Traditional ML (Logistic Regression-like)\",\n",
        "    \"Traditional ML (Random Forest-like)\",\n",
        "    \"Shallow Neural Network (MLP-like)\",\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\",\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\",\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\",\n",
        "    \"ALBERT (A Lite BERT)\",\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\",\n",
        "    \"Traditional ML (SVM-like)\",\n",
        "    \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\",\n",
        "    \"Recurrent Neural Network (LSTM/GRU-like)\",\n",
        "    \"Convolutional Neural Network (CNN-like)\",\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\",\n",
        "    \"Gated Recurrent Unit (GRU-like)\",\n",
        "    \"Feedforward Neural Network (FNN-like)\",\n",
        "    \"Hybrid (CNN-RNN)\",\n",
        "    \"Deep Learning (Custom Transformer)\",\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\",\n",
        "    \"T5 (Text-To-Text Transfer Transformer)\",\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\",\n",
        "    \"Longformer (for long documents)\",\n",
        "    \"BigBird (sparse attention for long sequences)\",\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\",\n",
        "    \"Traditional ML (Naive Bayes-like)\",\n",
        "    \"Traditional ML (Decision Tree-like)\",\n",
        "    \"Traditional ML (K-Nearest Neighbors-like)\",\n",
        "    \"Traditional ML (AdaBoost-like)\",\n",
        "    \"Traditional ML (LightGBM-like)\",\n",
        "    \"Traditional ML (CatBoost-like)\"\n",
        "]\n",
        "\n",
        "# Define missing variables with fallbacks\n",
        "if 'accuracy' not in globals():\n",
        "    accuracy = 0.0\n",
        "    logger.warning(\"accuracy not found. Defaulting to 0.0.\")\n",
        "if 'selected_config' not in globals():\n",
        "    selected_config = {\n",
        "        'approach': 'Deep Learning',\n",
        "        'model': 'Shallow Neural Network (MLP-like)',\n",
        "        'hyperparams': {}\n",
        "    }\n",
        "    logger.warning(\"selected_config not found. Initialized with placeholder.\")\n",
        "if 'target_accuracy' not in globals():\n",
        "    target_accuracy = 0.85\n",
        "    logger.warning(\"target_accuracy not found. Defaulting to 0.85.\")\n",
        "if 'model_priority' not in globals():\n",
        "    model_priority = deque(ALL_MODELS)\n",
        "    logger.info(\"model_priority not found. Initialized with default model list.\")\n",
        "\n",
        "# Validate selected_config\n",
        "logger.info(f\"Initial selected_config: {selected_config}\")\n",
        "if not isinstance(selected_config, dict):\n",
        "    logger.error(f\"selected_config is not a dictionary: {selected_config}\")\n",
        "    raise ValueError(\"selected_config must be a dictionary with 'approach' and 'model' keys.\")\n",
        "required_keys = ['approach', 'model']\n",
        "for key in required_keys:\n",
        "    if key not in selected_config:\n",
        "        logger.error(f\"selected_config missing required key: '{key}'. Current selected_config: {selected_config}\")\n",
        "        raise ValueError(f\"selected_config must contain '{key}' key. Please set it in an earlier cell.\")\n",
        "\n",
        "# Validate 'approach' and 'model' values\n",
        "valid_approaches = ['Traditional ML', 'Deep Learning', 'Hybrid']\n",
        "valid_models = ALL_MODELS\n",
        "if selected_config['approach'] not in valid_approaches:\n",
        "    logger.error(f\"Invalid 'approach' in selected_config: {selected_config['approach']}. Must be one of {valid_approaches}.\")\n",
        "    raise ValueError(f\"selected_config['approach'] must be one of {valid_approaches}.\")\n",
        "if selected_config['model'] not in valid_models:\n",
        "    logger.error(f\"Invalid 'model' in selected_config: {selected_config['model']}. Must be one of {valid_models}.\")\n",
        "    raise ValueError(f\"selected_config['model'] must be one of {valid_models}.\")\n",
        "\n",
        "# Define hyperparameter search spaces\n",
        "hyperparam_spaces = {\n",
        "    # Traditional ML Models\n",
        "    \"Traditional ML (Logistic Regression-like)\": [\n",
        "        Integer(1, 1000, name='max_iter'),\n",
        "        Real(1e-4, 1e2, name='C', prior='log-uniform')\n",
        "    ],\n",
        "    \"Traditional ML (Random Forest-like)\": [\n",
        "        Integer(10, 200, name='n_estimators'),\n",
        "        Integer(2, 50, name='max_depth'),\n",
        "        Integer(2, 10, name='min_samples_split')\n",
        "    ],\n",
        "    \"Traditional ML (SVM-like)\": [\n",
        "        Real(1e-4, 1e2, name='C', prior='log-uniform'),\n",
        "        Categorical(['linear', 'rbf'], name='kernel')\n",
        "    ],\n",
        "    \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\": [\n",
        "        Integer(10, 200, name='n_estimators'),\n",
        "        Real(1e-3, 1.0, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(1, 20, name='max_depth')\n",
        "    ],\n",
        "    \"Traditional ML (Naive Bayes-like)\": [],\n",
        "    \"Traditional ML (Decision Tree-like)\": [],\n",
        "    \"Traditional ML (K-Nearest Neighbors-like)\": [],\n",
        "    \"Traditional ML (AdaBoost-like)\": [],\n",
        "    \"Traditional ML (LightGBM-like)\": [],\n",
        "    \"Traditional ML (CatBoost-like)\": [],\n",
        "    # Deep Learning Models\n",
        "    \"Shallow Neural Network (MLP-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(1, 5, name='num_layers')\n",
        "    ],\n",
        "    \"Recurrent Neural Network (LSTM/GRU-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(50, 200, name='hidden_size')\n",
        "    ],\n",
        "    \"Convolutional Neural Network (CNN-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(16, 128, name='filters')\n",
        "    ],\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(50, 200, name='hidden_size')\n",
        "    ],\n",
        "    \"Gated Recurrent Unit (GRU-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(50, 200, name='hidden_size')\n",
        "    ],\n",
        "    \"Feedforward Neural Network (FNN-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(1, 5, name='num_layers')\n",
        "    ],\n",
        "    \"Deep Learning (Custom Transformer)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(2, 12, name='num_attention_heads')\n",
        "    ],\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"ALBERT (A Lite BERT)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"T5 (Text-To-Text Transfer Transformer)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"Longformer (for long documents)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"BigBird (sparse attention for long sequences)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size')\n",
        "    ],\n",
        "    # Hybrid Models\n",
        "    \"Hybrid (CNN-RNN)\": [\n",
        "        Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(8, 64, name='batch_size'),\n",
        "        Integer(16, 128, name='filters'),\n",
        "        Integer(50, 200, name='hidden_size')\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Define model mapping\n",
        "ml_model_map = {\n",
        "    \"Traditional ML (Naive Bayes-like)\": None,\n",
        "    \"Traditional ML (Logistic Regression-like)\": LogisticRegression,\n",
        "    \"Traditional ML (SVM-like)\": lambda **params: SVC(probability=True, **params),\n",
        "    \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\": GradientBoostingClassifier,\n",
        "    \"Traditional ML (Random Forest-like)\": RandomForestClassifier,\n",
        "    \"Traditional ML (Decision Tree-like)\": None,\n",
        "    \"Traditional ML (K-Nearest Neighbors-like)\": None,\n",
        "    \"Traditional ML (AdaBoost-like)\": None,\n",
        "    \"Traditional ML (LightGBM-like)\": None,\n",
        "    \"Traditional ML (CatBoost-like)\": None\n",
        "}\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        "model_map = {\n",
        "    \"Shallow Neural Network (MLP-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Recurrent Neural Network (LSTM/GRU-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Convolutional Neural Network (CNN-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Gated Recurrent Unit (GRU-like)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Feedforward Neural Network (FNN-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Hybrid (CNN-RNN)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Deep Learning (Custom Transformer)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": (RobertaForSequenceClassification, 'roberta-base'),\n",
        "    \"ALBERT (A Lite BERT)\": (DistilBertForSequenceClassification, 'albert-base-v2'),\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\": (BertForSequenceClassification, 'xlnet-base-cased'),\n",
        "    \"T5 (Text-To-Text Transfer Transformer)\": (BertForSequenceClassification, 't5-small'),\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": (BertForSequenceClassification, 'deberta-base'),\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": (BertForSequenceClassification, 'electra-base-discriminator'),\n",
        "    \"Longformer (for long documents)\": (BertForSequenceClassification, 'longformer-base-4096'),\n",
        "    \"BigBird (sparse attention for long sequences)\": (BertForSequenceClassification, 'google/bigbird-roberta-base'),\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": (RobertaForSequenceClassification, 'roberta-base')\n",
        "}\n",
        "\n",
        "# Prepare data for Traditional ML models with dynamic column handling\n",
        "prompt_file = os.path.join(project_dir, 'user_dataset_prompt.json')\n",
        "if os.path.exists(prompt_file):\n",
        "    with open(prompt_file, 'r') as f:\n",
        "        prompt_data = json.load(f)\n",
        "    text_column = prompt_data.get('text_column', 'review')\n",
        "    label_column = prompt_data.get('label_column', 'sentiment')\n",
        "    logger.info(\"Loaded columns from prompt - Text: %s, Label: %s\", text_column, label_column)\n",
        "else:\n",
        "    # Infer columns dynamically\n",
        "    text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]\n",
        "    label_cols = [col for col in df.columns if df[col].dtype in ['object', 'int', 'float'] and df[col].nunique() < len(df) / 10]\n",
        "    if not text_cols or not label_cols:\n",
        "        raise ValueError(\"Could not infer text or label columns. Ensure dataset contains text and categorical label columns.\")\n",
        "    text_column, label_column = text_cols[0], label_cols[0]\n",
        "    logger.info(\"Inferred columns - Text: %s, Label: %s\", text_column, label_column)\n",
        "\n",
        "# Validate columns exist\n",
        "if text_column not in df.columns or label_column not in df.columns:\n",
        "    raise ValueError(f\"Missing inferred columns: {text_column} or {label_column}.\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df[text_column].fillna(''))\n",
        "# Dynamically map labels\n",
        "unique_labels = df[label_column].unique()\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y = df[label_column].map(label_mapping).fillna(0).astype(int)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "best_accuracy = accuracy\n",
        "best_config = selected_config.copy()\n",
        "max_trials = 3\n",
        "trials = 0\n",
        "ram_threshold_mb = 10240\n",
        "\n",
        "def log_message(msg):\n",
        "    logger.info(msg)\n",
        "\n",
        "model_priority = deque(ALL_MODELS)\n",
        "logger.info(f\"model_priority reset for trials: {list(model_priority)}\")\n",
        "\n",
        "while accuracy < target_accuracy and trials < max_trials and model_priority:\n",
        "    current_ram = log_memory_usage()\n",
        "    if current_ram > ram_threshold_mb:\n",
        "        logger.warning(f\"RAM usage ({current_ram:.2f} MB) exceeds threshold ({ram_threshold_mb} MB). Stopping trials.\")\n",
        "        break\n",
        "\n",
        "    next_approach = model_priority.popleft()\n",
        "    log_message(f\"Switching to {next_approach} for trial {trials + 1}\")\n",
        "    selected_config['approach'] = 'Traditional ML' if 'Traditional ML' in next_approach else 'Deep Learning' if 'Deep Learning' in next_approach else 'Hybrid'\n",
        "    selected_config['model'] = next_approach\n",
        "\n",
        "    space = hyperparam_spaces.get(next_approach, [])\n",
        "    if not space:\n",
        "        logger.warning(f\"No hyperparameter space defined for {next_approach}. Skipping optimization.\")\n",
        "        selected_config['hyperparams'] = {}\n",
        "    else:\n",
        "        @use_named_args(space)\n",
        "        def objective(**params):\n",
        "            try:\n",
        "                if any(s in next_approach for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                   \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                   \"Hybrid\", \"Deep Learning\"]):\n",
        "                    model_class, pretrained_model = model_map.get(next_approach, (DistilBertForSequenceClassification, 'distilbert-base-uncased'))\n",
        "                    maml_model = model_class.from_pretrained(pretrained_model, num_labels=len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2)\n",
        "                    maml_model.to(device)\n",
        "                    meta_params_path = os.path.join(project_dir, 'meta_learned_params.pt')\n",
        "                    if os.path.exists(meta_params_path):\n",
        "                        try:\n",
        "                            maml_model.load_state_dict(torch.load(meta_params_path))\n",
        "                        except Exception as e:\n",
        "                            logger.warning(f\"Failed to load meta parameters: {e}. Using pre-trained weights.\")\n",
        "                    maml = MAML(maml_model)\n",
        "                    maml_model.train()\n",
        "                    for epoch in range(1):\n",
        "                        for batch in loader:\n",
        "                            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                            labels_batch = batch['labels'].to(device)\n",
        "                            learning_rate = params.get('learning_rate', 1e-5)\n",
        "                            updated_params = maml.inner_update(inputs, labels_batch, learning_rate=learning_rate)\n",
        "                            maml_model.load_state_dict(updated_params)\n",
        "                            torch.cuda.empty_cache()\n",
        "                    fine_tuned_path = os.path.join(project_dir, f'fine_tuned_{next_approach.replace(\" \", \"_\").lower()}.pt')\n",
        "                    torch.save(maml_model.state_dict(), fine_tuned_path)\n",
        "                    maml_model.eval()\n",
        "                    predictions = []\n",
        "                    true_labels = []\n",
        "                    with torch.no_grad():\n",
        "                        for batch in loader:\n",
        "                            if len(batch['input_ids']) == 0:\n",
        "                                logger.warning(\"Empty batch in loader. Skipping evaluation.\")\n",
        "                                return 1.0\n",
        "                            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                            labels_batch = batch['labels'].to(device)\n",
        "                            outputs = maml_model(**inputs)\n",
        "                            logits = outputs.logits\n",
        "                            preds = torch.argmax(logits, dim=1)\n",
        "                            predictions.extend(preds.cpu().numpy())\n",
        "                            true_labels.extend(labels_batch.cpu().numpy())\n",
        "                            torch.cuda.empty_cache()\n",
        "                    acc = accuracy_score(true_labels, predictions) if predictions else 0.0\n",
        "                    del maml_model, maml\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "                elif \"Traditional ML\" in next_approach:\n",
        "                    ml_model_class = ml_model_map.get(next_approach)\n",
        "                    if ml_model_class is None:\n",
        "                        logger.warning(f\"Model {next_approach} not implemented. Skipping.\")\n",
        "                        return 1.0\n",
        "                    ml_model = ml_model_class(**params)\n",
        "                    ml_model.fit(X_train, y_train)\n",
        "                    predictions = ml_model.predict(X_test)\n",
        "                    acc = accuracy_score(y_test, predictions)\n",
        "                else:\n",
        "                    return 1.0\n",
        "                return -acc\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Objective function failed for {next_approach} with params {params}: {e}\")\n",
        "                return 1.0\n",
        "\n",
        "        try:\n",
        "            result = gp_minimize(\n",
        "                objective,\n",
        "                space,\n",
        "                n_calls=10,\n",
        "                n_random_starts=3,\n",
        "                random_state=42,\n",
        "                verbose=True\n",
        "            )\n",
        "            optimized_params = {dim.name: result.x[i] for i, dim in enumerate(space)}\n",
        "            selected_config['hyperparams'] = optimized_params\n",
        "            logger.info(f\"Optimized hyperparameters for {next_approach}: {optimized_params}\")\n",
        "            accuracy = -result.fun\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Bayesian Optimization failed for {next_approach}: {e}\")\n",
        "            selected_config['hyperparams'] = {}\n",
        "            accuracy = 0.0\n",
        "\n",
        "    maml_model = None\n",
        "    ml_model = None\n",
        "    vectorizer = None\n",
        "\n",
        "    # Retrain with optimized hyperparameters\n",
        "    if any(s in next_approach for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                       \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                       \"Hybrid\", \"Deep Learning\"]):\n",
        "        model_class, pretrained_model = model_map.get(next_approach, (DistilBertForSequenceClassification, 'distilbert-base-uncased'))\n",
        "        try:\n",
        "            maml_model = model_class.from_pretrained(pretrained_model, num_labels=len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2)\n",
        "            maml_model.to(device)\n",
        "            meta_params_path = os.path.join(project_dir, 'meta_learned_params.pt')\n",
        "            if os.path.exists(meta_params_path):\n",
        "                try:\n",
        "                    maml_model.load_state_dict(torch.load(meta_params_path))\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Failed to load meta parameters: {e}. Using pre-trained weights.\")\n",
        "            maml = MAML(maml_model)\n",
        "            maml_model.train()\n",
        "            for epoch in tqdm(range(1), desc=f\"Fine-Tuning {next_approach}\"):\n",
        "                for batch in loader:\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels_batch = batch['labels'].to(device)\n",
        "                    learning_rate = selected_config['hyperparams'].get('learning_rate', 1e-5)\n",
        "                    updated_params = maml.inner_update(inputs, labels_batch, learning_rate=learning_rate)\n",
        "                    maml_model.load_state_dict(updated_params)\n",
        "                    torch.cuda.empty_cache()\n",
        "            fine_tuned_path = os.path.join(project_dir, f'fine_tuned_{next_approach.replace(\" \", \"_\").lower()}.pt')\n",
        "            torch.save(maml_model.state_dict(), fine_tuned_path)\n",
        "            log_message(f\"Fine-tuned model saved to {fine_tuned_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to train {next_approach}: {e}. Skipping to next model.\")\n",
        "            trials += 1\n",
        "            continue\n",
        "        finally:\n",
        "            del maml_model\n",
        "            del maml\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    elif \"Traditional ML\" in next_approach:\n",
        "        ml_model_class = ml_model_map.get(next_approach)\n",
        "        if ml_model_class is not None:\n",
        "            vectorizer = TfidfVectorizer(max_features=5000)\n",
        "            X = vectorizer.fit_transform(df[text_column].fillna(''))\n",
        "            y = df[label_column].map(label_mapping).fillna(0).astype(int)\n",
        "            ml_model = ml_model_class(**selected_config['hyperparams'])\n",
        "            ml_model.fit(X, y)\n",
        "            fine_tuned_path = os.path.join(project_dir, f'fine_tuned_{next_approach.replace(\" \", \"_\").lower()}.joblib')\n",
        "            joblib.dump(ml_model, fine_tuned_path)\n",
        "            log_message(f\"Fine-tuned ML model saved to {fine_tuned_path}\")\n",
        "        else:\n",
        "            log_message(f\"Skipping fine-tuning for {next_approach} (model not implemented).\")\n",
        "            ml_model = None\n",
        "            trials += 1\n",
        "            continue\n",
        "\n",
        "    # Re-evaluate with optimized model\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    if any(s in next_approach for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                       \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                       \"Hybrid\", \"Deep Learning\"]):\n",
        "        try:\n",
        "            maml_model = model_class.from_pretrained(pretrained_model, num_labels=len(set(labels)) if 'labels' in globals() and labels.size > 0 else 2)\n",
        "            maml_model.to(device)\n",
        "            if os.path.exists(fine_tuned_path):\n",
        "                maml_model.load_state_dict(torch.load(fine_tuned_path))\n",
        "            maml_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(loader, desc=f\"Evaluating {next_approach}\"):\n",
        "                    if len(batch['input_ids']) == 0:\n",
        "                        log_message(\"Empty batch in loader. Skipping evaluation.\")\n",
        "                        break\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels_batch = batch['labels'].to(device)\n",
        "                    outputs = maml_model(**inputs)\n",
        "                    logits = outputs.logits\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    predictions.extend(preds.cpu().numpy())\n",
        "                    true_labels.extend(labels_batch.cpu().numpy())\n",
        "                    torch.cuda.empty_cache()\n",
        "            accuracy = accuracy_score(true_labels, predictions) if predictions else 0.0\n",
        "            f1 = f1_score(true_labels, predictions, average='weighted') if predictions else 0.0\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed for {next_approach}: {e}. Setting accuracy to 0.\")\n",
        "            accuracy = 0.0\n",
        "            f1 = 0.0\n",
        "        finally:\n",
        "            del maml_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    elif \"Traditional ML\" in next_approach and ml_model is not None:\n",
        "        try:\n",
        "            X_test = vectorizer.transform(df[text_column].fillna(''))\n",
        "            predictions = ml_model.predict(X_test)\n",
        "            true_labels = y\n",
        "            accuracy = accuracy_score(true_labels, predictions)\n",
        "            f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed for {next_approach}: {e}. Setting accuracy to 0.\")\n",
        "            accuracy = 0.0\n",
        "            f1 = 0.0\n",
        "        finally:\n",
        "            del X_test\n",
        "            gc.collect()\n",
        "    else:\n",
        "        accuracy = 0.0\n",
        "        f1 = 0.0\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_config = selected_config.copy()\n",
        "        log_message(f\"New best model: {next_approach} with accuracy {best_accuracy:.3f} and F1-score {f1:.3f}\")\n",
        "    trials += 1\n",
        "\n",
        "# Finalize selected_config\n",
        "if best_accuracy > accuracy:\n",
        "    selected_config = best_config\n",
        "    accuracy = best_accuracy\n",
        "    log_message(f\"Final selected model switched to {selected_config['model']} with best accuracy {best_accuracy:.3f}\")\n",
        "else:\n",
        "    log_message(f\"No better model found. Sticking with initial {selected_config['model']} (accuracy: {accuracy:.3f})\")\n",
        "\n",
        "# Save updated selected_config to checkpoint\n",
        "checkpoint_path = os.path.join(project_dir, 'selected_config_checkpoint.pkl')\n",
        "try:\n",
        "    import pickle\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        pickle.dump(selected_config, f)\n",
        "    logger.info(f\"Updated selected_config saved to {checkpoint_path}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Failed to update selected_config checkpoint: {str(e)}\")\n",
        "\n",
        "# Final memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "log_memory_usage()\n",
        "\n",
        "# Optional: Print final chosen model configuration\n",
        "print(\"\\nFinal Model Configuration:\")\n",
        "print(f\"Approach: {selected_config['approach']}\")\n",
        "print(f\"Model: {selected_config['model']}\")\n",
        "print(f\"Hyperparameters: {selected_config['hyperparams']}\")\n",
        "print(f\"Best Accuracy Achieved: {best_accuracy:.3f}\")\n",
        "print(f\"Target Accuracy: {target_accuracy:.3f}\")\n",
        "print(f\"Target Achieved: {best_accuracy >= target_accuracy}\")"
      ],
      "metadata": {
        "id": "4F2UkLqzhrO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2038f5-6bf6-4821-a749-5ba7e2880178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Configuration:\n",
            "Approach: Traditional ML\n",
            "Model: Traditional ML (Random Forest-like)\n",
            "Hyperparameters: {'learning_rate': (1e-05, 5e-05), 'batch_size': (8, 32)}\n",
            "Best Accuracy Achieved: 1.000\n",
            "Target Accuracy: 0.920\n",
            "Target Achieved: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Final Model Evaluation and Deployment Preparation\n",
        "# Purpose: Evaluate the final selected model on a test set (or full dataset if no test set), save the model for deployment, and log the results.\n",
        "\n",
        "import torch\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define project directory\n",
        "project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# SentimentDataset definition\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        try:\n",
        "            if input_ids is None or attention_mask is None or labels is None:\n",
        "                raise ValueError(\"Input data (input_ids, attention_mask, labels) cannot be None\")\n",
        "            self.input_ids = torch.tensor(input_ids, dtype=torch.long) if input_ids is not None else torch.empty(0)\n",
        "            self.attention_mask = torch.tensor(attention_mask, dtype=torch.long) if attention_mask is not None else torch.ones(0, 128)\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else torch.empty(0)\n",
        "            if len(self.input_ids) != len(self.attention_mask) or len(self.input_ids) != len(self.labels):\n",
        "                raise ValueError(f\"Length mismatch: input_ids ({len(self.input_ids)}), attention_mask ({len(self.attention_mask)}), labels ({len(self.labels)})\")\n",
        "            logger.info(f\"SentimentDataset initialized with {len(self.labels)} samples\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize SentimentDataset: {e}. Using empty dataset.\")\n",
        "            self.input_ids = torch.empty(0)\n",
        "            self.attention_mask = torch.ones(0, 128)\n",
        "            self.labels = torch.empty(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in __getitem__ at index {idx}: {e}\")\n",
        "            raise\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    dataset_path = os.path.join(project_dir, 'processed_dataset.csv')\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    logger.info(f\"Loaded dataset from {dataset_path} with {len(df)} samples.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load dataset from {dataset_path}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load column names dynamically\n",
        "prompt_file = os.path.join(project_dir, 'user_dataset_prompt.json')\n",
        "if os.path.exists(prompt_file):\n",
        "    with open(prompt_file, 'r') as f:\n",
        "        prompt_data = json.load(f)\n",
        "    text_column = prompt_data.get('text_column', 'review')\n",
        "    label_column = prompt_data.get('label_column', 'sentiment')\n",
        "    logger.info(\"Loaded columns from prompt - Text: %s, Label: %s\", text_column, label_column)\n",
        "else:\n",
        "    # Infer columns dynamically\n",
        "    text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]\n",
        "    label_cols = [col for col in df.columns if df[col].dtype in ['object', 'int', 'float'] and df[col].nunique() < len(df) / 10]\n",
        "    if not text_cols or not label_cols:\n",
        "        raise ValueError(\"Could not infer text or label columns. Ensure dataset contains text and categorical label columns.\")\n",
        "    text_column, label_column = text_cols[0], label_cols[0]\n",
        "    logger.info(\"Inferred columns - Text: %s, Label: %s\", text_column, label_column)\n",
        "\n",
        "# Validate columns exist\n",
        "if text_column not in df.columns or label_column not in df.columns:\n",
        "    raise ValueError(f\"Missing inferred columns: {text_column} or {label_column}.\")\n",
        "\n",
        "# Dynamically map labels\n",
        "unique_labels = df[label_column].unique()\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "reverse_label_mapping = {idx: label for label, idx in label_mapping.items()}\n",
        "y = df[label_column].map(label_mapping).fillna(0).astype(int)\n",
        "\n",
        "# Load selected_config\n",
        "checkpoint_path = os.path.join(project_dir, 'selected_config_checkpoint.pkl')\n",
        "try:\n",
        "    import pickle\n",
        "    with open(checkpoint_path, 'rb') as f:\n",
        "        selected_config = pickle.load(f)\n",
        "    logger.info(f\"Loaded selected_config from {checkpoint_path}: {selected_config}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load selected_config from {checkpoint_path}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Define model mapping with valid hyperparameters\n",
        "model_map = {\n",
        "    \"Shallow Neural Network (MLP-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Recurrent Neural Network (LSTM/GRU-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Convolutional Neural Network (CNN-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Gated Recurrent Unit (GRU-like)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Feedforward Neural Network (FNN-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"Hybrid (CNN-RNN)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Deep Learning (Custom Transformer)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": (DistilBertForSequenceClassification, 'distilbert-base-uncased'),\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\": (BertForSequenceClassification, 'bert-base-uncased'),\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": (RobertaForSequenceClassification, 'roberta-base'),\n",
        "    \"ALBERT (A Lite BERT)\": (DistilBertForSequenceClassification, 'albert-base-v2'),\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\": (BertForSequenceClassification, 'xlnet-base-cased'),\n",
        "    \"T5 (Text-To-Text Transfer Transformer)\": (BertForSequenceClassification, 't5-small'),\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": (BertForSequenceClassification, 'deberta-base'),\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": (BertForSequenceClassification, 'electra-base-discriminator'),\n",
        "    \"Longformer (for long documents)\": (BertForSequenceClassification, 'longformer-base-4096'),\n",
        "    \"BigBird (sparse attention for long sequences)\": (BertForSequenceClassification, 'google/bigbird-roberta-base'),\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": (RobertaForSequenceClassification, 'roberta-base')\n",
        "}\n",
        "\n",
        "ml_model_map = {\n",
        "    \"Traditional ML (Naive Bayes-like)\": None,\n",
        "    \"Traditional ML (Logistic Regression-like)\": LogisticRegression,\n",
        "    \"Traditional ML (SVM-like)\": lambda **params: SVC(probability=True, **params),\n",
        "    \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\": GradientBoostingClassifier,\n",
        "    \"Traditional ML (Random Forest-like)\": RandomForestClassifier,\n",
        "    \"Traditional ML (Decision Tree-like)\": None,\n",
        "    \"Traditional ML (K-Nearest Neighbors-like)\": None,\n",
        "    \"Traditional ML (AdaBoost-like)\": None,\n",
        "    \"Traditional ML (LightGBM-like)\": None,\n",
        "    \"Traditional ML (CatBoost-like)\": None\n",
        "}\n",
        "\n",
        "# Define valid hyperparameters for ML models\n",
        "valid_ml_params = {\n",
        "    \"Traditional ML (Logistic Regression-like)\": ['max_iter', 'C'],\n",
        "    \"Traditional ML (SVM-like)\": ['C', 'kernel'],\n",
        "    \"Traditional ML (Gradient Boosting, e.g., XGBoost-like)\": ['n_estimators', 'learning_rate', 'max_depth'],\n",
        "    \"Traditional ML (Random Forest-like)\": ['n_estimators', 'max_depth', 'min_samples_split'],\n",
        "    \"Traditional ML (Naive Bayes-like)\": [],\n",
        "    \"Traditional ML (Decision Tree-like)\": [],\n",
        "    \"Traditional ML (K-Nearest Neighbors-like)\": [],\n",
        "    \"Traditional ML (AdaBoost-like)\": [],\n",
        "    \"Traditional ML (LightGBM-like)\": [],\n",
        "    \"Traditional ML (CatBoost-like)\": []\n",
        "}\n",
        "\n",
        "# Prepare data for evaluation\n",
        "if any(s in selected_config['approach'] for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                \"Hybrid\", \"Deep Learning\"]):\n",
        "    # Recreate loader if not defined\n",
        "    if 'loader' not in globals() or loader is None:\n",
        "        try:\n",
        "            logger.info(\"Loader not found. Recreating from input_ids, attention_mask, and labels.\")\n",
        "            if 'input_ids' not in globals() or 'attention_mask' not in globals() or 'labels' not in globals():\n",
        "                raise NameError(\"Required data (input_ids, attention_mask, labels) not found in globals. Please run earlier cells (e.g., Cell 2).\")\n",
        "            dataset = SentimentDataset(input_ids, attention_mask, labels)\n",
        "            if len(dataset) == 0:\n",
        "                raise ValueError(\"Dataset is empty after initialization.\")\n",
        "            loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
        "            logger.info(f\"Recreated loader with {len(dataset)} samples.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to recreate loader: {e}. Using empty loader.\")\n",
        "            dataset = SentimentDataset(None, None, None)\n",
        "            loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
        "else:\n",
        "    # Prepare data for Traditional ML\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(df[text_column].fillna(''))\n",
        "    y = df[label_column].map(label_mapping).fillna(0).astype(int)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    logger.info(f\"Prepared data for Traditional ML: X_train shape {X_train.shape}, X_test shape {X_test.shape}\")\n",
        "\n",
        "# Load the final model with sanitized hyperparameters\n",
        "final_model = None\n",
        "fine_tuned_path = os.path.join(project_dir, f'fine_tuned_{selected_config[\"model\"].replace(\" \", \"_\").lower()}.pt')\n",
        "ml_fine_tuned_path = os.path.join(project_dir, f'fine_tuned_{selected_config[\"model\"].replace(\" \", \"_\").lower()}.joblib')\n",
        "\n",
        "if any(s in selected_config['approach'] for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                \"Hybrid\", \"Deep Learning\"]):\n",
        "    model_class, pretrained_model = model_map.get(selected_config['model'], (DistilBertForSequenceClassification, 'distilbert-base-uncased'))\n",
        "    try:\n",
        "        final_model = model_class.from_pretrained(pretrained_model, num_labels=len(unique_labels))\n",
        "        final_model.to(device)\n",
        "        if os.path.exists(fine_tuned_path):\n",
        "            final_model.load_state_dict(torch.load(fine_tuned_path))\n",
        "            logger.info(f\"Loaded fine-tuned model from {fine_tuned_path}\")\n",
        "        else:\n",
        "            logger.warning(f\"Fine-tuned model not found at {fine_tuned_path}. Using pretrained weights.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {selected_config['model']}: {e}\")\n",
        "        raise\n",
        "elif \"Traditional ML\" in selected_config['approach']:\n",
        "    ml_model_class = ml_model_map.get(selected_config['model'])\n",
        "    if ml_model_class is not None:\n",
        "        try:\n",
        "            if os.path.exists(ml_fine_tuned_path):\n",
        "                final_model = joblib.load(ml_fine_tuned_path)\n",
        "                logger.info(f\"Loaded fine-tuned ML model from {ml_fine_tuned_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Fine-tuned ML model not found at {ml_fine_tuned_path}. Training a new model.\")\n",
        "                # Sanitize hyperparameters\n",
        "                valid_params = {k: v for k, v in selected_config.get('hyperparams', {}).items() if k in valid_ml_params.get(selected_config['model'], [])}\n",
        "                if not valid_params:\n",
        "                    valid_params = {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2}  # Default params for RandomForest\n",
        "                final_model = ml_model_class(**valid_params)\n",
        "                final_model.fit(X_train, y_train)\n",
        "                joblib.dump(final_model, ml_fine_tuned_path)\n",
        "                logger.info(f\"Trained and saved new ML model to {ml_fine_tuned_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load or train ML model {selected_config['model']}: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        logger.error(f\"Model {selected_config['model']} not implemented.\")\n",
        "        raise ValueError(f\"Model {selected_config['model']} not implemented.\")\n",
        "\n",
        "# Evaluate the final model\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "if any(s in selected_config['approach'] for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                \"Hybrid\", \"Deep Learning\"]):\n",
        "    try:\n",
        "        final_model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(loader, desc=f\"Final Evaluation of {selected_config['model']}\"):\n",
        "                if len(batch['input_ids']) == 0:\n",
        "                    logger.warning(\"Empty batch in loader. Skipping evaluation.\")\n",
        "                    break\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                labels_batch = batch['labels'].to(device)\n",
        "                outputs = final_model(**inputs)\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                predictions.extend(preds.cpu().numpy())\n",
        "                true_labels.extend(labels_batch.cpu().numpy())\n",
        "                torch.cuda.empty_cache()\n",
        "        final_accuracy = accuracy_score(true_labels, predictions) if predictions else 0.0\n",
        "        final_f1 = f1_score(true_labels, predictions, average='weighted') if predictions else 0.0\n",
        "        # Map numeric predictions back to original labels\n",
        "        predicted_labels = [reverse_label_mapping[pred] for pred in predictions]\n",
        "        true_labels_mapped = [reverse_label_mapping[label] for label in true_labels]\n",
        "        class_report = classification_report(true_labels_mapped, predicted_labels)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Final evaluation failed for {selected_config['model']}: {e}\")\n",
        "        final_accuracy = 0.0\n",
        "        final_f1 = 0.0\n",
        "        class_report = \"Evaluation failed.\"\n",
        "    finally:\n",
        "        del final_model\n",
        "        torch.cuda.empty_cache()\n",
        "elif \"Traditional ML\" in selected_config['approach']:\n",
        "    try:\n",
        "        predictions = final_model.predict(X_test)\n",
        "        true_labels = y_test\n",
        "        final_accuracy = accuracy_score(true_labels, predictions)\n",
        "        final_f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "        # Map numeric predictions back to original labels\n",
        "        predicted_labels = [reverse_label_mapping[pred] for pred in predictions]\n",
        "        true_labels_mapped = [reverse_label_mapping[label] for label in true_labels]\n",
        "        class_report = classification_report(true_labels_mapped, predicted_labels)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Final evaluation failed for {selected_config['model']}: {e}\")\n",
        "        final_accuracy = 0.0\n",
        "        final_f1 = 0.0\n",
        "        class_report = \"Evaluation failed.\"\n",
        "\n",
        "# Log and print evaluation results\n",
        "logger.info(f\"Final Evaluation Results for {selected_config['model']}:\")\n",
        "logger.info(f\"Accuracy: {final_accuracy:.3f}\")\n",
        "logger.info(f\"F1-Score (Weighted): {final_f1:.3f}\")\n",
        "logger.info(f\"Classification Report:\\n{class_report}\")\n",
        "\n",
        "print(\"\\nFinal Evaluation Results:\")\n",
        "print(f\"Model: {selected_config['model']}\")\n",
        "print(f\"Accuracy: {final_accuracy:.3f}\")\n",
        "print(f\"F1-Score (Weighted): {final_f1:.3f}\")\n",
        "print(f\"Classification Report:\\n{class_report}\")\n",
        "\n",
        "# Prepare for deployment\n",
        "deployment_path = os.path.join(project_dir, 'deploy')\n",
        "os.makedirs(deployment_path, exist_ok=True)\n",
        "\n",
        "# Save the model in a deployment-ready format\n",
        "if any(s in selected_config['approach'] for s in [\"Shallow Neural Network\", \"Recurrent Neural Network\", \"Convolutional Neural Network\",\n",
        "                                                \"Bidirectional LSTM\", \"Gated Recurrent Unit\", \"Feedforward Neural Network\",\n",
        "                                                \"Hybrid\", \"Deep Learning\"]):\n",
        "    # Reload and save the model using transformers' save_pretrained for deployment\n",
        "    try:\n",
        "        model_class, pretrained_model = model_map.get(selected_config['model'], (DistilBertForSequenceClassification, 'distilbert-base-uncased'))\n",
        "        final_model = model_class.from_pretrained(pretrained_model, num_labels=len(unique_labels))\n",
        "        if os.path.exists(fine_tuned_path):\n",
        "            final_model.load_state_dict(torch.load(fine_tuned_path))\n",
        "        final_model.save_pretrained(os.path.join(deployment_path, 'final_model'))\n",
        "        logger.info(f\"Model saved for deployment at {os.path.join(deployment_path, 'final_model')}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save model for deployment: {e}\")\n",
        "elif \"Traditional ML\" in selected_config['approach']:\n",
        "    # Save the ML model and vectorizer\n",
        "    try:\n",
        "        joblib.dump(final_model, os.path.join(deployment_path, 'final_model.joblib'))\n",
        "        joblib.dump(vectorizer, os.path.join(deployment_path, 'vectorizer.joblib'))\n",
        "        logger.info(f\"ML model and vectorizer saved for deployment at {deployment_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save ML model for deployment: {e}\")\n",
        "\n",
        "# Save label mapping for inference\n",
        "with open(os.path.join(deployment_path, 'label_mapping.json'), 'w') as f:\n",
        "    json.dump(label_mapping, f)\n",
        "logger.info(f\"Label mapping saved to {os.path.join(deployment_path, 'label_mapping.json')}\")\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results = {\n",
        "    'model': selected_config['model'],\n",
        "    'approach': selected_config['approach'],\n",
        "    'hyperparameters': selected_config['hyperparams'],\n",
        "    'accuracy': final_accuracy,\n",
        "    'f1_score': final_f1,\n",
        "    'classification_report': class_report\n",
        "}\n",
        "with open(os.path.join(deployment_path, 'evaluation_results.json'), 'w') as f:\n",
        "    json.dump(eval_results, f, indent=4)\n",
        "logger.info(f\"Evaluation results saved to {os.path.join(deployment_path, 'evaluation_results.json')}\")\n",
        "\n",
        "# Final log message\n",
        "logger.info(\"Model evaluation and deployment preparation completed successfully.\")\n",
        "print(\"Model evaluation and deployment preparation completed successfully.\")"
      ],
      "metadata": {
        "id": "GwaLBbWOhrU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03af184-cd74-4b3f-b414-d161bd04d957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Fine-tuned ML model not found at /content/drive/MyDrive/Sentiment_Project/fine_tuned_traditional_ml_(random_forest-like).joblib. Training a new model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Evaluation Results:\n",
            "Model: Traditional ML (Random Forest-like)\n",
            "Accuracy: 0.850\n",
            "F1-Score (Weighted): 0.850\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.84      0.85      4961\n",
            "    positive       0.85      0.85      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n",
            "Model evaluation and deployment preparation completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Ensure the selected_config from Cell 7 is saved to the checkpoint file.\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "\n",
        "# Set up logging (if not already set)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define project directory and checkpoint path\n",
        "project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(project_dir, 'selected_config_checkpoint.pkl')\n",
        "\n",
        "# Verify Google Drive is mounted\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    logger.error(\"Google Drive not mounted at /content/drive/MyDrive. Please mount Drive and rerun.\")\n",
        "    raise RuntimeError(\"Google Drive not mounted. Mount Drive using the Files tab in Colab.\")\n",
        "\n",
        "# Verify selected_config exists\n",
        "if 'selected_config' not in globals():\n",
        "    logger.error(\"selected_config not found in globals. Ensure Cell 7 ran successfully.\")\n",
        "    raise NameError(\"selected_config not found. Run Cell 7 to set selected_config.\")\n",
        "\n",
        "# Save selected_config to checkpoint\n",
        "try:\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        pickle.dump(selected_config, f)\n",
        "    logger.info(f\"Successfully saved selected_config to {checkpoint_path}: {selected_config}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to save selected_config to {checkpoint_path}: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Verify the file exists\n",
        "if os.path.exists(checkpoint_path):\n",
        "    logger.info(f\"Checkpoint file verified at {checkpoint_path}\")\n",
        "else:\n",
        "    logger.error(f\"Checkpoint file not found at {checkpoint_path} after saving.\")\n",
        "    raise FileNotFoundError(f\"Failed to create checkpoint file at {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "ZuqcDZAz3bRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Sentiment_Project\""
      ],
      "metadata": {
        "id": "PVlxaFBNwNNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bc5478-af14-4f80-9fda-366f669caad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " coordinator_logs.txt\n",
            " Data_Analysis_Quality_Preprocessing.ipynb\n",
            " data_analysis_report.json\n",
            " dataset_data.json\n",
            " deploy\n",
            " explainability_output\n",
            " explainability_outputs\n",
            "'fine_tuned_traditional_ml_(random_forest-like).joblib'\n",
            " hyperparams.json\n",
            " Notebook_5_CodeGen_Explainability.ipynb\n",
            " Part_1_Environment_Setup.ipynb\n",
            " Part_3_Model_Training_and_Evaluation.ipynb\n",
            " part3_output.json\n",
            " part4_output.json\n",
            " preprocessed_data.pt\n",
            " processed_dataset.csv\n",
            " quality_check_report.json\n",
            " selected_config_checkpoint.pkl\n",
            " Sentiment_Analysis_Model_Optimization.ipynb\n",
            " trained_traditional_ml_random_forest-like_encoder.joblib\n",
            " trained_traditional_ml_random_forest-like.joblib\n",
            " trained_traditional_ml_random_forest-like_vectorizer.joblib\n",
            "'train_traditional_ml_(random_forest-like).py'\n",
            " train_traditional_ml_random_forest-like.py\n",
            " user_dataset_prompt.json\n",
            " user_feedback.csv\n"
          ]
        }
      ]
    }
  ]
}