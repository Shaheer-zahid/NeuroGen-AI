{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install GPyOpt\n",
        "!pip install torch-pruning\n",
        "!pip install paramz==0.9.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2c_XUsluHtr",
        "outputId": "4390e430-8759-4c8f-a61f-18b9a983b6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: GPyOpt in /usr/local/lib/python3.11/dist-packages (1.2.6)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from GPyOpt) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.11/dist-packages (from GPyOpt) (1.12.0)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.11/dist-packages (from GPyOpt) (1.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from GPy>=1.8->GPyOpt) (1.17.0)\n",
            "Requirement already satisfied: paramz>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from GPy>=1.8->GPyOpt) (0.9.6)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.11/dist-packages (from GPy>=1.8->GPyOpt) (3.0.12)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from paramz>=0.9.6->GPy>=1.8->GPyOpt) (4.4.2)\n",
            "Requirement already satisfied: torch-pruning in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-pruning) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-pruning) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch-pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-pruning) (3.0.2)\n",
            "Requirement already satisfied: paramz==0.9.6 in /usr/local/lib/python3.11/dist-packages (0.9.6)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (1.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (1.17.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKKETqvDrpsp",
        "outputId": "bd5b312e-c8f5-4d6f-8a37-5a991b05a129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 0: Setup and Configuration Loading\n",
        "# Purpose: Set up the environment and dynamically load user-specific model configuration and dataset from Notebook 3.\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "import gc\n",
        "import json\n",
        "from google.colab import drive\n",
        "import psutil  # For memory usage monitoring\n",
        "from transformers import DistilBertTokenizer  # For fallback tokenization\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Google Drive for loading configuration and dataset\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "drive_path = '/content/drive/MyDrive/Sentiment_Project'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Function to monitor and log memory usage\n",
        "def log_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    mem_info = process.memory_info()\n",
        "    ram_usage_mb = mem_info.rss / 1024 ** 2\n",
        "    logger.info(f\"Current RAM usage: {ram_usage_mb:.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.memory_allocated() / 1024 ** 2\n",
        "        logger.info(f\"Current GPU memory usage: {gpu_mem:.2f} MB\")\n",
        "    return ram_usage_mb\n",
        "\n",
        "# SentimentDataset definition (consistent with previous cells)\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        try:\n",
        "            if input_ids is None or attention_mask is None or labels is None:\n",
        "                raise ValueError(\"Input data (input_ids, attention_mask, labels) cannot be None\")\n",
        "            self.input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "            self.attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "            if len(self.input_ids) != len(self.attention_mask) or len(self.input_ids) != len(self.labels):\n",
        "                raise ValueError(f\"Length mismatch: input_ids ({len(self.input_ids)}), attention_mask ({len(self.attention_mask)}), labels ({len(self.labels)})\")\n",
        "            logger.info(f\"SentimentDataset initialized with {len(self.labels)} samples\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize SentimentDataset: {e}. Using empty dataset.\")\n",
        "            self.input_ids = torch.empty(0)\n",
        "            self.attention_mask = torch.ones(0, 128)\n",
        "            self.labels = torch.empty(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.labels[idx]}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in __getitem__ at index {idx}: {e}\")\n",
        "            raise\n",
        "\n",
        "# Load or regenerate dataset\n",
        "input_ids, attention_mask, labels = None, None, None\n",
        "dataset_path = os.path.join(drive_path, 'dataset_data.json')\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    try:\n",
        "        with open(dataset_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            input_ids = np.array(data['input_ids'])\n",
        "            attention_mask = np.array(data['attention_mask'])\n",
        "            labels = np.array(data['labels'])\n",
        "        logger.info(f\"Loaded dataset from {dataset_path} with {len(labels)} samples.\")\n",
        "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
        "        logger.error(f\"Failed to load dataset from {dataset_path}: {e}. Attempting regeneration.\")\n",
        "else:\n",
        "    logger.warning(f\"Dataset file {dataset_path} not found. Attempting regeneration.\")\n",
        "    try:\n",
        "        from google.colab import files  # For manual upload\n",
        "        print(\"Dataset file not found. Please upload your dataset CSV (e.g., 'sentiment_data.csv') with 'review' and 'sentiment' columns.\")\n",
        "        uploaded = files.upload()\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv(next(iter(uploaded)))\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        texts = df['review'].fillna('').tolist()\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='np')\n",
        "        input_ids = encodings['input_ids']\n",
        "        attention_mask = encodings['attention_mask']\n",
        "        labels = df['sentiment'].map({'positive': 1, 'negative': 0, 'neutral': 2}).fillna(1).astype(int).values\n",
        "        # Save regenerated data for future use\n",
        "        with open(dataset_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'input_ids': input_ids.tolist(),\n",
        "                'attention_mask': attention_mask.tolist(),\n",
        "                'labels': labels.tolist()\n",
        "            }, f)\n",
        "        logger.info(f\"Regenerated and saved dataset to {dataset_path} with {len(labels)} samples.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to regenerate dataset: {e}. Please run Notebook 3 (Cells 0-6) to prepare data or upload a valid CSV.\")\n",
        "        raise ValueError(\"Dataset regeneration failed. Upload a CSV with 'review' and 'sentiment' columns or run Notebook 3 and retry.\")\n",
        "\n",
        "# Load dataset into DataLoader\n",
        "if 'loader' not in globals() or loader is None:\n",
        "    try:\n",
        "        logger.info(\"Loader not found. Recreating from input_ids, attention_mask, and labels.\")\n",
        "        dataset = SentimentDataset(input_ids, attention_mask, labels)\n",
        "        if len(dataset) == 0:\n",
        "            raise ValueError(\"Dataset is empty after initialization.\")\n",
        "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "        logger.info(f\"Recreated loader with {len(dataset)} samples.\")\n",
        "        first_batch = next(iter(loader), None)\n",
        "        if first_batch is None:\n",
        "            raise ValueError(\"Loader is empty; no batches available.\")\n",
        "        logger.info(f\"First batch shapes: input_ids={first_batch['input_ids'].shape}, attention_mask={first_batch['attention_mask'].shape}, labels={first_batch['labels'].shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to recreate loader: {e}. Please ensure valid data is available.\")\n",
        "        raise\n",
        "\n",
        "# Dynamically load selected_config, best_accuracy, and target_accuracy from Notebook 3\n",
        "output_path = os.path.join(drive_path, 'part3_output.json')\n",
        "selected_config = None\n",
        "best_accuracy = None\n",
        "target_accuracy = None\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    try:\n",
        "        with open(output_path, 'r') as f:\n",
        "            part3_output = json.load(f)\n",
        "            selected_config = part3_output['selected_config']\n",
        "            best_accuracy = float(part3_output['best_accuracy'])\n",
        "            target_accuracy = float(part3_output['target_accuracy'])\n",
        "        logger.info(f\"Loaded Part 3 output: {selected_config}, Best Accuracy={best_accuracy:.3f}, Target Accuracy={target_accuracy:.3f}\")\n",
        "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
        "        logger.error(f\"Failed to load Part 3 output from {output_path}: {e}. Configuration required.\")\n",
        "else:\n",
        "    logger.error(f\"Part 3 output file {output_path} not found. Configuration required.\")\n",
        "\n",
        "# Generic fallback if no valid configuration is loaded\n",
        "if selected_config is None or best_accuracy is None or target_accuracy is None:\n",
        "    logger.warning(\"No valid configuration loaded from Notebook 3. Please run Notebook 3 first or provide configuration manually.\")\n",
        "    selected_config = {'approach': 'Unconfigured', 'message': 'Run Notebook 3 to select a model'}\n",
        "    best_accuracy = 0.0\n",
        "    target_accuracy = 0.920  # Retain target as a constant unless specified\n",
        "    print(\"Warning: Model not configured. Please execute Notebook 3 (Sentiment_Analysis_Model_Optimization.ipynb) to select a model.\")\n",
        "    raise ValueError(\"Configuration missing. Please run Notebook 3 and retry.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Sentiment_Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN-04ibY1ZWY",
        "outputId": "6a1447e3-9b4d-4aa0-c28f-e3abe48b5d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " coordinator_logs.txt\n",
            " Data_Analysis_Quality_Preprocessing.ipynb\n",
            " data_analysis_report.json\n",
            " dataset_data.json\n",
            " deploy\n",
            " explainability_output\n",
            " explainability_outputs\n",
            "'fine_tuned_traditional_ml_(random_forest-like).joblib'\n",
            " hyperparams.json\n",
            " Notebook_5_CodeGen_Explainability.ipynb\n",
            " Part_1_Environment_Setup.ipynb\n",
            " Part_3_Model_Training_and_Evaluation.ipynb\n",
            " part3_output.json\n",
            " part4_output.json\n",
            " preprocessed_data.pt\n",
            " processed_dataset.csv\n",
            " quality_check_report.json\n",
            " selected_config_checkpoint.pkl\n",
            " Sentiment_Analysis_Model_Optimization.ipynb\n",
            " trained_traditional_ml_random_forest-like_encoder.joblib\n",
            " trained_traditional_ml_random_forest-like.joblib\n",
            " trained_traditional_ml_random_forest-like_vectorizer.joblib\n",
            "'train_traditional_ml_(random_forest-like).py'\n",
            " train_traditional_ml_random_forest-like.py\n",
            " user_dataset_prompt.json\n",
            " user_feedback.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8a: Setup and Initial Checks\n",
        "# Purpose: Set up the environment, define the device, and validate configuration.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import GPyOpt  # For Bayesian optimization\n",
        "import torch_pruning as tp  # For model pruning\n",
        "import torch.quantization  # For quantization\n",
        "import csv\n",
        "import psutil  # For memory usage monitoring (needed for log_memory_usage)\n",
        "\n",
        "# Ensure device is defined (redundant but ensures standalone execution)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define drive path (assumes Cell 0 has mounted the drive)\n",
        "drive_path = '/content/drive/MyDrive/Sentiment_Project'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Check configuration before proceeding\n",
        "if 'selected_config' not in globals() or selected_config['approach'] == 'Unconfigured':\n",
        "    logger.error(\"Model not configured. Skipping optimization and evaluation.\")\n",
        "    print(\"Error: Model not configured. Please run Notebook 3 and Cell 0 to set up the configuration.\")\n",
        "    raise ValueError(\"Configuration missing. Run Notebook 3 and Cell 0 first.\")\n",
        "\n",
        "# Check if loader exists (from Cell 0)\n",
        "if 'loader' not in globals():\n",
        "    logger.error(\"Data loader not found. Please run Cell 0 to initialize the dataset and loader.\")\n",
        "    raise ValueError(\"Data loader missing. Run Cell 0 first.\")\n",
        "\n",
        "# Log initial state\n",
        "logger.info(f\"Starting optimization and evaluation with approach: {selected_config['approach']}\")\n",
        "logger.info(f\"Target Accuracy: {target_accuracy:.3f}, Best Accuracy so far: {best_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "SE_scd2Z757J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8b: Hyperparameter Optimization Using Bayesian Optimization\n",
        "# Purpose: Dynamically optimize hyperparameters for any selected model and save results.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, AlbertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AlbertTokenizer\n",
        "import GPyOpt\n",
        "from scipy.sparse import issparse\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Sentiment_Project'\n",
        "LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'\n",
        "MIN_SAMPLES = 5  # Minimum samples for train-test split\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load Configuration\n",
        "if 'selected_config' in globals():\n",
        "    del selected_config\n",
        "checkpoint_path = os.path.join(DRIVE_PATH, 'selected_config_checkpoint.pkl')\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    raise FileNotFoundError(f\"Configuration file not found at {checkpoint_path}. Please run Notebook 3 first.\")\n",
        "with open(checkpoint_path, 'rb') as f:\n",
        "    selected_config = pickle.load(f)\n",
        "logger.info(f\"Loaded initial configuration: {selected_config}\")\n",
        "\n",
        "# Expanded Model Registry\n",
        "model_registry = {\n",
        "    \"Traditional ML (Random Forest-like)\": {'class': RandomForestClassifier, 'type': 'Traditional ML'},\n",
        "    \"Traditional ML (Logistic Regression-like)\": {'class': LogisticRegression, 'type': 'Traditional ML'},\n",
        "    \"Traditional ML (SVM-like)\": {'class': lambda **params: SVC(probability=True, **params), 'type': 'Traditional ML'},\n",
        "    \"Traditional ML (Gradient Boosting-like)\": {'class': GradientBoostingClassifier, 'type': 'Traditional ML'},\n",
        "    \"Traditional ML (Naive Bayes-like)\": {'class': MultinomialNB, 'type': 'Traditional ML'},\n",
        "    \"Traditional ML (KNN-like)\": {'class': KNeighborsClassifier, 'type': 'Traditional ML'},\n",
        "    \"Deep Learning (DistilBERT-like)\": {\n",
        "        'class': DistilBertForSequenceClassification, 'pretrained': 'distilbert-base-uncased', 'type': 'Deep Learning', 'tokenizer': DistilBertTokenizer\n",
        "    },\n",
        "    \"Deep Learning (BERT-like)\": {\n",
        "        'class': BertForSequenceClassification, 'pretrained': 'bert-base-uncased', 'type': 'Deep Learning', 'tokenizer': BertTokenizer\n",
        "    },\n",
        "    \"Deep Learning (RoBERTa-like)\": {\n",
        "        'class': RobertaForSequenceClassification, 'pretrained': 'roberta-base', 'type': 'Deep Learning', 'tokenizer': RobertaTokenizer\n",
        "    },\n",
        "    \"Deep Learning (ALBERT-like)\": {\n",
        "        'class': AlbertForSequenceClassification, 'pretrained': 'albert-base-v2', 'type': 'Deep Learning', 'tokenizer': AlbertTokenizer\n",
        "    },\n",
        "    \"Deep Learning (LSTM-like)\": {'class': None, 'type': 'Deep Learning (RNN)', 'custom': True},\n",
        "    \"Deep Learning (CNN-like)\": {'class': None, 'type': 'Deep Learning (CNN)', 'custom': True}\n",
        "}\n",
        "\n",
        "# Hyperparameter Spaces (Updated to Fix `learning_rate` Issue for RandomForestClassifier)\n",
        "hyperparam_spaces = {\n",
        "    \"Traditional ML (Random Forest-like)\": [\n",
        "        {'name': 'n_estimators', 'type': 'discrete', 'domain': (50, 100, 200, 300)},\n",
        "        {'name': 'max_depth', 'type': 'discrete', 'domain': (10, 20, 30, 50)},\n",
        "        {'name': 'min_samples_split', 'type': 'discrete', 'domain': (2, 5, 10)}\n",
        "    ],\n",
        "    \"Traditional ML (Logistic Regression-like)\": [\n",
        "        {'name': 'C', 'type': 'continuous', 'domain': (1e-4, 1e2)},\n",
        "        {'name': 'max_iter', 'type': 'discrete', 'domain': (100, 500, 1000)}\n",
        "    ],\n",
        "    \"Traditional ML (SVM-like)\": [\n",
        "        {'name': 'C', 'type': 'continuous', 'domain': (1e-4, 1e2)},\n",
        "        {'name': 'kernel', 'type': 'categorical', 'domain': ('linear', 'rbf')}\n",
        "    ],\n",
        "    \"Traditional ML (Gradient Boosting-like)\": [\n",
        "        {'name': 'n_estimators', 'type': 'discrete', 'domain': (50, 100, 200, 300)},\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-3, 1.0)},\n",
        "        {'name': 'max_depth', 'type': 'discrete', 'domain': (3, 5, 10)}\n",
        "    ],\n",
        "    \"Traditional ML (Naive Bayes-like)\": [\n",
        "        {'name': 'alpha', 'type': 'continuous', 'domain': (1e-3, 1.0)}\n",
        "    ],\n",
        "    \"Traditional ML (KNN-like)\": [\n",
        "        {'name': 'n_neighbors', 'type': 'discrete', 'domain': (3, 5, 7, 10)},\n",
        "        {'name': 'weights', 'type': 'categorical', 'domain': ('uniform', 'distance')}\n",
        "    ],\n",
        "    \"Deep Learning (DistilBERT-like)\": [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-5, 5e-5)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32)}\n",
        "    ],\n",
        "    \"Deep Learning (BERT-like)\": [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-5, 5e-5)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32)}\n",
        "    ],\n",
        "    \"Deep Learning (RoBERTa-like)\": [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-5, 5e-5)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32)}\n",
        "    ],\n",
        "    \"Deep Learning (ALBERT-like)\": [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-5, 5e-5)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32)}\n",
        "    ],\n",
        "    \"Deep Learning (LSTM-like)\": [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-4, 1e-2)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64)},\n",
        "        {'name': 'hidden_size', 'type': 'discrete', 'domain': (64, 128, 256)}\n",
        "    ],\n",
        "    \"Deep Learning (CNN-like)\": [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-4, 1e-2)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64)},\n",
        "        {'name': 'filters', 'type': 'discrete', 'domain': (32, 64, 128)}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Valid Hyperparameters (Updated to Match `hyperparam_spaces`)\n",
        "valid_hyperparams = {\n",
        "    \"Traditional ML (Random Forest-like)\": {'n_estimators', 'max_depth', 'min_samples_split'},\n",
        "    \"Traditional ML (Logistic Regression-like)\": {'C', 'max_iter'},\n",
        "    \"Traditional ML (SVM-like)\": {'C', 'kernel'},\n",
        "    \"Traditional ML (Gradient Boosting-like)\": {'n_estimators', 'learning_rate', 'max_depth'},\n",
        "    \"Traditional ML (Naive Bayes-like)\": {'alpha'},\n",
        "    \"Traditional ML (KNN-like)\": {'n_neighbors', 'weights'},\n",
        "    \"Deep Learning (DistilBERT-like)\": {'learning_rate', 'batch_size'},\n",
        "    \"Deep Learning (BERT-like)\": {'learning_rate', 'batch_size'},\n",
        "    \"Deep Learning (RoBERTa-like)\": {'learning_rate', 'batch_size'},\n",
        "    \"Deep Learning (ALBERT-like)\": {'learning_rate', 'batch_size'},\n",
        "    \"Deep Learning (LSTM-like)\": {'learning_rate', 'batch_size', 'hidden_size'},\n",
        "    \"Deep Learning (CNN-like)\": {'learning_rate', 'batch_size', 'filters'}\n",
        "}\n",
        "\n",
        "# Dynamic Model Selection\n",
        "model_name = selected_config.get('model')\n",
        "if not model_name or model_name not in model_registry:\n",
        "    raise ValueError(f\"Model '{model_name}' not specified or not supported. Supported models: {list(model_registry.keys())}\")\n",
        "model_info = model_registry[model_name]\n",
        "model_type = model_info['type']\n",
        "logger.info(f\"Selected model: {model_name}, type: {model_type}\")\n",
        "\n",
        "# Data Preprocessing\n",
        "selected_model = None\n",
        "preprocessed_data = {'X': None, 'y': None, 'texts': None, 'full_X': None, 'full_y': None}\n",
        "vectorizer = None\n",
        "tokenizer = None\n",
        "\n",
        "# Load Dataset\n",
        "df_path = os.path.join(DRIVE_PATH, 'processed_dataset.csv')\n",
        "if not os.path.exists(df_path):\n",
        "    raise FileNotFoundError(f\"Dataset not found at {df_path}\")\n",
        "\n",
        "df = pd.read_csv(df_path)\n",
        "logger.info(f\"Initial dataset shape: {df.shape}\")\n",
        "logger.info(f\"Columns: {list(df.columns)}\")\n",
        "logger.info(f\"Sample rows:\\n{df.head().to_string()}\")\n",
        "\n",
        "# Dynamic Column Selection\n",
        "feature_col = selected_config.get('feature_col', 'review')\n",
        "label_col = selected_config.get('label_col', 'sentiment')\n",
        "\n",
        "if feature_col not in df.columns or label_col not in df.columns:\n",
        "    logger.warning(f\"Specified columns (feature: {feature_col}, label: {label_col}) not found. Attempting to infer columns.\")\n",
        "    if len(df.columns) < 2:\n",
        "        raise ValueError(f\"Dataset must have at least 2 columns. Found: {list(df.columns)}\")\n",
        "    label_col = df.columns[-1]\n",
        "    feature_col = df.columns[-2]\n",
        "    logger.info(f\"Inferred feature column: {feature_col}, label column: {label_col}\")\n",
        "\n",
        "# Data Cleaning\n",
        "df = df.dropna(subset=[feature_col, label_col])\n",
        "df = df[df[feature_col].astype(str).str.strip() != '']\n",
        "df = df[df[label_col].astype(str).str.strip() != '']\n",
        "logger.info(f\"Shape after cleaning: {df.shape}\")\n",
        "\n",
        "# Dynamic Label Encoding\n",
        "unique_labels = df[label_col].astype(str).str.lower().unique()\n",
        "logger.info(f\"Unique labels in '{label_col}': {unique_labels}\")\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "logger.info(f\"Label mapping: {label_mapping}\")\n",
        "if len(unique_labels) < 2:\n",
        "    raise ValueError(f\"Label column '{label_col}' must have at least 2 unique values. Found: {unique_labels}\")\n",
        "\n",
        "# Define Custom LSTM and CNN Models\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, filters, output_size):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv = nn.Conv1d(embedding_dim, filters, kernel_size=3)\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc = nn.Linear(filters, output_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        embedded = self.embedding(input_ids).transpose(1, 2)\n",
        "        conv_out = self.conv(embedded)\n",
        "        pooled = self.pool(conv_out).squeeze(-1)\n",
        "        out = self.fc(pooled)\n",
        "        return out\n",
        "\n",
        "# Data Preprocessing Based on Model Type\n",
        "if model_type == 'Traditional ML':\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    full_X = vectorizer.fit_transform(df[feature_col].astype(str))\n",
        "    full_y = df[label_col].astype(str).str.lower().map(label_mapping).astype(int)\n",
        "elif model_type == 'Deep Learning':\n",
        "    tokenizer_class = model_info.get('tokenizer', DistilBertTokenizer)\n",
        "    tokenizer = tokenizer_class.from_pretrained(model_info['pretrained'])\n",
        "    encodings = tokenizer(df[feature_col].astype(str).tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "    full_X = {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']}\n",
        "    full_y = torch.tensor(df[label_col].astype(str).str.lower().map(label_mapping).values, dtype=torch.long)\n",
        "elif model_type in ['Deep Learning (RNN)', 'Deep Learning (CNN)']:\n",
        "    from torchtext.vocab import build_vocab_from_iterator\n",
        "    def yield_tokens(data_iter):\n",
        "        for text in data_iter:\n",
        "            yield text.lower().split()\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(df[feature_col].astype(str)), specials=['<unk>'])\n",
        "    vocab.set_default_index(vocab['<unk>'])\n",
        "    text_pipeline = lambda x: [vocab[token] for token in x.lower().split()]\n",
        "    max_len = 128\n",
        "    input_ids = []\n",
        "    for text in df[feature_col].astype(str):\n",
        "        tokens = text_pipeline(text)[:max_len]\n",
        "        tokens += [0] * (max_len - len(tokens))\n",
        "        input_ids.append(tokens)\n",
        "    full_X = {'input_ids': torch.tensor(input_ids, dtype=torch.long)}\n",
        "    full_y = torch.tensor(df[label_col].astype(str).str.lower().map(label_mapping).values, dtype=torch.long)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "# Validate Data\n",
        "if model_type == 'Traditional ML':\n",
        "    if full_X.shape[0] != len(full_y):\n",
        "        raise ValueError(f\"Feature-label mismatch: {full_X.shape[0]} features vs {len(full_y)} labels\")\n",
        "else:\n",
        "    if full_X['input_ids'].shape[0] != len(full_y):\n",
        "        raise ValueError(f\"Feature-label mismatch: {full_X['input_ids'].shape[0]} features vs {len(full_y)} labels\")\n",
        "if len(full_y) < 2:\n",
        "    raise ValueError(f\"Dataset too small: {len(full_y)} samples. Need at least 2 samples.\")\n",
        "\n",
        "# Check Label Distribution\n",
        "label_counts = pd.Series(full_y.numpy() if torch.is_tensor(full_y) else full_y).value_counts()\n",
        "logger.info(f\"Label distribution: {label_counts.to_dict()}\")\n",
        "\n",
        "# Train-Test Split with Enhanced Robustness\n",
        "use_full_dataset = False\n",
        "if len(full_y) < MIN_SAMPLES:\n",
        "    logger.warning(f\"Dataset has {len(full_y)} samples, below minimum {MIN_SAMPLES}. Using full dataset without optimization.\")\n",
        "    use_full_dataset = True\n",
        "else:\n",
        "    test_size = 0.2\n",
        "    min_train_samples = 4\n",
        "    min_test_samples = 2\n",
        "    required_samples = max(min_train_samples / (1 - test_size), min_test_samples / test_size)\n",
        "    if len(full_y) < required_samples:\n",
        "        logger.warning(f\"Dataset too small for split: {len(full_y)} samples. Need {required_samples} samples. Using full dataset.\")\n",
        "        use_full_dataset = True\n",
        "    else:\n",
        "        # Ensure at least 2 samples per class in test set\n",
        "        label_counts = pd.Series(full_y.numpy() if torch.is_tensor(full_y) else full_y).value_counts()\n",
        "        min_class_samples = label_counts.min()\n",
        "        test_class_samples = min_class_samples * test_size\n",
        "        if test_class_samples < 2:\n",
        "            logger.warning(f\"Insufficient samples per class for stratified split: {test_class_samples} samples per class in test set. Using full dataset.\")\n",
        "            use_full_dataset = True\n",
        "        else:\n",
        "            if model_type == 'Traditional ML':\n",
        "                X_train, X_test, y_train, y_test = train_test_split(full_X, full_y, test_size=test_size, random_state=42, stratify=full_y)\n",
        "                logger.info(f\"Split shapes: X_train={X_train.shape}, y_train={y_train.shape}, X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "                logger.info(f\"y_train distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
        "                logger.info(f\"y_test distribution: {pd.Series(y_test).value_counts().to_dict()}\")\n",
        "            else:\n",
        "                indices = np.arange(len(full_y))\n",
        "                train_idx, test_idx, y_train, y_test = train_test_split(indices, full_y, test_size=test_size, random_state=42, stratify=full_y)\n",
        "                X_train = {k: v[train_idx] for k, v in full_X.items()}\n",
        "                X_test = {k: v[test_idx] for k, v in full_X.items()}\n",
        "                logger.info(f\"Split shapes: X_train input_ids={X_train['input_ids'].shape}, y_train={y_train.shape}, X_test input_ids={X_test['input_ids'].shape}, y_test={y_test.shape}\")\n",
        "                logger.info(f\"y_train distribution: {pd.Series(y_train.numpy() if torch.is_tensor(y_train) else y_train).value_counts().to_dict()}\")\n",
        "                logger.info(f\"y_test distribution: {pd.Series(y_test.numpy() if torch.is_tensor(y_test) else y_test).value_counts().to_dict()}\")\n",
        "            # Additional validation\n",
        "            y_test_counts = pd.Series(y_test).value_counts()\n",
        "            if len(y_test) < 2 or len(y_train) < 2 or any(y_test_counts < 2):\n",
        "                logger.warning(f\"Split produced insufficient samples: y_train={len(y_train)}, y_test={len(y_test)}, y_test distribution={y_test_counts.to_dict()}. Using full dataset.\")\n",
        "                use_full_dataset = True\n",
        "\n",
        "if use_full_dataset:\n",
        "    X_train, X_test, y_train, y_test = full_X, None, full_y, None\n",
        "    logger.info(f\"Using full dataset: X_train={'shape' in dir(X_train) and X_train.shape or X_train['input_ids'].shape}, y_train={y_train.shape}\")\n",
        "\n",
        "# Store Preprocessed Data\n",
        "preprocessed_data['full_X'] = full_X\n",
        "preprocessed_data['full_y'] = full_y\n",
        "preprocessed_data['texts'] = df[feature_col].astype(str).tolist()\n",
        "if not use_full_dataset:\n",
        "    preprocessed_data['X'] = X_test\n",
        "    preprocessed_data['y'] = y_test\n",
        "\n",
        "# Bayesian Optimization\n",
        "space = hyperparam_spaces.get(model_name, [])\n",
        "optimal_params = []\n",
        "if not space or use_full_dataset:\n",
        "    logger.info(f\"Optimization skipped due to {'no hyperparameter space' if not space else 'insufficient data'}. Using defaults.\")\n",
        "    if model_type == 'Traditional ML':\n",
        "        default_params = {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2} if 'Random Forest' in model_name else \\\n",
        "                         {'C': 1.0, 'max_iter': 100} if 'Logistic Regression' in model_name else \\\n",
        "                         {'C': 1.0, 'kernel': 'rbf'} if 'SVM' in model_name else \\\n",
        "                         {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3} if 'Gradient Boosting' in model_name else \\\n",
        "                         {'alpha': 1.0} if 'Naive Bayes' in model_name else \\\n",
        "                         {'n_neighbors': 5, 'weights': 'uniform'} if 'KNN' in model_name else {}\n",
        "        selected_config['hyperparams'] = default_params\n",
        "        optimal_params = list(default_params.values())\n",
        "    else:\n",
        "        default_params = {'learning_rate': 2e-5, 'batch_size': 16} if 'Deep Learning' in model_type else \\\n",
        "                         {'learning_rate': 1e-3, 'batch_size': 32, 'hidden_size': 128} if 'LSTM' in model_name else \\\n",
        "                         {'learning_rate': 1e-3, 'batch_size': 32, 'filters': 64}\n",
        "        selected_config['hyperparams'] = default_params\n",
        "        optimal_params = list(default_params.values())\n",
        "    logger.info(f\"Default hyperparameters: {selected_config['hyperparams']}\")\n",
        "else:\n",
        "    def objective(params):\n",
        "        params_dict = None\n",
        "        try:\n",
        "            params_dict = {space[i]['name']: params[i] for i in range(len(space))}\n",
        "            # Validate hyperparameters against valid_hyperparams\n",
        "            valid_params_set = valid_hyperparams.get(model_name, set())\n",
        "            invalid_params = [k for k in params_dict.keys() if k not in valid_params_set]\n",
        "            if invalid_params:\n",
        "                logger.error(f\"Invalid hyperparameters for {model_name}: {invalid_params}. Expected: {valid_params_set}\")\n",
        "                return 1.0  # Fallback score\n",
        "            if model_type == 'Traditional ML':\n",
        "                typed_params = {k: int(v) if k in {'n_estimators', 'max_depth', 'min_samples_split', 'max_iter', 'n_neighbors'} else v for k, v in params_dict.items()}\n",
        "                model = model_info['class'](**typed_params)\n",
        "                model.fit(X_train, y_train)\n",
        "                if X_test is not None and y_test is not None:\n",
        "                    logger.info(f\"Before predict: X_test shape={'shape' in dir(X_test) and X_test.shape or X_test['input_ids'].shape}, y_test shape={y_test.shape}\")\n",
        "                    predictions = model.predict(X_test)\n",
        "                    logger.info(f\"After predict: predictions shape={predictions.shape}, y_test shape={y_test.shape}\")\n",
        "                    if len(predictions) != len(y_test):\n",
        "                        logger.error(f\"Prediction-label mismatch: {len(predictions)} predictions vs {len(y_test)} labels\")\n",
        "                        return 1.0  # Fallback score\n",
        "                    if len(y_test) < 2:\n",
        "                        logger.warning(f\"y_test has insufficient samples: {len(y_test)}. Need at least 2. Returning default score.\")\n",
        "                        return 1.0  # Fallback score\n",
        "                    y_test_counts = pd.Series(y_test).value_counts()\n",
        "                    if any(y_test_counts < 2):\n",
        "                        logger.warning(f\"y_test has insufficient samples per class: {y_test_counts.to_dict()}. Returning default score.\")\n",
        "                        return 1.0  # Fallback score\n",
        "                    accuracy = accuracy_score(y_test, predictions)\n",
        "                    logger.debug(f\"Accuracy: {accuracy} with params {typed_params}\")\n",
        "                    return -accuracy\n",
        "                else:\n",
        "                    predictions = model.predict(X_train)\n",
        "                    if len(y_train) < 2:\n",
        "                        logger.warning(f\"y_train has insufficient samples: {len(y_train)}. Returning default score.\")\n",
        "                        return 1.0\n",
        "                    accuracy = accuracy_score(y_train, predictions)\n",
        "                    logger.debug(f\"Fallback accuracy: {accuracy} with params {typed_params}\")\n",
        "                    return -accuracy\n",
        "            else:\n",
        "                typed_params = {k: int(v) if k in {'batch_size', 'hidden_size', 'filters'} else v for k, v in params_dict.items()}\n",
        "                if model_type == 'Deep Learning':\n",
        "                    model_class = model_info['class']\n",
        "                    pretrained = model_info['pretrained']\n",
        "                    model = model_class.from_pretrained(pretrained, num_labels=len(unique_labels)).to(DEVICE)\n",
        "                elif model_type == 'Deep Learning (RNN)':\n",
        "                    model = LSTMClassifier(vocab_size=len(vocab), embedding_dim=100, hidden_size=typed_params['hidden_size'], output_size=len(unique_labels)).to(DEVICE)\n",
        "                else:\n",
        "                    model = CNNClassifier(vocab_size=len(vocab), embedding_dim=100, filters=typed_params['filters'], output_size=len(unique_labels)).to(DEVICE)\n",
        "\n",
        "                optimizer = optim.Adam(model.parameters(), lr=typed_params['learning_rate'])\n",
        "                batch_size = typed_params['batch_size']\n",
        "                dataset = TensorDataset(*[X_train[k] for k in X_train], y_train)\n",
        "                temp_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "                model.train()\n",
        "                for _ in range(1):\n",
        "                    for batch in temp_loader:\n",
        "                        inputs = {k: v.to(DEVICE) for k, v in zip(['input_ids', 'attention_mask'] if 'attention_mask' in X_train else ['input_ids'], batch[:-1])}\n",
        "                        labels = batch[-1].to(DEVICE)\n",
        "                        outputs = model(**inputs) if model_type == 'Deep Learning' else model(inputs['input_ids'])\n",
        "                        logits = outputs.logits if model_type == 'Deep Learning' else outputs\n",
        "                        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                        torch.cuda.empty_cache()\n",
        "                model.eval()\n",
        "                predictions, true_labels = [], []\n",
        "                if X_test is not None and y_test is not None:\n",
        "                    test_dataset = TensorDataset(*[X_test[k] for k in X_test], y_test)\n",
        "                    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "                else:\n",
        "                    test_loader = temp_loader\n",
        "                with torch.no_grad():\n",
        "                    for batch in test_loader:\n",
        "                        inputs = {k: v.to(DEVICE) for k, v in zip(['input_ids', 'attention_mask'] if 'attention_mask' in X_train else ['input_ids'], batch[:-1])}\n",
        "                        labels = batch[-1].to(DEVICE)\n",
        "                        outputs = model(**inputs) if model_type == 'Deep Learning' else model(inputs['input_ids'])\n",
        "                        logits = outputs.logits if model_type == 'Deep Learning' else outputs\n",
        "                        preds = torch.argmax(logits, dim=1)\n",
        "                        predictions.extend(preds.cpu().numpy())\n",
        "                        true_labels.extend(labels.cpu().numpy())\n",
        "                if len(true_labels) < 2:\n",
        "                    logger.warning(f\"true_labels has insufficient samples: {len(true_labels)}. Returning default score.\")\n",
        "                    return 1.0\n",
        "                true_labels_counts = pd.Series(true_labels).value_counts()\n",
        "                if any(true_labels_counts < 2):\n",
        "                    logger.warning(f\"true_labels has insufficient samples per class: {true_labels_counts.to_dict()}. Returning default score.\")\n",
        "                    return 1.0\n",
        "                accuracy = accuracy_score(true_labels, predictions)\n",
        "                return -accuracy\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Objective failed with params {params_dict if params_dict else 'unknown'}: {str(e)}\")\n",
        "            return 1.0\n",
        "\n",
        "    try:\n",
        "        bo = GPyOpt.methods.BayesianOptimization(\n",
        "            f=objective,\n",
        "            domain=space,\n",
        "            acquisition_type='EI',\n",
        "            maximize=False,\n",
        "            verbosity=True\n",
        "        )\n",
        "        bo.run_optimization(max_iter=20)\n",
        "        optimized_params = {space[i]['name']: bo.x_opt[i] for i in range(len(space))}\n",
        "        typed_params = {k: int(v) if k in {'n_estimators', 'max_depth', 'min_samples_split', 'max_iter', 'n_neighbors', 'batch_size', 'hidden_size', 'filters'} else v for k, v in optimized_params.items()}\n",
        "        # Filter optimized_params to include only valid hyperparameters\n",
        "        valid_params_set = valid_hyperparams.get(model_name, set())\n",
        "        filtered_params = {k: v for k, v in typed_params.items() if k in valid_params_set}\n",
        "        selected_config['hyperparams'] = filtered_params\n",
        "        optimal_params = list(filtered_params.values())\n",
        "        logger.info(f\"Optimized hyperparameters (filtered): {filtered_params}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Optimization failed: {str(e)}\")\n",
        "        if model_type == 'Traditional ML':\n",
        "            default_params = {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2} if 'Random Forest' in model_name else \\\n",
        "                             {'C': 1.0, 'max_iter': 100} if 'Logistic Regression' in model_name else \\\n",
        "                             {'C': 1.0, 'kernel': 'rbf'} if 'SVM' in model_name else \\\n",
        "                             {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3} if 'Gradient Boosting' in model_name else \\\n",
        "                             {'alpha': 1.0} if 'Naive Bayes' in model_name else \\\n",
        "                             {'n_neighbors': 5, 'weights': 'uniform'} if 'KNN' in model_name else {}\n",
        "            selected_config['hyperparams'] = default_params\n",
        "            optimal_params = list(default_params.values())\n",
        "        else:\n",
        "            default_params = {'learning_rate': 2e-5, 'batch_size': 16} if 'Deep Learning' in model_type else \\\n",
        "                             {'learning_rate': 1e-3, 'batch_size': 32, 'hidden_size': 128} if 'LSTM' in model_name else \\\n",
        "                             {'learning_rate': 1e-3, 'batch_size': 32, 'filters': 64}\n",
        "            selected_config['hyperparams'] = default_params\n",
        "            optimal_params = list(default_params.values())\n",
        "        logger.info(f\"Using default hyperparameters: {selected_config['hyperparams']}\")\n",
        "\n",
        "# Model Initialization and Training\n",
        "best_accuracy = None\n",
        "try:\n",
        "    if model_type == 'Traditional ML':\n",
        "        if model_info['class'] is not None:\n",
        "            typed_hyperparams = {k: int(v) if k in {'n_estimators', 'max_depth', 'min_samples_split', 'max_iter', 'n_neighbors'} else v for k, v in selected_config['hyperparams'].items()}\n",
        "            selected_model = model_info['class'](**typed_hyperparams)\n",
        "            selected_model.fit(preprocessed_data['full_X'], preprocessed_data['full_y'])\n",
        "            if preprocessed_data['X'] is not None:\n",
        "                predictions = selected_model.predict(preprocessed_data['X'])\n",
        "                best_accuracy = accuracy_score(preprocessed_data['y'], predictions)\n",
        "            else:\n",
        "                predictions = selected_model.predict(preprocessed_data['full_X'])\n",
        "                best_accuracy = accuracy_score(preprocessed_data['full_y'], predictions)\n",
        "            logger.info(f\"Model trained with hyperparameters: {typed_hyperparams}, Best Accuracy: {best_accuracy}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Model {model_name} not implemented\")\n",
        "    else:\n",
        "        typed_hyperparams = {k: int(v) if k in {'batch_size', 'hidden_size', 'filters'} else v for k, v in selected_config['hyperparams'].items()}\n",
        "        if model_type == 'Deep Learning':\n",
        "            model_class = model_info['class']\n",
        "            pretrained = model_info['pretrained']\n",
        "            selected_model = model_class.from_pretrained(pretrained, num_labels=len(unique_labels)).to(DEVICE)\n",
        "        elif model_type == 'Deep Learning (RNN)':\n",
        "            selected_model = LSTMClassifier(vocab_size=len(vocab), embedding_dim=100, hidden_size=typed_hyperparams['hidden_size'], output_size=len(unique_labels)).to(DEVICE)\n",
        "        else:\n",
        "            selected_model = CNNClassifier(vocab_size=len(vocab), embedding_dim=100, filters=typed_hyperparams['filters'], output_size=len(unique_labels)).to(DEVICE)\n",
        "\n",
        "        optimizer = optim.Adam(selected_model.parameters(), lr=typed_hyperparams['learning_rate'])\n",
        "        batch_size = typed_hyperparams['batch_size']\n",
        "        dataset = TensorDataset(*[preprocessed_data['full_X'][k] for k in preprocessed_data['full_X']], preprocessed_data['full_y'])\n",
        "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        selected_model.train()\n",
        "        for _ in range(1):\n",
        "            for batch in train_loader:\n",
        "                inputs = {k: v.to(DEVICE) for k, v in zip(['input_ids', 'attention_mask'] if 'attention_mask' in preprocessed_data['full_X'] else ['input_ids'], batch[:-1])}\n",
        "                labels = batch[-1].to(DEVICE)\n",
        "                outputs = selected_model(**inputs) if model_type == 'Deep Learning' else selected_model(inputs['input_ids'])\n",
        "                logits = outputs.logits if model_type == 'Deep Learning' else outputs\n",
        "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                torch.cuda.empty_cache()\n",
        "        selected_model.eval()\n",
        "        predictions, true_labels = [], []\n",
        "        if preprocessed_data['X'] is not None:\n",
        "            test_dataset = TensorDataset(*[preprocessed_data['X'][k] for k in preprocessed_data['X']], preprocessed_data['y'])\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "        else:\n",
        "            test_loader = train_loader\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                inputs = {k: v.to(DEVICE) for k, v in zip(['input_ids', 'attention_mask'] if 'attention_mask' in preprocessed_data['X'] else ['input_ids'], batch[:-1])}\n",
        "                labels = batch[-1].to(DEVICE)\n",
        "                outputs = model(**inputs) if model_type == 'Deep Learning' else model(inputs['input_ids'])\n",
        "                logits = outputs.logits if model_type == 'Deep Learning' else outputs\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                predictions.extend(preds.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "        best_accuracy = accuracy_score(true_labels, predictions)\n",
        "        logger.info(f\"Model trained with hyperparameters: {typed_hyperparams}, Best Accuracy: {best_accuracy}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Model initialization failed: {str(e)}\")\n",
        "    best_accuracy = 0.0\n",
        "    raise\n",
        "\n",
        "# Convert NumPy Types to Python Types for JSON Serialization\n",
        "def convert_to_json_serializable(obj):\n",
        "    if isinstance(obj, (np.float32, np.float64)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, (np.int32, np.int64)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_to_json_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_json_serializable(item) for item in obj]\n",
        "    return obj\n",
        "\n",
        "# Save Results\n",
        "try:\n",
        "    with open(os.path.join(DRIVE_PATH, 'selected_config_checkpoint.pkl'), 'wb') as f:\n",
        "        pickle.dump(selected_config, f)\n",
        "    logger.info(\"Configuration saved to checkpoint\")\n",
        "\n",
        "    part4_output = {\n",
        "        \"selected_config\": convert_to_json_serializable(selected_config),\n",
        "        \"best_accuracy\": float(best_accuracy),\n",
        "        \"target_accuracy\": float(selected_config.get('target_accuracy', 0.92)),\n",
        "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"user_run_id\": selected_config.get('user_run_id', '13735640')\n",
        "    }\n",
        "    with open(os.path.join(DRIVE_PATH, 'part4_output.json'), 'w') as f:\n",
        "        json.dump(part4_output, f, indent=2)\n",
        "    logger.info(\"Results saved to part4_output.json\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to save results: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Cleanup\n",
        "torch.cuda.empty_cache()\n",
        "logger.info(\"Execution completed successfully\")"
      ],
      "metadata": {
        "id": "WoRsOu-B9kOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0dbdfd-fd89-4819-e922-1326a699fa92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n",
            "ERROR:__main__:Objective failed with params unknown: index 1 is out of bounds for axis 0 with size 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8c: Apply Optimized Hyperparameters and Train Final Model\n",
        "# Purpose: Use the optimized hyperparameters to train the final model.\n",
        "\n",
        "# Ensure required imports are available\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure logging is set up (if not already)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Validate optimal_params before applying\n",
        "if 'optimal_params' not in globals():\n",
        "    logger.error(\"optimal_params not defined. Ensure Cell 8b executed successfully.\")\n",
        "    raise ValueError(\"optimal_params must be defined from Cell 8b.\")\n",
        "\n",
        "# Apply optimization\n",
        "if 'Traditional ML' in selected_config.get('approach', ''):\n",
        "    # Validate optimal_params format (should align with hyperparam_spaces for the model)\n",
        "    expected_params = len(valid_hyperparams.get(selected_config.get('model', ''), []))\n",
        "    if not isinstance(optimal_params, (list, np.ndarray)) or len(optimal_params) != expected_params:\n",
        "        logger.warning(f\"Unexpected optimal_params format: {optimal_params}. Expected array with {expected_params} values. Using defaults.\")\n",
        "        optimal_params = [100, 10, 2] if 'Random Forest' in selected_config.get('model', '') else \\\n",
        "                         [1.0, 100] if 'Logistic Regression' in selected_config.get('model', '') else \\\n",
        "                         [1.0, 'rbf'] if 'SVM' in selected_config.get('model', '') else \\\n",
        "                         [100, 0.1, 3] if 'Gradient Boosting' in selected_config.get('model', '') else \\\n",
        "                         [1.0] if 'Naive Bayes' in selected_config.get('model', '') else \\\n",
        "                         [5, 'uniform'] if 'KNN' in selected_config.get('model', '') else [100, 10, 2]\n",
        "    # Map optimal_params to the correct hyperparameter names\n",
        "    hyperparam_names = list(valid_hyperparams.get(selected_config.get('model', ''), []))\n",
        "    hyperparam_dict = {name: int(val) if name in {'n_estimators', 'max_depth', 'min_samples_split', 'max_iter', 'n_neighbors'} else val\n",
        "                       for name, val in zip(hyperparam_names, optimal_params)}\n",
        "    logger.info(f\"Applying optimized hyperparameters: {hyperparam_dict}\")\n",
        "    try:\n",
        "        selected_model.set_params(**hyperparam_dict)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to apply hyperparameters {hyperparam_dict}: {str(e)}. Using default parameters.\")\n",
        "        selected_model.set_params(n_estimators=100, max_depth=10, min_samples_split=2)\n",
        "\n",
        "    logger.info(\"Training final model on full dataset with optimized hyperparameters...\")\n",
        "    selected_model.fit(preprocessed_data['full_X'], preprocessed_data['full_y'])\n",
        "    logger.info(\"Final model training completed.\")\n",
        "else:  # Deep Learning\n",
        "    # Validate optimal_params format (should be a list with 2 values for Deep Learning)\n",
        "    expected_params = 2\n",
        "    if not isinstance(optimal_params, (list, np.ndarray)) or len(optimal_params) != expected_params:\n",
        "        logger.warning(f\"Unexpected optimal_params format: {optimal_params}. Expected array with {expected_params} values. Using defaults.\")\n",
        "        optimal_params = [2e-5, 16]  # Default values for learning_rate, batch_size\n",
        "    learning_rate, batch_size = optimal_params[0], int(optimal_params[1])\n",
        "    optimizer = optim.AdamW(selected_model.parameters(), lr=learning_rate)\n",
        "    loader = DataLoader(loader.dataset, batch_size=batch_size, shuffle=True)\n",
        "    selected_model.train()\n",
        "    for epoch in range(2):\n",
        "        for batch in tqdm(loader, desc=f\"Fine-Tuning {selected_config['approach']}\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = selected_model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "v-UpzelaF1d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8c: Optimizer Agent - Population-Based Training (PBT)\n",
        "# Purpose: Use PBT to dynamically adjust hyperparameters for deep learning models.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        ")\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure logging is set up\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mapping of model names to their classes\n",
        "model_mapping = {\n",
        "    \"Shallow Neural Network (MLP-like)\": DistilBertForSequenceClassification,\n",
        "    \"Recurrent Neural Network (LSTM/GRU-like)\": DistilBertForSequenceClassification,\n",
        "    \"Convolutional Neural Network (CNN-like)\": DistilBertForSequenceClassification,\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\": BertForSequenceClassification,\n",
        "    \"Gated Recurrent Unit (GRU-like)\": BertForSequenceClassification,\n",
        "    \"Feedforward Neural Network (FNN-like)\": DistilBertForSequenceClassification,\n",
        "    \"Hybrid (CNN-RNN)\": BertForSequenceClassification,\n",
        "    \"Deep Learning (Custom Transformer)\": BertForSequenceClassification,\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": DistilBertForSequenceClassification,\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\": BertForSequenceClassification,\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": RobertaForSequenceClassification,\n",
        "    \"ALBERT (A Lite BERT)\": DistilBertForSequenceClassification,\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\": BertForSequenceClassification,\n",
        "    \"T5 (Text-To-Text Transfer Transformer)\": BertForSequenceClassification,\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": BertForSequenceClassification,\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": BertForSequenceClassification,\n",
        "    \"Longformer (for long documents)\": BertForSequenceClassification,\n",
        "    \"BigBird (sparse attention for long sequences)\": BertForSequenceClassification,\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": RobertaForSequenceClassification\n",
        "}\n",
        "\n",
        "# PBT training function\n",
        "def pbt_training(model, loader, num_generations=3, population_size=3, initial_lr=None, initial_batch_size=None):\n",
        "    # Validate inputs\n",
        "    if model is None or not hasattr(model, 'train'):\n",
        "        logger.error(\"Invalid model provided for PBT. Must be a trainable deep learning model.\")\n",
        "        raise ValueError(\"Model must be a valid deep learning model with train/eval methods.\")\n",
        "    if loader is None or not hasattr(loader, 'dataset'):\n",
        "        logger.error(\"Invalid loader provided for PBT. Must be a valid DataLoader.\")\n",
        "        raise ValueError(\"Loader must be a valid DataLoader instance.\")\n",
        "\n",
        "    # Determine model class based on selected_config\n",
        "    model_name = selected_config.get('model', 'Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)')\n",
        "    model_class = model_mapping.get(model_name, DistilBertForSequenceClassification)\n",
        "    pretrained_model = model_class.from_pretrained(model_class.pretrained if hasattr(model_class, 'pretrained') else 'distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "    # Initialize population with initial hyperparameters if provided (e.g., from Bayesian optimization)\n",
        "    population = []\n",
        "    for i in range(population_size):\n",
        "        lr = initial_lr if initial_lr is not None else np.random.uniform(1e-5, 5e-5)\n",
        "        batch_size = initial_batch_size if initial_batch_size is not None else np.random.choice([8, 16, 32])\n",
        "        individual_model = pretrained_model if i == 0 else pretrained_model  # Reuse first model, clone others if needed\n",
        "        population.append({\n",
        "            'model': individual_model.to(device),\n",
        "            'lr': lr,\n",
        "            'batch_size': batch_size,\n",
        "            'accuracy': 0.0\n",
        "        })\n",
        "\n",
        "    # PBT loop\n",
        "    for generation in range(num_generations):\n",
        "        logger.info(f\"PBT Generation {generation + 1}\")\n",
        "        for individual in population:\n",
        "            model = individual['model']\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=individual['lr'])\n",
        "            temp_loader = DataLoader(loader.dataset, batch_size=individual['batch_size'], shuffle=True)\n",
        "            model.train()\n",
        "            try:\n",
        "                for epoch in range(1):\n",
        "                    for batch in temp_loader:\n",
        "                        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                        labels = batch['labels'].to(device)\n",
        "                        outputs = model(**inputs, labels=labels)\n",
        "                        loss = outputs.loss\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                        torch.cuda.empty_cache()\n",
        "                # Evaluate\n",
        "                model.eval()\n",
        "                predictions, true_labels = [], []\n",
        "                with torch.no_grad():\n",
        "                    for batch in temp_loader:\n",
        "                        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                        labels = batch['labels'].to(device)\n",
        "                        outputs = model(**inputs)\n",
        "                        preds = torch.argmax(outputs.logits, dim=1)\n",
        "                        predictions.extend(preds.cpu().numpy())\n",
        "                        true_labels.extend(labels.cpu().numpy())\n",
        "                individual['accuracy'] = accuracy_score(true_labels, predictions)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during training/evaluation for generation {generation + 1}: {str(e)}\")\n",
        "                individual['accuracy'] = 0.0  # Assign low accuracy to handle failures\n",
        "\n",
        "        # Exploit and explore\n",
        "        population = sorted(population, key=lambda x: x['accuracy'], reverse=True)\n",
        "        best_individual = population[0]\n",
        "        for i in range(1, len(population)):\n",
        "            if np.random.random() < 0.5:  # Exploit\n",
        "                population[i]['lr'] = best_individual['lr']\n",
        "                population[i]['batch_size'] = best_individual['batch_size']\n",
        "            else:  # Explore\n",
        "                population[i]['lr'] *= np.random.uniform(0.8, 1.2)\n",
        "                population[i]['batch_size'] = np.random.choice([8, 16, 32])\n",
        "                population[i]['model'] = pretrained_model.to(device)  # Reset model for exploration\n",
        "\n",
        "        # Clear memory after each generation\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    best_individual = max(population, key=lambda x: x['accuracy'])\n",
        "    logger.info(f\"PBT Best Hyperparameters: lr={best_individual['lr']:.6f}, batch_size={best_individual['batch_size']}, accuracy={best_individual['accuracy']:.3f}\")\n",
        "    return best_individual['model'], best_individual['lr'], best_individual['batch_size']\n",
        "\n",
        "# Apply PBT if deep learning model\n",
        "if \"Traditional ML\" not in selected_config.get('approach', ''):\n",
        "    try:\n",
        "        # Use Bayesian-optimized hyperparameters as initial values if available\n",
        "        initial_lr = None\n",
        "        initial_batch_size = None\n",
        "        if 'Deep Learning' in selected_config.get('approach', '') and 'optimal_params' in globals():\n",
        "            initial_lr, initial_batch_size = optimal_params[0], int(optimal_params[1]) if len(optimal_params) >= 2 else (None, None)\n",
        "            logger.info(f\"Using Bayesian-optimized initial hyperparameters: lr={initial_lr}, batch_size={initial_batch_size}\")\n",
        "\n",
        "        # Validate selected_model before PBT\n",
        "        if not hasattr(selected_model, 'train'):\n",
        "            logger.warning(\"Selected model is not a deep learning model. Initializing new DistilBert model for PBT.\")\n",
        "            selected_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)\n",
        "\n",
        "        selected_model, learning_rate, batch_size = pbt_training(selected_model, loader, initial_lr=initial_lr, initial_batch_size=initial_batch_size)\n",
        "        loader = DataLoader(loader.dataset, batch_size=batch_size, shuffle=True)\n",
        "        logger.info(\"PBT completed. Updated loader with best batch_size.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"PBT failed with error: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "G8M-6shaHscp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8d: Optimizer Agent - Lightweight NAS (DARTS), Pruning, and Quantization\n",
        "# Purpose: Optimize the deep learning model's architecture and efficiency.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.quantization\n",
        "import logging\n",
        "from transformers import (\n",
        "    DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
        ")\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure logging is set up\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mapping of model names to their classes for architecture validation\n",
        "model_mapping = {\n",
        "    \"Shallow Neural Network (MLP-like)\": DistilBertForSequenceClassification,\n",
        "    \"Recurrent Neural Network (LSTM/GRU-like)\": DistilBertForSequenceClassification,\n",
        "    \"Convolutional Neural Network (CNN-like)\": DistilBertForSequenceClassification,\n",
        "    \"Bidirectional LSTM (BiLSTM-like)\": BertForSequenceClassification,\n",
        "    \"Gated Recurrent Unit (GRU-like)\": BertForSequenceClassification,\n",
        "    \"Feedforward Neural Network (FNN-like)\": DistilBertForSequenceClassification,\n",
        "    \"Hybrid (CNN-RNN)\": BertForSequenceClassification,\n",
        "    \"Deep Learning (Custom Transformer)\": BertForSequenceClassification,\n",
        "    \"Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)\": DistilBertForSequenceClassification,\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers)\": BertForSequenceClassification,\n",
        "    \"RoBERTa (Robustly Optimized BERT Pretraining Approach)\": RobertaForSequenceClassification,\n",
        "    \"ALBERT (A Lite BERT)\": DistilBertForSequenceClassification,\n",
        "    \"XLNet (Generalized Autoregressive Pretraining)\": BertForSequenceClassification,\n",
        "    \"T5 (Text-To-Text Transfer Transformer)\": BertForSequenceClassification,\n",
        "    \"DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\": BertForSequenceClassification,\n",
        "    \"ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\": BertForSequenceClassification,\n",
        "    \"Longformer (for long documents)\": BertForSequenceClassification,\n",
        "    \"BigBird (sparse attention for long sequences)\": BertForSequenceClassification,\n",
        "    \"Deep Learning (Advanced Pretrained Transformer, e.g., PaLM-like)\": RobertaForSequenceClassification\n",
        "}\n",
        "\n",
        "# Lightweight NAS with DARTS\n",
        "def apply_darts(model):\n",
        "    if not hasattr(model, 'eval') or not hasattr(model, 'train'):\n",
        "        logger.error(\"Model is not a valid deep learning model for DARTS optimization.\")\n",
        "        raise ValueError(\"Model must be a trainable deep learning model.\")\n",
        "\n",
        "    model_name = selected_config.get('model', 'Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)')\n",
        "    if isinstance(model, DistilBertForSequenceClassification):\n",
        "        if hasattr(model.distilbert, 'transformer') and hasattr(model.distilbert.transformer, 'layer'):\n",
        "            original_layers = len(model.distilbert.transformer.layer)\n",
        "            model.distilbert.transformer.layer = nn.ModuleList(model.distilbert.transformer.layer[:3])  # Reduce to 3 layers\n",
        "            logger.info(f\"Applied DARTS: Reduced DistilBert layers from {original_layers} to 3.\")\n",
        "        else:\n",
        "            logger.warning(\"DARTS not fully applicable to DistilBert model due to missing transformer layers.\")\n",
        "    elif isinstance(model, (BertForSequenceClassification, RobertaForSequenceClassification)):\n",
        "        if hasattr(model.bert, 'encoder') and hasattr(model.bert.encoder, 'layer'):\n",
        "            original_layers = len(model.bert.encoder.layer)\n",
        "            model.bert.encoder.layer = nn.ModuleList(model.bert.encoder.layer[:3])  # Reduce to 3 layers\n",
        "            logger.info(f\"Applied DARTS: Reduced {model_name} layers from {original_layers} to 3.\")\n",
        "        else:\n",
        "            logger.warning(f\"DARTS not fully applicable to {model_name} due to missing encoder layers.\")\n",
        "    else:\n",
        "        logger.warning(f\"DARTS not implemented for model type: {model_name}\")\n",
        "    return model\n",
        "\n",
        "# Pruning\n",
        "def apply_pruning(model):\n",
        "    if not hasattr(model, 'named_modules'):\n",
        "        logger.error(\"Model is not a valid deep learning model for pruning.\")\n",
        "        raise ValueError(\"Model must be a valid deep learning model with named modules.\")\n",
        "\n",
        "    model_name = selected_config.get('model', 'Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)')\n",
        "    if isinstance(model, (DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification)):\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                prune.l1_unstructured(module, name='weight', amount=0.3)  # Prune 30% of weights using L1 norm\n",
        "        logger.info(f\"Applied pruning to {model_name} model (30% of Linear layer weights).\")\n",
        "    else:\n",
        "        logger.warning(f\"Pruning not implemented for model type: {model_name}\")\n",
        "    return model\n",
        "\n",
        "# Quantization\n",
        "def apply_quantization(model):\n",
        "    if not hasattr(model, 'eval'):\n",
        "        logger.error(\"Model is not a valid deep learning model for quantization.\")\n",
        "        raise ValueError(\"Model must be a valid deep learning model with eval method.\")\n",
        "\n",
        "    model_name = selected_config.get('model', 'Deep Learning (Lightweight Pretrained Transformer, e.g., DistilBERT-like)')\n",
        "    if isinstance(model, (DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification)):\n",
        "        model.eval()  # Required for quantization\n",
        "        try:\n",
        "            model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "            logger.info(f\"Applied quantization to {model_name} model (dynamic qint8 on Linear layers).\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Quantization failed: {str(e)}\")\n",
        "            raise\n",
        "    else:\n",
        "        logger.warning(f\"Quantization not implemented for model type: {model_name}\")\n",
        "    return model\n",
        "\n",
        "# Apply optimizations if deep learning model\n",
        "if \"Traditional ML\" not in selected_config.get('approach', ''):\n",
        "    try:\n",
        "        # Validate selected_model\n",
        "        if not hasattr(selected_model, 'eval') or not hasattr(selected_model, 'train'):\n",
        "            logger.error(\"Selected model is not a valid deep learning model for optimization.\")\n",
        "            raise ValueError(\"Selected model must be a deep learning model with eval/train methods.\")\n",
        "\n",
        "        # Apply optimizations sequentially\n",
        "        selected_model = apply_darts(selected_model)\n",
        "        selected_model = apply_pruning(selected_model)\n",
        "        selected_model = apply_quantization(selected_model)\n",
        "        logger.info(\"All optimizations (DARTS, Pruning, Quantization) applied successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Optimization failed with error: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "1yl2dsCyIVa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8e: Evaluator Agent - Fine-Tuned BERT for Evaluation and Retraining\n",
        "# Purpose: Evaluate the optimized model using a fine-tuned BERT (for DL) or direct prediction (for ML) and retrain if needed.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure logging is set up\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define target_accuracy (example value; adjust as needed based on project requirements)\n",
        "target_accuracy = 0.85  # Placeholder; replace with your desired threshold\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "vocab_size = tokenizer.vocab_size  # Should be 30522 for bert-base-uncased\n",
        "\n",
        "# Evaluator agent function\n",
        "def evaluator_agent(model, loader, X=None, y=None):\n",
        "    # Validate inputs based on model type\n",
        "    if model is None:\n",
        "        logger.error(\"Invalid model provided for evaluation. Model is None.\")\n",
        "        raise ValueError(\"Model must not be None.\")\n",
        "    if \"Traditional ML\" not in selected_config.get('approach', '') and not hasattr(model, 'eval'):\n",
        "        logger.error(\"Invalid model provided for evaluation. Must be a valid deep learning model with eval method.\")\n",
        "        raise ValueError(\"Model must be a valid deep learning model with eval method.\")\n",
        "    if loader is None and (\"Traditional ML\" not in selected_config.get('approach', '')):\n",
        "        logger.error(\"Invalid loader provided for evaluation. Must be a valid DataLoader for deep learning models.\")\n",
        "        raise ValueError(\"Loader must be a valid DataLoader instance for deep learning models.\")\n",
        "\n",
        "    # Debug: Inspect loader and dataset before reconstruction\n",
        "    logger.info(f\"Original Loader type: {type(loader)}\")\n",
        "    logger.info(f\"Original Loader dataset type: {type(loader.dataset)}\")\n",
        "    try:\n",
        "        sample_batch = next(iter(loader))\n",
        "        logger.info(f\"Original Sample batch keys: {list(sample_batch.keys())}\")\n",
        "        if 'input_ids' in sample_batch:\n",
        "            logger.info(f\"Original Sample input_ids shape: {sample_batch['input_ids'].shape}, min: {sample_batch['input_ids'].min().item()}, max: {sample_batch['input_ids'].max().item()}\")\n",
        "        if 'labels' in sample_batch:\n",
        "            logger.info(f\"Original Sample labels shape: {sample_batch['labels'].shape}, min: {sample_batch['labels'].min().item()}, max: {sample_batch['labels'].max().item()}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not inspect original sample batch: {str(e)}\")\n",
        "\n",
        "    # Reconstruct loader for deep learning if necessary\n",
        "    reconstructed_loader = loader  # Default to original loader\n",
        "    if \"Traditional ML\" not in selected_config.get('approach', ''):\n",
        "        # Check if loader provides BERT-compatible inputs\n",
        "        sample_batch = next(iter(loader))\n",
        "        if 'input_ids' not in sample_batch or 'attention_mask' not in sample_batch or sample_batch['input_ids'].max().item() >= vocab_size or sample_batch['input_ids'].min().item() < 0:\n",
        "            logger.warning(\"Loader does not provide BERT-compatible inputs or contains invalid input_ids. Reconstructing loader...\")\n",
        "            # Extract raw text and labels from preprocessed_data or processed_dataset.csv\n",
        "            texts = None\n",
        "            labels = None\n",
        "            if 'preprocessed_data' in globals():\n",
        "                logger.info(f\"preprocessed_data keys: {list(preprocessed_data.keys())}\")\n",
        "                possible_text_keys = ['texts', 'text', 'review', 'content', 'sentence']\n",
        "                for key in possible_text_keys:\n",
        "                    if key in preprocessed_data:\n",
        "                        texts = preprocessed_data[key]\n",
        "                        labels = preprocessed_data['y']\n",
        "                        logger.info(f\"Using preprocessed_data['{key}'] as text source.\")\n",
        "                        break\n",
        "            if texts is None:\n",
        "                df_path = os.path.join(drive_path, 'processed_dataset.csv')\n",
        "                if not os.path.exists(df_path):\n",
        "                    logger.error(f\"Dataset CSV {df_path} not found. Ensure it exists in Google Drive.\")\n",
        "                    raise FileNotFoundError(f\"Dataset CSV {df_path} not found.\")\n",
        "                df = pd.read_csv(df_path)\n",
        "                for col in df.columns:\n",
        "                    if col.lower() in possible_text_keys:\n",
        "                        texts = df[col]\n",
        "                        labels = df['sentiment'].map({'positive': 1, 'negative': 0, 'neutral': 2}).fillna(1).astype(int)\n",
        "                        logger.info(f\"Using df['{col}'] as text source from processed_dataset.csv.\")\n",
        "                        break\n",
        "                if texts is None:\n",
        "                    logger.error(\"No text column found in dataset. Expected one of: \" + \", \".join(possible_text_keys))\n",
        "                    raise ValueError(\"Dataset must contain a text column for tokenization.\")\n",
        "            # Tokenize the text\n",
        "            encoded = tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "            if encoded['input_ids'].max().item() >= vocab_size or encoded['input_ids'].min().item() < 0:\n",
        "                logger.error(f\"Tokenized input_ids out of range: min={encoded['input_ids'].min().item()}, max={encoded['input_ids'].max().item()}, vocab_size={vocab_size}\")\n",
        "                raise ValueError(\"Tokenized input_ids contain invalid indices.\")\n",
        "            dataset = TensorDataset(encoded['input_ids'], encoded['attention_mask'], torch.tensor(labels))\n",
        "            reconstructed_loader = DataLoader(dataset, batch_size=loader.batch_size if hasattr(loader, 'batch_size') else 32, shuffle=True)\n",
        "            logger.info(\"Reconstructed loader with BERT-compatible inputs.\")\n",
        "            # Debug: Inspect reconstructed loader\n",
        "            sample_batch = next(iter(reconstructed_loader))\n",
        "            logger.info(f\"Reconstructed Sample batch keys: {list(sample_batch.keys())}\")\n",
        "            if 'input_ids' in sample_batch:\n",
        "                logger.info(f\"Reconstructed Sample input_ids shape: {sample_batch['input_ids'].shape}, min: {sample_batch['input_ids'].min().item()}, max: {sample_batch['input_ids'].max().item()}\")\n",
        "\n",
        "    # Load or fine-tune BERT evaluator (skip for Traditional ML)\n",
        "    if \"Traditional ML\" not in selected_config.get('approach', ''):\n",
        "        evaluator = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "        evaluator_path = os.path.join(drive_path, 'fine_tuned_bert_evaluator.pt')\n",
        "        if os.path.exists(evaluator_path):\n",
        "            try:\n",
        "                evaluator.load_state_dict(torch.load(evaluator_path))\n",
        "                logger.info(\"Loaded fine-tuned BERT evaluator from checkpoint.\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load evaluator checkpoint: {str(e)}. Fine-tuning from scratch.\")\n",
        "        else:\n",
        "            logger.info(\"No evaluator checkpoint found. Fine-tuning BERT evaluator...\")\n",
        "            optimizer = optim.AdamW(evaluator.parameters(), lr=2e-5)\n",
        "            evaluator.train()\n",
        "            try:\n",
        "                for epoch in range(2):\n",
        "                    for batch in tqdm(reconstructed_loader, desc=\"Fine-Tuning BERT Evaluator\"):\n",
        "                        # Validate input_ids range\n",
        "                        if 'input_ids' in batch:\n",
        "                            input_ids = batch['input_ids']\n",
        "                            if input_ids.max().item() >= vocab_size or input_ids.min().item() < 0:\n",
        "                                logger.error(f\"Invalid input_ids in batch: min={input_ids.min().item()}, max={input_ids.max().item()}, vocab_size={vocab_size}\")\n",
        "                                raise ValueError(\"Batch input_ids contain invalid indices.\")\n",
        "                        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                        labels = batch['labels'].to(device)\n",
        "                        outputs = evaluator(**inputs, labels=labels)\n",
        "                        loss = outputs.loss\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                        torch.cuda.empty_cache()\n",
        "                torch.save(evaluator.state_dict(), evaluator_path)\n",
        "                logger.info(\"Fine-tuned BERT evaluator saved to checkpoint.\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during fine-tuning: {str(e)}\")\n",
        "                raise\n",
        "    else:\n",
        "        evaluator = None\n",
        "\n",
        "    # Evaluate the optimized model\n",
        "    predictions, true_labels = [], []\n",
        "    if \"Traditional ML\" in selected_config.get('approach', ''):\n",
        "        logger.info(\"Evaluating Traditional ML model with direct prediction.\")\n",
        "        if X is None or y is None:\n",
        "            df_path = os.path.join(drive_path, 'processed_dataset.csv')\n",
        "            if not os.path.exists(df_path):\n",
        "                raise FileNotFoundError(f\"Dataset CSV {df_path} not found. Ensure it exists in Google Drive.\")\n",
        "            df = pd.read_csv(df_path)\n",
        "            if 'vectorizer' not in globals():\n",
        "                logger.error(\"Vectorizer not available for Traditional ML evaluation. Ensure it was defined in Cell 8b.\")\n",
        "                raise ValueError(\"Vectorizer must be defined for Traditional ML evaluation.\")\n",
        "            X_test = vectorizer.transform(df['review'].fillna(''))\n",
        "            predictions = model.predict(X_test)\n",
        "            true_labels = y if y is not None else df['sentiment'].map({'positive': 1, 'negative': 0, 'neutral': 2}).fillna(1).astype(int)\n",
        "            # Log prediction and label shapes for debugging\n",
        "            logger.info(f\"Traditional ML predictions shape: {np.array(predictions).shape}, true_labels shape: {np.array(true_labels).shape}\")\n",
        "        else:\n",
        "            predictions = model.predict(X)\n",
        "            true_labels = y\n",
        "            logger.info(f\"Traditional ML predictions shape: {np.array(predictions).shape}, true_labels shape: {np.array(true_labels).shape}\")\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                for batch in tqdm(reconstructed_loader, desc=\"Evaluating Optimized Model\"):\n",
        "                    # Validate input_ids range\n",
        "                    if 'input_ids' in batch:\n",
        "                        input_ids = batch['input_ids']\n",
        "                        if input_ids.max().item() >= vocab_size or input_ids.min().item() < 0:\n",
        "                            logger.error(f\"Invalid input_ids in batch: min={input_ids.min().item()}, max={input_ids.max().item()}, vocab_size={vocab_size}\")\n",
        "                            raise ValueError(\"Batch input_ids contain invalid indices.\")\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels = batch['labels'].to(device)\n",
        "                    outputs = model(**inputs)\n",
        "                    preds = torch.argmax(outputs.logits, dim=1) if hasattr(outputs, 'logits') else torch.zeros_like(labels)\n",
        "                    predictions.extend(preds.cpu().numpy())\n",
        "                    true_labels.extend(labels.cpu().numpy())\n",
        "                    torch.cuda.empty_cache()\n",
        "                # Log prediction and label shapes for debugging\n",
        "                logger.info(f\"Deep Learning predictions shape: {np.array(predictions).shape}, true_labels shape: {np.array(true_labels).shape}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during evaluation: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "    logger.info(f\"Evaluation Metrics: Accuracy={accuracy:.3f}, F1={f1:.3f}, Target={target_accuracy:.3f}\")\n",
        "    return accuracy, f1\n",
        "\n",
        "# Evaluate the optimized model\n",
        "try:\n",
        "    # Use preprocessed_data from Cell 8b if available, otherwise set X and y to None\n",
        "    X_eval = preprocessed_data['X'] if 'preprocessed_data' in globals() and 'X' in preprocessed_data else None\n",
        "    y_eval = preprocessed_data['y'] if 'preprocessed_data' in globals() and 'y' in preprocessed_data else None\n",
        "    accuracy, f1 = evaluator_agent(selected_model, loader, X_eval, y_eval)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Initial evaluation failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Trigger retraining if target not met\n",
        "if accuracy < target_accuracy:\n",
        "    logger.info(\"Target accuracy not met. Triggering retraining.\")\n",
        "    try:\n",
        "        if \"Traditional ML\" in selected_config.get('approach', ''):\n",
        "            if X_eval is None or y_eval is None:\n",
        "                logger.error(\"X and y must be provided for Traditional ML retraining.\")\n",
        "                raise ValueError(\"X and y are required for Traditional ML retraining.\")\n",
        "            selected_model.fit(X_eval, y_eval) if hasattr(selected_model, 'fit') else logger.warning(\"No fit method for retraining.\")\n",
        "        else:\n",
        "            selected_model.train()\n",
        "            optimizer = optim.AdamW(selected_model.parameters(), lr=learning_rate if 'learning_rate' in globals() else 2e-5)\n",
        "            for epoch in range(2):  # Increased epochs for better retraining\n",
        "                for batch in tqdm(loader, desc=\"Retraining\"):\n",
        "                    # Validate input_ids range\n",
        "                    if 'input_ids' in batch:\n",
        "                        input_ids = batch['input_ids']\n",
        "                        if input_ids.max().item() >= vocab_size or input_ids.min().item() < 0:\n",
        "                            logger.error(f\"Invalid input_ids in batch: min={input_ids.min().item()}, max={input_ids.max().item()}, vocab_size={vocab_size}\")\n",
        "                            raise ValueError(\"Batch input_ids contain invalid indices.\")\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels = batch['labels'].to(device)\n",
        "                    outputs = selected_model(**inputs, labels=labels)\n",
        "                    loss = outputs.loss if hasattr(outputs, 'loss') else torch.tensor(0.0)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    torch.cuda.empty_cache()\n",
        "        # Re-evaluate after retraining\n",
        "        accuracy, f1 = evaluator_agent(selected_model, loader, X_eval, y_eval)\n",
        "        logger.info(f\"Post-retraining Metrics: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Retraining failed: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "vnLB5gR6Ioz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8f: Feedback Collection and RL/MAML Reward Update\n",
        "# Purpose: Collect user feedback and update RL/MAML rewards.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Ensure logging is set up (consistent with previous cells)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Verify drive_path exists\n",
        "if 'drive_path' not in globals():\n",
        "    logger.error(\"drive_path is not defined. Ensure it is set in an earlier cell (e.g., Cell 8a).\")\n",
        "    raise ValueError(\"drive_path must be defined.\")\n",
        "\n",
        "feedback_path = os.path.join(drive_path, 'user_feedback.csv')\n",
        "\n",
        "# Collect feedback using input() since 'forms' module is unavailable\n",
        "try:\n",
        "    print(\"Please provide feedback on the model's performance:\")\n",
        "    satisfaction_input = input(\"How satisfied are you with the model's performance (1-5)? [default=3]: \") or \"3\"\n",
        "    satisfaction = int(satisfaction_input)\n",
        "    if not (1 <= satisfaction <= 5):\n",
        "        logger.warning(f\"Satisfaction score {satisfaction} is out of range (1-5). Clamping to nearest valid value.\")\n",
        "        satisfaction = max(1, min(5, satisfaction))\n",
        "\n",
        "    comments = input(\"Any additional comments? [default='']: \") or \"\"\n",
        "    logger.info(f\"User Feedback: Satisfaction={satisfaction}, Comments={comments}\")\n",
        "except ValueError as e:\n",
        "    logger.error(f\"Invalid input for satisfaction score. Expected an integer between 1 and 5, got: {satisfaction_input}. Error: {str(e)}\")\n",
        "    raise ValueError(\"Satisfaction score must be an integer between 1 and 5.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error collecting feedback: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Save to CSV\n",
        "try:\n",
        "    file_exists = os.path.exists(feedback_path) and os.path.getsize(feedback_path) > 0\n",
        "    with open(feedback_path, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if not file_exists:\n",
        "            writer.writerow(['Satisfaction', 'Comments', 'Timestamp'])\n",
        "        writer.writerow([satisfaction, comments, pd.Timestamp.now()])\n",
        "    logger.info(f\"Feedback saved to {feedback_path}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error saving feedback to CSV: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Update RL/MAML Rewards (Placeholder for MAML integration)\n",
        "def update_maml_rewards(satisfaction):\n",
        "    reward = (satisfaction - 3) / 2  # Normalize to [-1, 1]\n",
        "    logger.info(f\"Updated MAML reward: {reward:.2f}\")\n",
        "    # Placeholder: In a real MAML setup, this reward would update the meta-learner's loss\n",
        "    return reward\n",
        "\n",
        "reward = update_maml_rewards(satisfaction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZXYxtAmNOLh",
        "outputId": "e1468ff5-bde0-4f94-9484-756ad43aa469"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide feedback on the model's performance:\n",
            "How satisfied are you with the model's performance (1-5)? [default=3]: 5\n",
            "Any additional comments? [default='']: well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8g: Final Output and Cleanup\n",
        "# Purpose: Display final metrics and clean up memory.\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import logging\n",
        "import psutil\n",
        "\n",
        "# Ensure logging is set up (consistent with previous cells)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Function to monitor and log memory usage\n",
        "def log_memory_usage():\n",
        "    try:\n",
        "        process = psutil.Process()\n",
        "        mem_info = process.memory_info()\n",
        "        ram_usage_mb = mem_info.rss / 1024 ** 2\n",
        "        logger.info(f\"Current RAM usage: {ram_usage_mb:.2f} MB\")\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_mem = torch.cuda.memory_allocated() / 1024 ** 2\n",
        "            logger.info(f\"Current GPU memory usage: {gpu_mem:.2f} MB\")\n",
        "        else:\n",
        "            logger.info(\"CUDA not available; GPU memory usage not reported.\")\n",
        "        return ram_usage_mb\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error logging memory usage: {str(e)}\")\n",
        "        return 0\n",
        "\n",
        "# Display final metrics\n",
        "try:\n",
        "    print(\"\\nFinal Model Configuration:\")\n",
        "    print(f\"Approach: {selected_config['approach']}\")\n",
        "    print(f\"Best Accuracy Achieved: {accuracy:.3f}\")\n",
        "    print(f\"Target Accuracy: {target_accuracy:.3f}\")\n",
        "    print(f\"Target Achieved: {accuracy >= target_accuracy}\")\n",
        "    print(f\"F1 Score: {f1:.3f}\")\n",
        "except NameError as e:\n",
        "    logger.error(f\"Required variables (e.g., accuracy, f1, selected_config, target_accuracy) are not defined. Ensure Cell 8e executed successfully: {str(e)}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error displaying final metrics: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Memory Cleanup\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    log_memory_usage()\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during memory cleanup: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DccvRL0gNOdH",
        "outputId": "e2a1e758-7b25-4d21-f731-af173f98c35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Configuration:\n",
            "Approach: Traditional ML\n",
            "Best Accuracy Achieved: 0.743\n",
            "Target Accuracy: 0.850\n",
            "Target Achieved: False\n",
            "F1 Score: 0.743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DG-TjO4lg4_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}