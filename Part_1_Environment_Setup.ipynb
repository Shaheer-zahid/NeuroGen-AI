{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define source and destination paths\n",
        "source_path1 = '/content/drive/MyDrive/Colab Notebooks/Part_1_Environment_Setup.ipynb'\n",
        "source_path2 = '/content/drive/MyDrive/Colab Notebooks/Data_Analysis_Quality_Preprocessing.ipynb'\n",
        "dest_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "\n",
        "# Create destination directory if it doesn't exist\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "logger.info(\"Destination directory ensured: %s\", dest_dir)\n",
        "\n",
        "# Move notebooks\n",
        "for source in [source_path1, source_path2]:\n",
        "    if os.path.exists(source):\n",
        "        shutil.move(source, os.path.join(dest_dir, os.path.basename(source)))\n",
        "        logger.info(\"Moved %s to %s\", source, dest_dir)\n",
        "    else:\n",
        "        logger.warning(\"%s not found.\", source)\n",
        "\n",
        "# Verify moved files\n",
        "!ls \"/content/drive/MyDrive/Sentiment_Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFY_A_uWbhci",
        "outputId": "6e525453-88bb-4388-e4bb-095878d06467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:/content/drive/MyDrive/Colab Notebooks/Part_1_Environment_Setup.ipynb not found.\n",
            "WARNING:__main__:/content/drive/MyDrive/Colab Notebooks/Data_Analysis_Quality_Preprocessing.ipynb not found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " coordinator_logs.txt\n",
            " Data_Analysis_Quality_Preprocessing.ipynb\n",
            " data_analysis_report.json\n",
            " dataset_data.json\n",
            " deploy\n",
            " explainability_output\n",
            " explainability_outputs\n",
            "'fine_tuned_traditional_ml_(random_forest-like).joblib'\n",
            " hyperparams.json\n",
            " model_details.json\n",
            " model_integration.json\n",
            " Notebook_5_CodeGen_Explainability.ipynb\n",
            " Part_1_Environment_Setup.ipynb\n",
            " Part_3_Model_Training_and_Evaluation.ipynb\n",
            " part3_output.json\n",
            " part4_output.json\n",
            " preprocessed_data.pt\n",
            " processed_dataset.csv\n",
            " quality_check_report.json\n",
            " selected_config_checkpoint.pkl\n",
            " Sentiment_Analysis_Model_Optimization.ipynb\n",
            " trained_traditional_ml_random_forest-like_encoder.joblib\n",
            " trained_traditional_ml_random_forest-like.joblib\n",
            " trained_traditional_ml_random_forest-like_vectorizer.joblib\n",
            "'train_traditional_ml_(random_forest-like).py'\n",
            " train_traditional_ml_random_forest-like.py\n",
            " user_dataset_prompt.json\n",
            " user_feedback.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVJJIkCIQ8Lc",
        "outputId": "f61d631c-33a9-4208-da97-67407253ea41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.14.1\n",
            "Uninstalling scipy-1.14.1:\n",
            "  Successfully uninstalled scipy-1.14.1\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gpy 1.13.2 requires scipy<=1.12.0,>=1.3.0, which is not installed.\n",
            "paramz 0.9.6 requires scipy, which is not installed.\n",
            "gpyopt 1.2.6 requires scipy>=0.16, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scipy, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "scikit-learn 1.6.1 requires scipy>=1.6.0, which is not installed.\n",
            "pytensor 2.30.3 requires scipy<2,>=1, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "osqp 1.0.4 requires scipy>=0.13.2, which is not installed.\n",
            "albumentations 2.0.7 requires scipy>=1.10.0, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "spacy 3.8.6 requires thinc<8.4.0,>=8.3.4, but you have thinc 8.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting scipy==1.14.1\n",
            "  Using cached scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy==1.14.1) (1.26.4)\n",
            "Using cached scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "Installing collected packages: scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "gpy 1.13.2 requires scipy<=1.12.0,>=1.3.0, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.14.1\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping numba as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ray==2.46.0 in /usr/local/lib/python3.11/dist-packages (2.46.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (5.29.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray==2.46.0) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.46.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.46.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.46.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.46.0) (0.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema->ray==2.46.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.46.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.46.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.46.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.46.0) (2025.4.26)\n",
            "Requirement already satisfied: langchain==0.3.25 in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (4.13.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain==0.3.25) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.25) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.25) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.25) (3.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25) (1.3.1)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (2025.4.26)\n",
            "Requirement already satisfied: nlpaug==1.1.11 in /usr/local/lib/python3.11/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (3.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug==1.1.11) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (4.13.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug==1.1.11) (1.7.1)\n",
            "Requirement already satisfied: psutil==7.0.0 in /usr/local/lib/python3.11/dist-packages (7.0.0)\n",
            "Requirement already satisfied: thinc==8.2.3 in /usr/local/lib/python3.11/dist-packages (8.2.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (0.7.11)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (2.0.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (0.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (75.2.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (2.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (24.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.2.3) (1.26.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.3) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.3) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.3) (0.4.1)\n",
            "Requirement already satisfied: GPyOpt==1.2.6 in /usr/local/lib/python3.11/dist-packages (1.2.6)\n",
            "Requirement already satisfied: GPy==1.13.2 in /usr/local/lib/python3.11/dist-packages (1.13.2)\n",
            "Requirement already satisfied: paramz==0.9.6 in /usr/local/lib/python3.11/dist-packages (0.9.6)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (1.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (1.17.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from paramz==0.9.6) (4.4.2)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu118) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu118) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.2.0\n",
            "\u001b[2K    Uninstalling triton-3.2.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.2.0\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torch-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0+cu118 triton-2.1.0\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Initial Setup and Library Installation\n",
        "# Purpose: Install required libraries with specific versions for compatibility.\n",
        "# Note: Enforces scipy 1.14.1 and handles runtime restart requirement.\n",
        "# Strategy: Clean install, isolate conflicts, and verify integrity.\n",
        "\n",
        "# Update pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Clean install numpy and scipy\n",
        "!pip uninstall -y numpy scipy\n",
        "!pip install numpy==1.26.4\n",
        "!pip install scipy==1.14.1\n",
        "\n",
        "# Uninstall conflicting pre-installed packages\n",
        "!pip uninstall -y tensorflow numba\n",
        "\n",
        "# Install project-specific libraries\n",
        "!pip install ray==2.46.0\n",
        "!pip install langchain==0.3.25\n",
        "!pip install transformers==4.51.3\n",
        "!pip install nlpaug==1.1.11\n",
        "!pip install psutil==7.0.0\n",
        "\n",
        "# Install thinc with compatible version\n",
        "!pip install thinc==8.2.3\n",
        "\n",
        "# Install GPyOpt, GPy, and paramz with scipy constraint\n",
        "!pip install GPyOpt==1.2.6 --no-deps\n",
        "!pip install GPy==1.13.2 --no-deps\n",
        "!pip install paramz==0.9.6\n",
        "\n",
        "!pip install torch==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAdE0c6Sbf5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installations\n",
        "import numpy\n",
        "import scipy\n",
        "import ray\n",
        "import langchain\n",
        "import transformers\n",
        "import nlpaug\n",
        "import psutil\n",
        "import GPyOpt\n",
        "import GPy\n",
        "import paramz\n",
        "import thinc\n",
        "import torch\n",
        "\n",
        "print(f\"Numpy version: {numpy.__version__}\")\n",
        "print(f\"Scipy version: {scipy.__version__}\")\n",
        "print(f\"Ray version: {ray.__version__}\")\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Thinc version: {thinc.__version__}\")\n",
        "print(f\"NLP Augmentation version: {nlpaug.__version__}\")\n",
        "print(f\"Psutil version: {psutil.__version__}\")\n",
        "print(f\"GPyOpt version: {GPyOpt.__version__}\")\n",
        "print(f\"GPy version: {GPy.__version__}\")\n",
        "print(f\"Paramz version: {paramz.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "\n",
        "# Check versions\n",
        "assert numpy.__version__ == \"1.26.4\", \"Numpy version mismatch\"\n",
        "assert scipy.__version__ == \"1.14.1\", \"Scipy version mismatch\"\n",
        "assert ray.__version__ == \"2.46.0\", \"Ray version mismatch\"\n",
        "assert langchain.__version__ == \"0.3.25\", \"LangChain version mismatch\"\n",
        "assert transformers.__version__ == \"4.51.3\", \"Transformers version mismatch\"\n",
        "assert thinc.__version__ == \"8.2.3\", \"Thinc version mismatch\"\n",
        "assert nlpaug.__version__ == \"1.1.11\", \"NLP Augmentation version mismatch\"\n",
        "assert psutil.__version__ == \"7.0.0\", \"Psutil version mismatch\"\n",
        "assert GPyOpt.__version__ == \"1.2.6\", \"GPyOpt version mismatch\"\n",
        "assert GPy.__version__ == \"1.13.2\", \"GPy version mismatch\"\n",
        "assert paramz.__version__ == \"0.9.6\", \"Paramz version mismatch\"\n",
        "assert torch.__version__.startswith(\"2.1.0\"), \"Torch version mismatch\"\n",
        "print(\"All libraries installed and verified successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTtCSm45Ws_n",
        "outputId": "31f6e9ae-933b-453f-984a-3b6b8ab255e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy version: 1.26.4\n",
            "Scipy version: 1.14.1\n",
            "Ray version: 2.46.0\n",
            "LangChain version: 0.3.25\n",
            "Transformers version: 4.51.3\n",
            "Thinc version: 8.2.3\n",
            "NLP Augmentation version: 1.1.11\n",
            "Psutil version: 7.0.0\n",
            "GPyOpt version: 1.2.6\n",
            "GPy version: 1.13.2\n",
            "Paramz version: 0.9.6\n",
            "Torch version: 2.1.0+cu118\n",
            "All libraries installed and verified successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Google Drive Integration\n",
        "# Purpose: Mount Google Drive for persistent storage of models, weights, feedback, and MAML parameters.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Attempt to mount Google Drive with retry logic\n",
        "max_attempts = 2\n",
        "for attempt in range(max_attempts):\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        project_dir = '/content/drive/MyDrive/Sentiment_Project'\n",
        "        os.makedirs(project_dir, exist_ok=True)\n",
        "        logger.info(\"Google Drive mounted successfully at %s\", project_dir)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        logger.error(\"Attempt %d failed: %s\", attempt + 1, e)\n",
        "        if attempt == max_attempts - 1:\n",
        "            raise SystemExit(\"Drive mounting failed after maximum attempts. Please check authentication.\")\n",
        "        continue\n",
        "\n",
        "# Verify Drive access with write test\n",
        "try:\n",
        "    test_file = os.path.join(project_dir, 'test_write.txt')\n",
        "    with open(test_file, 'w') as f:\n",
        "        f.write('Test successful')\n",
        "    os.remove(test_file)\n",
        "    logger.info(\"Drive contents verified with write access.\")\n",
        "    !ls /content/drive/MyDrive\n",
        "except Exception as e:\n",
        "    logger.error(\"Error verifying Drive access: %s\", e)\n",
        "    raise SystemExit(\"Drive verification failed. Check permissions or connection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZoRbYflhZLd",
        "outputId": "f78dcc49-76d6-4020-e4ce-402e5fd2d2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " 1543920364764.mp4\n",
            " 58a5b6c7-392a-4679-9bee-4a247fa83aae.mov\n",
            " airtel-ringtone_original.mp3\n",
            " Bbq\n",
            " Character\n",
            " Classroom\n",
            "'Colab Notebooks'\n",
            " CS-041.pdf\n",
            " Dance\n",
            "'DLD 1.pdf'\n",
            "'Edited - jpg2pdf.pdf'\n",
            "'fine_tuned_traditional_ml_(logistic_regression-like).joblib'\n",
            "'fine_tuned_traditional_ml_(random_forest-like).joblib'\n",
            "'https:  www.fac.txt'\n",
            "'images (9).jpeg'\n",
            " IMG_0664.jpeg\n",
            " IMG_20151003_130302.jpg\n",
            " IMG_20171219_164852-1.jpg\n",
            " IMG_20180127_150527.jpg\n",
            " IMG_20180127_150818.jpg\n",
            " IMG_20180128_143759.jpg\n",
            " IMG-20180324-WA0000.jpg\n",
            " IMG-20180324-WA0001.jpg\n",
            " IMG-20180324-WA0002.jpg\n",
            " IMG-20180324-WA0003.jpg\n",
            " IMG-20180324-WA0004.jpg\n",
            " IMG-20180324-WA0025.jpg\n",
            " IMG-20180324-WA0026.jpg\n",
            " IMG-20180324-WA0027.jpg\n",
            " IMG_20180616_111511.jpg\n",
            " IMG_20180616_111526.jpg\n",
            " IMG_20180616_111552.jpg\n",
            " IMG_20180616_111603.jpg\n",
            " IMG_20180616_111643.jpg\n",
            " IMG_20180616_111708.jpg\n",
            " IMG_20181019_154232.jpg\n",
            " IMG_20181019_160956.jpg\n",
            " IMG_20181019_171839.jpg\n",
            " IMG_20181021_150156.jpg\n",
            " IMG_20181021_151340.jpg\n",
            " IMG_4542.jpeg\n",
            " IMG_4543.png\n",
            " IMG_4544.png\n",
            " IMG_4545.jpeg\n",
            " IMG_4546.png\n",
            " IMG_6512.png\n",
            " IMG_6513.png\n",
            "'jpg2pdf (4)urdu test.pdf'\n",
            " msgstore-2015-02-08.1.db.crypt8\n",
            "'Resume (10).pdf'\n",
            "'Resume (1) (1).pdf'\n",
            "'Resume (11).pdf'\n",
            "'Resume (1) (2).pdf'\n",
            "'Resume (12).pdf'\n",
            "'Resume (1) (3).pdf'\n",
            "'Resume (13).pdf'\n",
            "'Resume (14).pdf'\n",
            "'Resume (1).pdf'\n",
            "'Resume (2).pdf'\n",
            "'Resume (3).pdf'\n",
            "'Resume (4).pdf'\n",
            "'Resume (5).pdf'\n",
            "'Resume (6).pdf'\n",
            "'Resume (7).pdf'\n",
            "'Resume (8).pdf'\n",
            "'Resume (9).pdf'\n",
            " Resume.pdf\n",
            " selected_config_checkpoint.pkl\n",
            " Sentiment_Project\n",
            " Tiddi_Eid-ul-adha\n",
            " VID-20180123-WA0001.mp4\n",
            " VID-20180320-WA0014.mp4\n",
            " VID-20180320-WA0023.mp4\n",
            "'Watch \"Pepsi presents the much awaited launch song for Pepsi Battle of the Bands 2017!\" on YouTube'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Coordinator Agent Implementation with Ray\n",
        "# Purpose: Implement a sophisticated Coordinator Agent for agent communication, logging, and resource monitoring.\n",
        "\n",
        "import ray\n",
        "import psutil\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import torch  # Added for GPU check\n",
        "\n",
        "# Check if Ray is already initialized; if not, initialize in local mode\n",
        "if not ray.is_initialized():\n",
        "    ray.init(address='local', ignore_reinit_error=True, logging_level=logging.INFO)\n",
        "    logger.info(\"Ray cluster initialized in local mode.\")\n",
        "else:\n",
        "    logger.info(\"Ray cluster already initialized, skipping reinitialization.\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define Coordinator Agent as a Ray actor\n",
        "@ray.remote(num_cpus=1, num_gpus=0 if not torch.cuda.is_available() else 0.1)\n",
        "class CoordinatorAgent:\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "        self.start_time = datetime.now()\n",
        "        self.log_file = os.path.join(project_dir, 'coordinator_logs.txt')\n",
        "        logger.info(\"Coordinator Agent initialized.\")\n",
        "        self._check_log_file()\n",
        "\n",
        "    def _check_log_file(self):\n",
        "        \"\"\"Ensure log file exists and manage size.\"\"\"\n",
        "        if os.path.exists(self.log_file) and os.path.getsize(self.log_file) > 10 * 1024 * 1024:  # 10MB limit\n",
        "            with open(self.log_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "            with open(self.log_file, 'w') as f:\n",
        "                f.writelines(lines[-1000:])  # Keep last 1000 lines\n",
        "            logger.info(\"Log file rotated due to size limit.\")\n",
        "\n",
        "    def log_message(self, message):\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_entry = f\"[{timestamp}] {message}\"\n",
        "        self.logs.append(log_entry)\n",
        "        logger.info(log_entry)\n",
        "        try:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                f.write(log_entry + '\\n')\n",
        "        except Exception as e:\n",
        "            logger.error(\"Failed to write to log file: %s\", e)\n",
        "        return log_entry\n",
        "\n",
        "    def get_resource_usage(self):\n",
        "        try:\n",
        "            cpu_usage = psutil.cpu_percent(interval=1)\n",
        "            ram = psutil.virtual_memory()\n",
        "            ram_usage = ram.percent\n",
        "            gpu_usage = 0.0\n",
        "            memory_usage = 0.0\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_query = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,noheader,nounits'],\n",
        "                                          stdout=subprocess.PIPE, text=True, timeout=5)\n",
        "                gpu_data = [float(x.strip()) for x in gpu_query.stdout.strip().split(',') if x.strip()]\n",
        "                gpu_usage = gpu_data[0] if gpu_data else 0.0\n",
        "                memory_used = gpu_data[1] if len(gpu_data) > 1 else 0.0\n",
        "                memory_total = gpu_data[2] if len(gpu_data) > 2 else 1.0  # Avoid division by zero\n",
        "                memory_usage = (memory_used / memory_total * 100) if memory_total > 0 else 0.0\n",
        "            resource_dict = {\n",
        "                \"cpu_usage (%)\": cpu_usage,\n",
        "                \"ram_usage (%)\": ram_usage,\n",
        "                \"gpu_usage (%)\": gpu_usage,\n",
        "                \"gpu_memory_usage (%)\": memory_usage,\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "            logger.info(\"Resource usage: %s\", json.dumps(resource_dict))\n",
        "            return resource_dict\n",
        "        except Exception as e:\n",
        "            logger.error(\"Resource monitoring error: %s\", e)\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def get_logs(self):\n",
        "        return self.logs\n",
        "\n",
        "# Instantiate Coordinator Agent\n",
        "coordinator = CoordinatorAgent.remote()\n",
        "\n",
        "# Test Coordinator functionalities\n",
        "ray.get(coordinator.log_message.remote(\"System startup successful.\"))\n",
        "resource_usage = ray.get(coordinator.get_resource_usage.remote())\n",
        "print(\"Current Resource Usage:\", resource_usage)\n",
        "logs = ray.get(coordinator.get_logs.remote())\n",
        "print(\"Coordinator Logs:\", logs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7tggaZbjM9n",
        "outputId": "3fe4ca35-6f51-45c1-c560-a759f98316ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-29 07:28:29,726\tINFO worker.py:1888 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Resource Usage: {'cpu_usage (%)': 8.2, 'ram_usage (%)': 18.1, 'gpu_usage (%)': 0.0, 'gpu_memory_usage (%)': 0.0, 'timestamp': '2025-05-29 07:28:36'}\n",
            "Coordinator Logs: ['[2025-05-29 07:28:34] System startup successful.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Data Input Handling\n",
        "# Purpose: Handle dataset upload, validation, and storage with advanced error checking, dynamic column detection, and user prompt.\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Function to infer text and label columns\n",
        "def infer_columns(df):\n",
        "    \"\"\"Infer text and label columns based on data characteristics.\"\"\"\n",
        "    text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]\n",
        "    label_cols = [col for col in df.columns if df[col].dtype in ['object', 'int', 'float'] and df[col].nunique() < len(df) / 10]\n",
        "    if not text_cols or not label_cols:\n",
        "        raise ValueError(\"Could not infer text or label columns. Ensure dataset contains text and categorical label columns.\")\n",
        "    return text_cols[0], label_cols[0]\n",
        "\n",
        "def validate_dataset(df, text_column, label_column):\n",
        "    \"\"\"Validate dataset structure and content dynamically.\"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise ValueError(\"Input must be a pandas DataFrame.\")\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Dataset is empty.\")\n",
        "    if df[text_column].isnull().all() or df[label_column].isnull().all():\n",
        "        raise ValueError(f\"{text_column} or {label_column} columns contain only null values.\")\n",
        "    # Validate minimum text length\n",
        "    if (df[text_column].str.len() < 10).any():\n",
        "        logger.warning(\"Some text entries in %s are shorter than 10 characters.\", text_column)\n",
        "    # Validate label diversity (at least 2 unique classes)\n",
        "    if df[label_column].nunique() < 2:\n",
        "        raise ValueError(f\"Label column {label_column} must have at least 2 unique categories.\")\n",
        "    return True\n",
        "\n",
        "def process_dataset():\n",
        "    try:\n",
        "        # Prompt user for dataset upload\n",
        "        print(\"Please upload your dataset (e.g., dataset.csv):\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded.\")\n",
        "        filename = list(uploaded.keys())[0]\n",
        "\n",
        "        # Support CSV and JSON\n",
        "        if filename.endswith('.csv'):\n",
        "            df = pd.read_csv(filename, encoding='utf-8', on_bad_lines='skip')\n",
        "        elif filename.endswith('.json'):\n",
        "            df = pd.read_json(filename, encoding='utf-8')\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Use CSV or JSON.\")\n",
        "\n",
        "        # Infer columns dynamically\n",
        "        text_column, label_column = infer_columns(df)\n",
        "        logger.info(\"Inferred text column: %s, label column: %s\", text_column, label_column)\n",
        "\n",
        "        # Validate dataset\n",
        "        validate_dataset(df, text_column, label_column)\n",
        "        logger.info(\"Dataset validation successful.\")\n",
        "        print(\"Dataset Preview (first 5 rows):\")\n",
        "        print(df[[text_column, label_column]].head())\n",
        "        print(f\"Null counts: \\n{df.isnull().sum()}\")\n",
        "\n",
        "        # Save processed dataset\n",
        "        output_file = os.path.join(project_dir, 'processed_dataset.csv')\n",
        "        max_attempts = 2\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "                logger.info(\"Dataset saved to %s\", output_file)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logger.error(\"Attempt %d failed to save dataset: %s\", attempt + 1, e)\n",
        "                if attempt == max_attempts - 1:\n",
        "                    raise\n",
        "\n",
        "        # Additional validation and statistics\n",
        "        duplicates = df.duplicated().sum()\n",
        "        logger.info(\"Number of duplicate rows: %d\", duplicates)\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        print(f\"Label distribution: \\n{df[label_column].value_counts()}\")\n",
        "\n",
        "        # Prompt user for additional instructions/context\n",
        "        print(\"Please enter any additional instructions or context for the dataset (e.g., 'Use for sentiment analysis with 3 classes') or press Enter to skip:\")\n",
        "        user_prompt = input().strip()\n",
        "        prompt_data = {\n",
        "            'dataset_prompt': user_prompt if user_prompt else \"No additional instructions provided\",\n",
        "            'filename': filename,\n",
        "            'text_column': text_column,\n",
        "            'label_column': label_column,\n",
        "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "        prompt_file = os.path.join(project_dir, 'user_dataset_prompt.json')\n",
        "        with open(prompt_file, 'w') as f:\n",
        "            json.dump(prompt_data, f)\n",
        "        logger.info(\"User dataset prompt saved to %s\", prompt_file)\n",
        "\n",
        "        return df, text_column, label_column\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error processing dataset: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Execute data processing\n",
        "dataset, text_column, label_column = process_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "fEt6LMMMjj1e",
        "outputId": "992153ed-8e61-4954-f208-1f1dd6bca27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your dataset (e.g., dataset.csv):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a0c0bab1-3d18-455b-84d0-dcc2810afa01\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a0c0bab1-3d18-455b-84d0-dcc2810afa01\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving imdb.csv to imdb.csv\n",
            "Dataset Preview (first 5 rows):\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "Null counts: \n",
            "review       0\n",
            "sentiment    0\n",
            "dtype: int64\n",
            "Dataset shape: (50000, 2)\n",
            "Label distribution: \n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "Please enter any additional instructions or context for the dataset (e.g., 'Use for sentiment analysis with 3 classes') or press Enter to skip:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Final Validation and Cleanup\n",
        "# Purpose: Validate the entire setup and clean up temporary files.\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Validate Coordinator, dataset, and user prompt\n",
        "try:\n",
        "    ray.get(coordinator.log_message.remote(\"Setup validation started.\"))\n",
        "    assert os.path.exists(os.path.join(project_dir, 'processed_dataset.csv')), \"Dataset not saved to Drive.\"\n",
        "    assert os.path.exists(os.path.join(project_dir, 'user_dataset_prompt.json')), \"User dataset prompt file not saved.\"\n",
        "    resource_usage = ray.get(coordinator.get_resource_usage.remote())\n",
        "    assert all(k in resource_usage for k in [\"cpu_usage (%)\", \"ram_usage (%)\", \"gpu_usage (%)\"]), \"Resource usage data incomplete.\"\n",
        "    assert all(v >= 0 for k, v in resource_usage.items() if k != \"timestamp\"), \"Invalid resource usage values.\"\n",
        "    logger.info(\"Inferred columns - Text: %s, Label: %s\", text_column, label_column)\n",
        "    print(\"Setup validation successful. System ready for Part 2.\")\n",
        "except AssertionError as e:\n",
        "    error_msg = f\"Validation failed: {e}\"\n",
        "    logger.error(error_msg)\n",
        "    ray.get(coordinator.log_message.remote(error_msg))\n",
        "    raise\n",
        "finally:\n",
        "    ray.get(coordinator.log_message.remote(\"Setup process completed.\"))\n",
        "    # Automatic cleanup of temporary uploaded files\n",
        "    temp_files = glob.glob(\"*.csv\") + glob.glob(\"*.json\")\n",
        "    for temp_file in temp_files:\n",
        "        try:\n",
        "            os.remove(temp_file)\n",
        "            logger.info(\"Cleaned up temporary file: %s\", temp_file)\n",
        "        except Exception as e:\n",
        "            logger.error(\"Failed to clean up %s: %s\", temp_file, e)\n",
        "    logger.info(\"Cleanup completed.\")"
      ],
      "metadata": {
        "id": "CIya4Asqlwow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Drive mounted successfully.\")"
      ],
      "metadata": {
        "id": "AK1MpgHUaWV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the root directory to start the search\n",
        "root_dir = '/content/drive/MyDrive'\n",
        "\n",
        "# Search for the notebooks\n",
        "notebook1 = 'Part_1_Environment_Setup.ipynb'\n",
        "notebook2 = 'Data_Analysis_Quality_Preprocessing.ipynb'\n",
        "\n",
        "# Walk through the directory to find the files\n",
        "for root, dirs, files in os.walk(root_dir):\n",
        "    if notebook1 in files:\n",
        "        print(f\"Found {notebook1} at: {os.path.join(root, notebook1)}\")\n",
        "    if notebook2 in files:\n",
        "        print(f\"Found {notebook2} at: {os.path.join(root, notebook2)}\")"
      ],
      "metadata": {
        "id": "B1r3YDUcacQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}